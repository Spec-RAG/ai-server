import json
import logging
from typing import AsyncGenerator
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch
from langchain_tavily._utilities import TavilySearchAPIWrapper
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage

from app.core.config import settings

logger = logging.getLogger(__name__)

@tool
async def spring_docs_search(query: str) -> list[dict]:
    """Search the Spring official documentation (docs.spring.io) for the given query.
    Use this tool whenever you need to find factual information, guides, or API details about Spring Framework, Spring Boot, or any other Spring projects.
    """
    search_query = f"{query} site:docs.spring.io".strip()
    logger.info(f"ğŸ¤– [Agent Decision] Calling Tavily Search with query: {search_query}")

    try:
        tavily_tool = TavilySearch(
            api_wrapper=TavilySearchAPIWrapper(tavily_api_key=settings.TAVILY_API_KEY),
            max_results=5,
            include_raw_content=True,
            include_domains=["docs.spring.io"],
        )
        results = await tavily_tool.ainvoke({"query": search_query})
        
        if not results:
            logger.info("ğŸ¤– [Agent Observation] Tavily Search returned empty results.")
            return []
            
        logger.info(f"ğŸ¤– [Agent Observation] Tavily Search returned {len(results)} results.")
        return results

    except Exception as e:
        logger.error(f"[Tavily Search Error] {e}")
        return []

def _build_llm_with_tools():
    llm = ChatGoogleGenerativeAI(
        model=settings.GEMINI_CHAT_MODEL,
        google_api_key=settings.GEMINI_API_KEY,
        temperature=0,
    )
    tools = [spring_docs_search]
    # bind_toolsë¥¼ í†µí•´ LLMì´ ë„êµ¬ë¥¼ ì¸ì‹í•˜ê³  í˜¸ì¶œì„ ê²°ì •í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    return llm.bind_tools(tools), tools

async def get_tavily_rag_answer_stream_with_sources_async(
    question: str, search_query: str, history_messages: list=[]
) -> AsyncGenerator[dict, None]:
    """LangChainì˜ bind_toolsë¥¼ ì´ìš©í•´ ìˆœìˆ˜ Python ë£¨í”„ë¡œ ë„êµ¬ í˜¸ì¶œê³¼ ìŠ¤íŠ¸ë¦¬ë°ì„ êµ¬í˜„í•©ë‹ˆë‹¤."""
    
    llm_with_tools, tools = _build_llm_with_tools()
    tool_map = {tool.name: tool for tool in tools}
    
    system_prompt = SystemMessage(content=(
        "ë‹¹ì‹ ì€ ì¹œì ˆí•œ Spring Projects ì „ë¬¸ê°€ ì±—ë´‡ì…ë‹ˆë‹¤.\n"
        "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n"
        "Springê³¼ ê´€ë ¨ëœ êµ¬ì²´ì ì¸ ì •ë³´, ì‚¬ìš©ë²•, APIë‚˜ ìµœì‹  ë³€ê²½ì  ë“±ì— ëŒ€í•œ ë‹µë³€ì„ í•  ë•ŒëŠ” "
        "ë°˜ë“œì‹œ `spring_docs_search` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³µì‹ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•œ í›„ ê·¸ ê²°ê´ê°’ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ë„êµ¬ë¥¼ ì‚¬ìš©í–ˆë‹¤ë©´, ë‹µë³€ì˜ ê° ë¬¸ë‹¨ ëì— í•´ë‹¹ ë‚´ìš©ì´ ì°¸ì¡°í•œ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ [1], [2] í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.\n"
        "ë‹¨ìˆœí•œ ì¸ì‚¬ë‚˜ ê²€ìƒ‰ì´ í•„ìš” ì—†ëŠ” ì¼ë°˜ì ì¸ ëŒ€í™”ë¼ë©´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë°”ë¡œ ë‹µë³€í•˜ì…”ë„ ë©ë‹ˆë‹¤."
    ))

    # LLMì—ê²Œ ì „ë‹¬í•  ì „ì²´ ë©”ì‹œì§€ ëª©ë¡ êµ¬ì„±
    messages = [system_prompt] + history_messages + [HumanMessage(content=question)]
    sources_collected = []
    
    logger.info("ğŸ¤– [Model] Processing user request...")

    # 1ë‹¨ê³„: ëª¨ë¸ì—ê²Œ ì§ˆë¬¸ì„ ì „ë‹¬í•˜ì—¬ ë‹µë³€ ë˜ëŠ” ë„êµ¬ í˜¸ì¶œ ì—¬ë¶€ë¥¼ ê²°ì •
    response: AIMessage = await llm_with_tools.ainvoke(messages)
    messages.append(response)

    # 2ë‹¨ê³„: ë„êµ¬ í˜¸ì¶œ(Tool Calls)ì´ ìˆë‹¤ë©´ ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ë§ë¶™ì´ê³  ë‹¤ì‹œ ëª¨ë¸ í˜¸ì¶œ
    if response.tool_calls:
        for tool_call in response.tool_calls:
            selected_tool = tool_map[tool_call["name"]]
            
            # ë¹„ë™ê¸°ë¡œ ë„êµ¬ ì‹¤í–‰
            tool_result = await selected_tool.ainvoke(tool_call["args"])
            
            # ì†ŒìŠ¤ ìˆ˜ì§‘ìš© (TavilySearchëŠ” dictë¥¼ ë°˜í™˜í•˜ê³ , ë‚´ë¶€ì— 'results' ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì§‘ë‹ˆë‹¤)
            # ê¸°ì¡´ TavilySearchResultsëŠ” list[dict]ë¥¼ ë°˜í™˜í–ˆì—ˆìŒ.
            if isinstance(tool_result, dict) and "results" in tool_result:
                results_list = tool_result["results"]
            elif isinstance(tool_result, list):
                results_list = tool_result
            else:
                results_list = []
                
            for idx, doc in enumerate(results_list, start=len(sources_collected) + 1):
                sources_collected.append({
                    "index": idx,
                    "source_url": doc.get("url", ""),
                    "page_content": doc.get("content", "")[:1000],
                })

            # ëª¨ë¸ì—ê²Œ ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì•Œë ¤ì£¼ê¸° ìœ„í•œ ë©”ì‹œì§€ ì¶”ê°€
            tool_message = ToolMessage(
                content=json.dumps(tool_result, ensure_ascii=False),
                tool_call_id=tool_call["id"]
            )
            messages.append(tool_message)
        
        # 3ë‹¨ê³„: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼(Observation)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ì„ ìŠ¤íŠ¸ë¦¬ë°
        full_answer = ""
        # StrOutputParserë¥¼ íŒŒì´í”„ì— ì¶”ê°€í•˜ë©´ ì•ˆì „í•œ ìŠ¤íŠ¸ë§ë§Œ ë„˜ì–´ì˜µë‹ˆë‹¤
        final_chain = llm_with_tools | StrOutputParser()
        
        async for chunk_text in final_chain.astream(messages):
            if chunk_text:
                full_answer += chunk_text
                yield {"type": "chunk", "content": chunk_text}
        
        yield {"type": "answer", "content": full_answer}

    else:
        # ë„êµ¬ í˜¸ì¶œì´ í•„ìš” ì—†ëŠ” ê²½ìš°(ë‹¨ìˆœ ì¸ì‚¬ ë“±)
        text_content = ""
        if isinstance(response.content, str):
            text_content = response.content
        elif isinstance(response.content, list):
            for item in response.content:
                if isinstance(item, str):
                    text_content += item
                elif isinstance(item, dict) and "text" in item:
                    text_content += item["text"]
        else:
            text_content = str(response.content)

        yield {"type": "chunk", "content": text_content}
        yield {"type": "answer", "content": text_content}

    # ìˆ˜ì§‘ëœ ì¶œì²˜ ëª©ë¡ ë¦¬í„´
    yield {"type": "sources", "sources": sources_collected}
    logger.info("ğŸ¤– [Model Stream Completed]")
