{"id": "sha256:b1119d2057a883060c71680c721e95e2cf5185e44dec07aecc8f7fead67097ba", "content": "[[spring-batch-reference-documentation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/header/index-header.adoc", "title": "index-header", "heading": "index-header", "heading_level": 1, "file_order": 0, "section_index": 0, "content_hash": "b1119d2057a883060c71680c721e95e2cf5185e44dec07aecc8f7fead67097ba", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/header/index-header.adoc"}}
{"id": "sha256:59896eb8569be49d3da8ddde21422d42340ba1d14fcc1d7c0596f50e5391a991", "content": "[[advancedMetaData]]\n\n[[jobregistry]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc", "title": "advanced-meta-data", "heading": "advanced-meta-data", "heading_level": 1, "file_order": 1, "section_index": 0, "content_hash": "59896eb8569be49d3da8ddde21422d42340ba1d14fcc1d7c0596f50e5391a991", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc"}}
{"id": "sha256:938c3a44691e46f2f60421be6dd6b9ae7fb718ee7ca6dc1b21df5973129b1b12", "content": "A `JobRegistry` is used to track which jobs are available in the context and can be operated by\nthe `JobOperator`. It is also useful for collecting jobs centrally in an application context when\nthey have been created elsewhere (for example, in child contexts). You can also use custom `JobRegistry`\nimplementations to manipulate the names and other properties of the jobs that are registered.\nThere is only one implementation provided by the framework and this is based on a simple\nmap from job name to job instance, the `MapJobregistry`.\n\n[tabs]\n====\nJava::\n+\nWhen using `@EnableBatchProcessing`, a `MapJobregistry` is provided for you.\nThe following example shows how to configure your own `JobRegistry`:\n+\n[source, java]\n----\n...\n@Bean\npublic JobRegistry jobRegistry() throws Exception {\n\treturn new MyCustomJobRegistry();\n}\n...\n----\n\nXML::\n+\nThe following example shows how to include a `JobRegistry` for a job defined in XML:\n+\n[source, xml]\n----\n<bean id=\"jobRegistry\" class=\"org.springframework.batch.core.configuration.support.MapJobRegistry\" />\n----\n\n====\n\nThe `MapJobRegistry` provided by Spring Batch is smart enough to populate itself with all the jobs\nin the application context. However, if you are using a custom implementation of `JobRegistry`, you\nneed to populate it manually with the jobs that you want to operate through the job operator.\n\n[[JobParametersIncrementer]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc", "title": "advanced-meta-data", "heading": "JobRegistry", "heading_level": 2, "file_order": 1, "section_index": 1, "content_hash": "938c3a44691e46f2f60421be6dd6b9ae7fb718ee7ca6dc1b21df5973129b1b12", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc"}}
{"id": "sha256:eead56a48e7e80a649269e0549432047ef1cf671699f0dbc0effbaf3581e6c18", "content": "Most of the methods on `JobOperator` are\nself-explanatory, and you can find more detailed explanations in the\nhttps://docs.spring.io/spring-batch/docs/current/api/org/springframework/batch/core/launch/JobOperator.html[Javadoc of the interface]. However, the\n`startNextInstance` method is worth noting. This\nmethod always starts a new instance of a `Job`.\nThis can be extremely useful if there are serious issues in a\n`JobExecution` and the `Job`\nneeds to be started over again from the beginning. Unlike\n`JobLauncher` (which requires a new\n`JobParameters` object that triggers a new\n`JobInstance`), if the parameters are different from\nany previous set of parameters, the\n`startNextInstance` method uses the\n`JobParametersIncrementer` tied to the\n`Job` to force the `Job` to a\nnew instance:\n\n[source, java]\n----\npublic interface JobParametersIncrementer {\n\n JobParameters getNext(JobParameters parameters);\n\n}\n----\n\nThe contract of `JobParametersIncrementer` is\nthat, given a xref:domain.adoc#jobParameters[JobParameters]\nobject, it returns the \"`next`\" `JobParameters`\nobject by incrementing any necessary values it may contain. This\nstrategy is useful because the framework has no way of knowing what\nchanges to the `JobParameters` make it the \"`next`\"\ninstance. For example, if the only value in\n`JobParameters` is a date and the next instance\nshould be created, should that value be incremented by one day or one\nweek (if the job is weekly, for instance)? The same can be said for any\nnumerical values that help to identify the `Job`,\nas the following example shows:\n\n[source, java]\n----\npublic class SampleIncrementer implements JobParametersIncrementer {\n\n public JobParameters getNext(JobParameters parameters) {\n if (parameters==null || parameters.isEmpty()) {\n return new JobParametersBuilder().addLong(\"run.id\", 1L).toJobParameters();\n }\n long id = parameters.getLong(\"run.id\",1L) + 1;\n return new JobParametersBuilder().addLong(\"run.id\", id).toJobParameters();\n }\n}\n----\n\nIn this example, the value with a key of `run.id` is used to\ndiscriminate between `JobInstances`. If the\n`JobParameters` passed in is null, it can be\nassumed that the `Job` has never been run before\nand, thus, its initial state can be returned. However, if not, the old\nvalue is obtained, incremented by one, and returned.\n\n[tabs]\n====\nJava::\n+\nFor jobs defined in Java, you can associate an incrementer with a `Job` through the\n`incrementer` method provided in the builders, as follows:\n+\n[source, java]\n----\n@Bean\npublic Job footballJob(JobRepository jobRepository) {\n return new JobBuilder(\"footballJob\", jobRepository)\n .incrementer(sampleIncrementer())\n ...\n .build();\n}\n----\n\nXML::\n+\nFor jobs defined in XML, you can associate an incrementer with a `Job` through the\n`incrementer` attribute in the namespace, as follows:\n+\n[source, xml]\n----\n<job id=\"footballJob\" incrementer=\"sampleIncrementer\">\n ...\n</job>\n----\n====\n\n[[stoppingAJob]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc", "title": "advanced-meta-data", "heading": "JobParametersIncrementer", "heading_level": 2, "file_order": 1, "section_index": 2, "content_hash": "eead56a48e7e80a649269e0549432047ef1cf671699f0dbc0effbaf3581e6c18", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc"}}
{"id": "sha256:5dd1aeb27eb9056361ecb27bac51e15e9637e45094120d74746586b5ef2ddc90", "content": "One of the most common use cases of\n`JobOperator` is gracefully stopping a\nJob:\n\n[source, java]\n----\nSet<Long> executions = jobOperator.getRunningExecutions(\"sampleJob\");\njobOperator.stop(executions.iterator().next());\n----\n\nThe shutdown is not immediate, since there is no way to force\nimmediate shutdown, especially if the execution is currently in\ndeveloper code that the framework has no control over, such as a\nbusiness service. However, as soon as control is returned back to the\nframework, it sets the status of the current\n`StepExecution` to\n`BatchStatus.STOPPED`, saves it, and does the same\nfor the `JobExecution` before finishing.\n\n[[graceful-shutdown]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc", "title": "advanced-meta-data", "heading": "Stopping a Job", "heading_level": 2, "file_order": 1, "section_index": 3, "content_hash": "5dd1aeb27eb9056361ecb27bac51e15e9637e45094120d74746586b5ef2ddc90", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc"}}
{"id": "sha256:0d96cdd7df3cd9d8823dab15f7c9961d68e52c60122adb9de12f18ad5f0a0f8c", "content": "As of v6.0+, Spring Batch provides a `JobExecutionShutdownHook` that you can attach to the JVM runtime\nin order to intercept external interruption signals and gracefully stop the job execution:\n\n[source, java]\n----\nThread springBatchHook = new JobExecutionShutdownHook(jobExecution, jobOperator);\nRuntime.getRuntime().addShutdownHook(springBatchHook);\n----\n\nA `JobExecutionShutdownHook` requires the job execution to track as well as a reference to a job operator\nthat will be used to stop the execution.\n\n[[recover-job]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc", "title": "advanced-meta-data", "heading": "Handling external interruption signals", "heading_level": 2, "file_order": 1, "section_index": 4, "content_hash": "0d96cdd7df3cd9d8823dab15f7c9961d68e52c60122adb9de12f18ad5f0a0f8c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc"}}
{"id": "sha256:bdc91ece5455bdc2b2546621af8b7af4cc73c68a2295a615b6a673888560c766", "content": "If a graceful shutdown is not performed properly (ie the JVM is shutdown abruptly), Spring Batch will not\nhave a chance to update the execution state correctly in order to restart the failed job execution. In this\ncase, the job execution will stay in a `STARTED` state which is not restartable. In this case, it is possible\nto recover such a job execution with the `JobOperator` API:\n\n[source, java]\n----\nJobExecution jobExecution = ...; // get the job execution to recover\njobOperator.recover(jobExecution);\njobOperator.restart(jobExecution);\n----\n\n[[aborting-a-job]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc", "title": "advanced-meta-data", "heading": "Recovering a job", "heading_level": 2, "file_order": 1, "section_index": 5, "content_hash": "bdc91ece5455bdc2b2546621af8b7af4cc73c68a2295a615b6a673888560c766", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc"}}
{"id": "sha256:158aee6569524b1cecb10807f8a9b9c6fe3ba47872d2baeeed9a08e5bd86171b", "content": "A job execution that is `FAILED` can be\nrestarted (if the `Job` is restartable). A job execution whose status is\n`ABANDONED` cannot be restarted by the framework.\nThe `ABANDONED` status is also used in step\nexecutions to mark them as skippable in a restarted job execution. If a\njob is running and encounters a step that has been marked\n`ABANDONED` in the previous failed job execution, it\nmoves on to the next step (as determined by the job flow definition\nand the step execution exit status).\n\nIf the process died (`kill -9` or server\nfailure), the job is, of course, not running, but the `JobRepository` has\nno way of knowing because no one told it before the process died. You\nhave to tell it manually that you know that the execution either failed\nor should be considered aborted (change its status to\n`FAILED` or `ABANDONED`). This is\na business decision, and there is no way to automate it. Change the\nstatus to `FAILED` only if it is restartable and you know that the restart data is valid.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc", "title": "advanced-meta-data", "heading": "Aborting a Job", "heading_level": 2, "file_order": 1, "section_index": 6, "content_hash": "158aee6569524b1cecb10807f8a9b9c6fe3ba47872d2baeeed9a08e5bd86171b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/advanced-meta-data.adoc"}}
{"id": "sha256:6f23848fe2895db73a103d33eb5f366e87f012d95772a5516bf3baa28d06679a", "content": "[[infraConfig]]\n\nAs described earlier, Spring Batch relies on a number of infrastructure beans to operate jobs and steps,\nincluding the `JobOperator` and the `JobRepository`. While it is possible to define these beans manually, it is much easier to use the\n`@EnableBatchProcessing` annotation or the `DefaultBatchConfiguration` class to provide a base configuration.\n\nBy default, Spring Batch will provide a resourceless batch infrastructure configuration, which is based on\nthe `ResourcelessJobRepository` implementation. If you want to use a database-backed job repository, you can\nuse the `@EnableJdbcJobRepository` / `@EnableMongoJobRepository` annotations or the equivalent classes\n`JdbcDefaultBatchConfiguration` / `MongoDefaultBatchConfiguration` as described in the\nxref:job/configuring-repository.adoc[Configuring a JobRepository] section.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-infrastructure.adoc", "title": "configuring-infrastructure", "heading": "configuring-infrastructure", "heading_level": 1, "file_order": 2, "section_index": 0, "content_hash": "6f23848fe2895db73a103d33eb5f366e87f012d95772a5516bf3baa28d06679a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-infrastructure.adoc"}}
{"id": "sha256:17b5c5122d8a83ec66c123e6232f31ba6ec7cee2a6dad77e8df4192d08d08a3e", "content": "The `@EnableBatchProcessing` annotation works similarly to other `@Enable*` annotations in the\nSpring family. In this case, `@EnableBatchProcessing` provides a base configuration for\nbuilding batch jobs. Within this base configuration, an instance of `StepScope` and `JobScope` are\ncreated, in addition to a number of beans being made available to be autowired:\n\n* `JobRepository`: a bean named `jobRepository`\n* `JobOperator`: a bean named `jobOperator`\n\nHere is an example of how to use the `@EnableBatchProcessing` annotation in a Java configuration class:\n\n[source, java]\n----\n@Configuration\n@EnableBatchProcessing\npublic class MyJobConfiguration {\n\n\t@Bean\n\tpublic Job job(JobRepository jobRepository) {\n return new JobBuilder(\"myJob\", jobRepository)\n //define job flow as needed\n .build();\n\t}\n\n}\n----\n\nIt is possible to customize the configuration of any infrastructure bean by using the attributes of\nthe `@EnableBatchProcessing` annotation.\n\nNOTE: Only one configuration class needs to have the `@EnableBatchProcessing` annotation. Once\nyou have a class annotated with it, you have all the configuration described earlier.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-infrastructure.adoc", "title": "configuring-infrastructure", "heading": "Annotation-based Configuration", "heading_level": 2, "file_order": 2, "section_index": 1, "content_hash": "17b5c5122d8a83ec66c123e6232f31ba6ec7cee2a6dad77e8df4192d08d08a3e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-infrastructure.adoc"}}
{"id": "sha256:12052ea1da7fa5fd6f2642fe4294918c5def45f3ee9ff2b10c4664065edd4b05", "content": "Similarly to the annotation-based configuration, a programmatic way of configuring infrastructure\nbeans is provided through the `DefaultBatchConfiguration` class. This class provides the same beans\nprovided by `@EnableBatchProcessing` and can be used as a base class to configure batch jobs.\nThe following snippet is a typical example of how to use it:\n\n[source, java]\n----\n@Configuration\nclass MyJobConfiguration extends DefaultBatchConfiguration {\n\n\t@Bean\n\tpublic Job job(JobRepository jobRepository) {\n return new JobBuilder(\"myJob\", jobRepository)\n // define job flow as needed\n .build();\n\t}\n\n}\n----\n\nYou can customize the configuration of any infrastructure bean by overriding the required setter.\n\nIMPORTANT: `@EnableBatchProcessing` should *not* be used with `DefaultBatchConfiguration`. You should\neither use the declarative way of configuring Spring Batch through `@EnableBatchProcessing`,\nor use the programmatic way of extending `DefaultBatchConfiguration`, but not both ways at\nthe same time.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-infrastructure.adoc", "title": "configuring-infrastructure", "heading": "Programmatic Configuration", "heading_level": 2, "file_order": 2, "section_index": 2, "content_hash": "12052ea1da7fa5fd6f2642fe4294918c5def45f3ee9ff2b10c4664065edd4b05", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-infrastructure.adoc"}}
{"id": "sha256:cd5009d5cd637575b3225ca3a17aa639e5fbdaf73c5973bdde367040058f6b80", "content": "[[configuringAJob]]\n\nThere are multiple implementations of the xref:job.adoc[`Job`] interface. However,\nthese implementations are abstracted behind either the provided builders (for Java configuration) or the XML\nnamespace (for XML-based configuration). The following example shows both Java and XML configuration:\n\n[tabs]\n====\nJava::\n+\n[source, java]\n----\n@Bean\npublic Job footballJob(JobRepository jobRepository) {\n return new JobBuilder(\"footballJob\", jobRepository)\n .start(playerLoad())\n .next(gameLoad())\n .next(playerSummarization())\n .build();\n}\n----\n+\nA `Job` (and, typically, any `Step` within it) requires a `JobRepository`.\n+\nThe preceding example illustrates a `Job` that consists of three `Step` instances. The job related\nbuilders can also contain other elements that help with parallelization (`Split`),\ndeclarative flow control (`Decision`), and externalization of flow definitions (`Flow`).\n\nXML::\n+\nThere are multiple implementations of the xref:job.adoc[`Job`]\ninterface. However, the namespace abstracts away the differences in configuration. It has\nonly three required dependencies: a name, `JobRepository` , and a list of `Step` instances.\nThe following example creates a `footballJob`:\n+\n[source, xml]\n----\n<job id=\"footballJob\">\n <step id=\"playerload\" parent=\"s1\" next=\"gameLoad\"/>\n <step id=\"gameLoad\" parent=\"s2\" next=\"playerSummarization\"/>\n <step id=\"playerSummarization\" parent=\"s3\"/>\n</job>\n----\n+\nThe preceding examples uses a parent bean definition to create the steps.\nSee the section on xref:step.adoc[step configuration]\nfor more options when declaring specific step details inline. The XML namespace\ndefaults to referencing a repository with an `id` of `jobRepository`, which\nis a sensible default. However, you can explicitly override this default:\n+\n[source, xml]\n----\n<job id=\"footballJob\" job-repository=\"specialRepository\">\n <step id=\"playerload\" parent=\"s1\" next=\"gameLoad\"/>\n <step id=\"gameLoad\" parent=\"s3\" next=\"playerSummarization\"/>\n <step id=\"playerSummarization\" parent=\"s3\"/>\n</job>\n----\n+\nIn addition to steps, a job configuration can contain other elements\nthat help with parallelization (`<split>`),\ndeclarative flow control (`<decision>`), and\nexternalization of flow definitions\n(`<flow/>`).\n\n====\n\n[[restartability]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc", "title": "configuring-job", "heading": "configuring-job", "heading_level": 1, "file_order": 3, "section_index": 0, "content_hash": "cd5009d5cd637575b3225ca3a17aa639e5fbdaf73c5973bdde367040058f6b80", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc"}}
{"id": "sha256:b349f2925ee2d0e1f5df6eb94a9f4ddbf701b1d7f473d8d8ac53393107796ed7", "content": "One key issue when executing a batch job concerns the behavior of a `Job` when it is\nrestarted. The launching of a `Job` is considered to be a \"`restart`\" if a `JobExecution`\nalready exists for the particular `JobInstance`. Ideally, all jobs should be able to start\nup where they left off, but there are scenarios where this is not possible.\n_In this scenario, it is entirely up to the developer to ensure that a new `JobInstance` is created._\nHowever, Spring Batch does provide some help. If a `Job` should never be\nrestarted but should always be run as part of a new `JobInstance`, you can set the\nrestartable property to `false`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to set the `restartable` field to `false` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job footballJob(JobRepository jobRepository) {\n return new JobBuilder(\"footballJob\", jobRepository)\n .preventRestart()\n ...\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to set the `restartable` field to `false` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"footballJob\" restartable=\"false\">\n ...\n</job>\n----\n====\n\nTo phrase it another way, setting `restartable` to `false` means \"`this\n`Job` does not support being started again`\". Restarting a `Job` that is not\nrestartable causes a `JobRestartException` to\nbe thrown.\nThe following Junit code causes the exception to be thrown:\n\n[source, java]\n----\nJob job = new SimpleJob();\njob.setRestartable(false);\n\nJobParameters jobParameters = new JobParameters();\n\nJobExecution firstExecution = jobRepository.createJobExecution(job, jobParameters);\njobRepository.saveOrUpdate(firstExecution);\n\ntry {\n jobRepository.createJobExecution(job, jobParameters);\n fail();\n}\ncatch (JobRestartException e) {\n // expected\n}\n----\n\nThe first attempt to create a\n`JobExecution` for a non-restartable\njob causes no issues. However, the second\nattempt throws a `JobRestartException`.\n\n[[interceptingJobExecution]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc", "title": "configuring-job", "heading": "Restartability", "heading_level": 2, "file_order": 3, "section_index": 1, "content_hash": "b349f2925ee2d0e1f5df6eb94a9f4ddbf701b1d7f473d8d8ac53393107796ed7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc"}}
{"id": "sha256:ff13f2ffb02c15736443b08c5d429dad7f0b2682c453a26c1aa000b5e09e40f1", "content": "During the course of the execution of a\n`Job`, it may be useful to be notified of various\nevents in its lifecycle so that custom code can be run.\n`SimpleJob` allows for this by calling a\n`JobListener` at the appropriate time:\n\n[source, java]\n----\npublic interface JobExecutionListener {\n\n void beforeJob(JobExecution jobExecution);\n\n void afterJob(JobExecution jobExecution);\n}\n----\n\nYou can add `JobListeners` to a `SimpleJob` by setting listeners on the job.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to add a listener method to a Java job definition:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job footballJob(JobRepository jobRepository) {\n return new JobBuilder(\"footballJob\", jobRepository)\n .listener(sampleListener())\n ...\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to add a listener element to an XML job definition:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"footballJob\">\n <step id=\"playerload\" parent=\"s1\" next=\"gameLoad\"/>\n <step id=\"gameLoad\" parent=\"s2\" next=\"playerSummarization\"/>\n <step id=\"playerSummarization\" parent=\"s3\"/>\n <listeners>\n <listener ref=\"sampleListener\"/>\n </listeners>\n</job>\n----\n====\n\nNote that the `afterJob` method is called regardless of the success or\nfailure of the `Job`. If you need to determine success or failure, you can get that information\nfrom the `JobExecution`:\n\n[source, java]\n----\npublic void afterJob(JobExecution jobExecution){\n if (jobExecution.getStatus() == BatchStatus.COMPLETED ) {\n //job success\n }\n else if (jobExecution.getStatus() == BatchStatus.FAILED) {\n //job failure\n }\n}\n----\n\nThe annotations corresponding to this interface are:\n\n* `@BeforeJob`\n* `@AfterJob`\n\n[[inheritingFromAParentJob]]\n[role=\"xmlContent\"]\n[[inheriting-from-a-parent-job]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc", "title": "configuring-job", "heading": "Intercepting Job Execution", "heading_level": 2, "file_order": 3, "section_index": 2, "content_hash": "ff13f2ffb02c15736443b08c5d429dad7f0b2682c453a26c1aa000b5e09e40f1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc"}}
{"id": "sha256:37983ea19ca57b872e29e3a03f998e34e958d5ee936164100a541eddc5ab1ac7", "content": "ifdef::backend-pdf[]\nThis section applies only to XML based configuration, as Java configuration provides better\nreuse capabilities.\nendif::backend-pdf[]\n\n[role=\"xmlContent\"]\nIf a group of Jobs share similar but not\nidentical configurations, it may help to define a \"`parent`\"\n`Job` from which the concrete\n`Job` instances can inherit properties. Similar to class\ninheritance in Java, a \"`child`\" `Job` combines\nits elements and attributes with the parent's.\n\n[role=\"xmlContent\"]\nIn the following example, `baseJob` is an abstract\n`Job` definition that defines only a list of\nlisteners. The `Job` (`job1`) is a concrete\ndefinition that inherits the list of listeners from `baseJob` and merges\nit with its own list of listeners to produce a\n`Job` with two listeners and one\n`Step` (`step1`).\n\n[source, xml]\n----\n<job id=\"baseJob\" abstract=\"true\">\n <listeners>\n <listener ref=\"listenerOne\"/>\n </listeners>\n</job>\n\n<job id=\"job1\" parent=\"baseJob\">\n <step id=\"step1\" parent=\"standaloneStep\"/>\n\n <listeners merge=\"true\">\n <listener ref=\"listenerTwo\"/>\n </listeners>\n</job>\n----\n\n[role=\"xmlContent\"]\nSee the section on xref:step/chunk-oriented-processing/inheriting-from-parent.adoc[Inheriting from a Parent Step]\nfor more detailed information.\n\n[[jobparametersvalidator]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc", "title": "configuring-job", "heading": "Inheriting from a Parent Job", "heading_level": 2, "file_order": 3, "section_index": 3, "content_hash": "37983ea19ca57b872e29e3a03f998e34e958d5ee936164100a541eddc5ab1ac7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc"}}
{"id": "sha256:bd6204b78bb07970cf23cc0ddc6c199bbc6f74c0a8d96be73784e13791420698", "content": "A job declared in the XML namespace or using any subclass of\n`AbstractJob` can optionally declare a validator for the job parameters at\nruntime. This is useful when, for instance, you need to assert that a job\nis started with all its mandatory parameters. There is a\n`DefaultJobParametersValidator` that you can use to constrain combinations\nof simple mandatory and optional parameters. For more complex\nconstraints, you can implement the interface yourself.\n\n[tabs]\n====\nJava::\n+\nThe configuration of a validator is supported through the Java builders:\n+\n[source, java]\n----\n@Bean\npublic Job job1(JobRepository jobRepository) {\n return new JobBuilder(\"job1\", jobRepository)\n .validator(parametersValidator())\n ...\n .build();\n}\n----\n\nXML::\n+\nThe configuration of a validator is supported through the XML namespace through a child\nelement of the job, as the following example shows:\n+\n[source, xml]\n----\n<job id=\"job1\" parent=\"baseJob3\">\n <step id=\"step1\" parent=\"standaloneStep\"/>\n <validator ref=\"parametersValidator\"/>\n</job>\n----\n+\nYou can specify the validator as a reference (as shown earlier) or as a nested bean\ndefinition in the `beans` namespace.\n\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc", "title": "configuring-job", "heading": "JobParametersValidator", "heading_level": 2, "file_order": 3, "section_index": 4, "content_hash": "bd6204b78bb07970cf23cc0ddc6c199bbc6f74c0a8d96be73784e13791420698", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-job.adoc"}}
{"id": "sha256:a2158a53fee75c0540ace1eb0c899cfe93475551d7705282f48b9c72e522b99a", "content": "[[configuringJobOperator]]\n\nThe most basic implementation of the `JobOperator` interface is the `TaskExecutorJobOperator`.\nIt requires only one dependency: a `JobRepository`. All other dependencies like `JobRegistry`,\n`MeterRegistry`, `TransactionManager`, etc are optional. Spring Batch provides a factory bean\nto simplify the configuration of this operator: `JobOperatorFactoryBean`. This factory bean\ncreates a transactional proxy around the `TaskExecutorJobOperator` to ensure that all its public methods\nare executed within a transaction.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to configure a `TaskExecutorJobOperator` in Java:\n+\n.Java Configuration\n[source, java]\n----\n...\n@Bean\npublic JobOperatorFactoryBean jobOperator(JobRepository jobRepository) {\n\tJobOperatorFactoryBean jobOperatorFactoryBean = new JobOperatorFactoryBean();\n\tjobOperatorFactoryBean.setJobRepository(jobRepository);\n\treturn jobOperatorFactoryBean;\n}\n...\n----\n\nXML::\n+\nThe following example shows how to configure a `TaskExecutorJobOperator` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"jobOperator\" class=\"org.springframework.batch.core.launch.support.JobOperatorFactoryBean\">\n <property name=\"jobRepository\" ref=\"jobRepository\" />\n</bean>\n----\n\n====\n\nOnce a xref:domain.adoc[JobExecution] is obtained, it is passed to the\nexecute method of `Job`, ultimately returning the `JobExecution` to the caller, as\nthe following image shows:\n\n.Job Launcher Sequence\nimage::job-launcher-sequence-sync.png[Job Launcher Sequence, scaledwidth=\"50%\"]\n\nThe sequence is straightforward and works well when launched from a scheduler. However,\nissues arise when trying to launch from an HTTP request. In this scenario, the launching\nneeds to be done asynchronously so that the `TaskExecutorJobOperator` returns immediately to its\ncaller. This is because it is not good practice to keep an HTTP request open for the\namount of time needed by long running processes (such as batch jobs). The following image shows\nan example sequence:\n\n.Asynchronous Job Launcher Sequence\nimage::job-launcher-sequence-async.png[Async Job Launcher Sequence, scaledwidth=\"50%\"]\n\nYou can configure the `TaskExecutorJobOperator` to allow for this scenario by configuring a\n`TaskExecutor`.\n\n[tabs]\n====\nJava::\n+\nThe following Java example configures a `TaskExecutorJobOperator` to return immediately:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic JobOperatorFactoryBean jobOperator(JobRepository jobRepository) {\n\tJobOperatorFactoryBean jobOperatorFactoryBean = new JobOperatorFactoryBean();\n\tjobOperatorFactoryBean.setJobRepository(jobRepository);\n\tjobOperatorFactoryBean.setTaskExecutor(new SimpleAsyncTaskExecutor());\n\treturn jobOperatorFactoryBean;\n}\n----\n\nXML::\n+\nThe following XML example configures a `TaskExecutorJobOperator` to return immediately:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"jobOperator\" class=\"org.springframework.batch.core.launch.support.JobOperatorFactoryBean\">\n <property name=\"jobRepository\" ref=\"jobRepository\" />\n <property name=\"taskExecutor\">\n <bean class=\"org.springframework.core.task.SimpleAsyncTaskExecutor\" />\n </property>\n</bean>\n----\n\n====\n\nYou can use any implementation of the Spring `TaskExecutor`\ninterface to control how jobs are asynchronously\nexecuted.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-operator.adoc", "title": "configuring-operator", "heading": "configuring-operator", "heading_level": 1, "file_order": 4, "section_index": 0, "content_hash": "a2158a53fee75c0540ace1eb0c899cfe93475551d7705282f48b9c72e522b99a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-operator.adoc"}}
{"id": "sha256:60bd31a067c15327960c55429fe929153a6ad7efd2027e02de177a31c900d3df", "content": "[[configuringJobRepository]]\n\nAs described the xref:job.adoc[earlier], the `JobRepository` is used for basic CRUD operations\nof the various persisted domain objects within Spring Batch, such as `JobExecution` and `StepExecution`.\nIt is required by many of the major framework features, such as the `JobOperator`,\n`Job`, and `Step`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc", "title": "configuring-repository", "heading": "configuring-repository", "heading_level": 1, "file_order": 5, "section_index": 0, "content_hash": "60bd31a067c15327960c55429fe929153a6ad7efd2027e02de177a31c900d3df", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc"}}
{"id": "sha256:25f85ddc0e27da1db07d23f7a5771e01a3be22f735ff11af5d5f74766abe196e", "content": "The simplest implementation of the `JobRepository` interface is the\n`ResourcelessJobRepository`. This implementation does not use or store batch meta-data.\nIt is intended for use-cases where restartability is not required and where the execution context\nis not involved in any way (like sharing data between steps through the execution context,\nor partitioned steps where partitions meta-data is shared between the manager and workers through\nthe execution context, etc). This implementation holds the minimal state to run a single job\n(ie 1 job instance + 1 job execution + N step executions). This is suitable for one-time jobs\nexecuted in their own JVM. This job repository works with transactional steps as well as non-transactional\nsteps (in conjunction with `ResourcelessTransactionManager`).\n\nIMPORTANT: This implementation is *not* thread-safe and should *not* be used in any concurrent environment.\n\nBy default, when using `@EnableBatchProcessing` or `DefaultBatchConfiguration`, a `ResourcelessJobRepository`\nis provided for you.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc", "title": "configuring-repository", "heading": "Configuring a Resourceless JobRepository", "heading_level": 2, "file_order": 5, "section_index": 1, "content_hash": "25f85ddc0e27da1db07d23f7a5771e01a3be22f735ff11af5d5f74766abe196e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc"}}
{"id": "sha256:cf15da0c61a70fe535c3fb17d6ef15c8c28db30d7cd891c03655d4352f09e297", "content": "[tabs]\n====\nJava::\n+\nWhen using `@EnableBatchProcessing`, a `ResourcelessJobRepository` is provided for you.\nThis section describes how to customize it. Spring Batch provides two implementations\nof the `JobRepository` interface which are backed by a database: a JDBC implementation\n(which can be used with any JDBC-compliant database) and a MongoDB implementation. These two\nimplementations are provided by the `@EnableJdbcJobRepository` and `@EnableMongoJobRepository`\nannotations, respectively.\n+\nThe following example shows how to customize a JDBC-based job repository through the attributes\nof the `@EnableJdbcJobRepository` annotation:\n+\n.Java Configuration\n[source, java]\n----\n@Configuration\n@EnableBatchProcessing\n@EnableJdbcJobRepository(\n dataSourceRef = \"batchDataSource\",\n transactionManagerRef = \"batchTransactionManager\",\n tablePrefix = \"BATCH_\",\n maxVarCharLength = 1000,\n isolationLevelForCreate = \"SERIALIZABLE\")\npublic class MyJobConfiguration {\n\n // job definition\n\n}\n----\n+\nNone of the configuration options listed here are required.\nIf they are not set, the defaults shown earlier are used.\nThe max `varchar` length defaults to `2500`, which is the\nlength of the long `VARCHAR` columns in the\nxref:schema-appendix.adoc#metaDataSchemaOverview[sample schema scripts]\n\nXML::\n+\nThe batch namespace abstracts away many of the implementation details of the\n`JobRepository` implementations and their collaborators. However, there are still a few\nconfiguration options available, as the following example shows:\n+\n.XML Configuration\n[source, xml]\n----\n<job-repository id=\"jobRepository\"\n data-source=\"dataSource\"\n transaction-manager=\"transactionManager\"\n isolation-level-for-create=\"SERIALIZABLE\"\n table-prefix=\"BATCH_\"\n\tmax-varchar-length=\"1000\"/>\n----\n+\nOther than the `id`, none of the configuration options listed earlier are required. If they are\nnot set, the defaults shown earlier are used.\nThe `max-varchar-length` defaults to `2500`, which is the length of the long\n`VARCHAR` columns in the xref:schema-appendix.adoc#metaDataSchemaOverview[sample schema scripts].\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc", "title": "configuring-repository", "heading": "Configuring a JDBC JobRepository", "heading_level": 2, "file_order": 5, "section_index": 2, "content_hash": "cf15da0c61a70fe535c3fb17d6ef15c8c28db30d7cd891c03655d4352f09e297", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc"}}
{"id": "sha256:bde3dadc30eb02f1b392abc19ee8ea544c63bd3476925025c0a52260c31cc530", "content": "Similar to the JDBC-based `JobRepository`, the MongoDB-based `JobRepository` requires some collections\nto store the batch metadata. These collections are defined in the `org/springframework/batch/core/schema-mongodb.jsonl`\nof the `spring-batch-core` jar. As with the JDBC-based `JobRepository`, you need to create these collections\nin your MongoDB database before running any job.\n\nMoreover, since it is https://www.mongodb.com/docs/manual/core/dot-dollar-considerations/[not recommended] to use `.` in\nfield names in MongoDB documents, you need to customize the `MongoTemplate` used by the `MongoJobRepositoryFactoryBean`\nto replace `.` with another character (for example, `_`) in the field names. You can do this by customizing the `MappingMongoConverter`\nused by the `MongoTemplate`. The following example shows how to do this in Java configuration:\n\n.Java Configuration\n[source, java]\n----\n@Bean\npublic MongoTemplate mongoTemplate(MongoDatabaseFactory mongoDatabaseFactory) {\n MongoTemplate template = new MongoTemplate(mongoDatabaseFactory);\n MappingMongoConverter converter = (MappingMongoConverter) template.getConverter();\n converter.setMapKeyDotReplacement(\"_\");\n return template;\n}\n----\n\n[[txConfigForJobRepository]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc", "title": "configuring-repository", "heading": "Configuring a MongoDB JobRepository", "heading_level": 2, "file_order": 5, "section_index": 3, "content_hash": "bde3dadc30eb02f1b392abc19ee8ea544c63bd3476925025c0a52260c31cc530", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc"}}
{"id": "sha256:e4a57d85e277a4e880e3ba334ad5cc70a7b8c3e6882d850ba37d4cc6541c85d9", "content": "If the namespace or the provided `FactoryBean` is used, transactional advice is\nautomatically created around the repository. This is to ensure that the batch metadata,\nincluding state that is necessary for restarts after a failure, is persisted correctly.\nThe behavior of the framework is not well defined if the repository methods are not\ntransactional. The isolation level in the `create*` method attributes is specified\nseparately to ensure that, when jobs are launched, if two processes try to launch\nthe same job at the same time, only one succeeds. The default isolation level for that\nmethod is `SERIALIZABLE`, which is quite aggressive. `READ_COMMITTED` usually works equally\nwell. `READ_UNCOMMITTED` is fine if two processes are not likely to collide in this\nway. However, since a call to the `create*` method is quite short, it is unlikely that\n`SERIALIZED` causes problems, as long as the database platform supports it. However, you\ncan override this setting.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to override the isolation level in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Configuration\n@EnableBatchProcessing\n@EnableJdbcJobRepository(isolationLevelForCreate = \"ISOLATION_REPEATABLE_READ\")\npublic class MyJobConfiguration {\n\n // job definition\n\n}\n----\n\nXML::\n+\nThe following example shows how to override the isolation level in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job-repository id=\"jobRepository\"\n isolation-level-for-create=\"REPEATABLE_READ\" />\n----\n====\n\nIf the namespace is not used, you must also configure the\ntransactional behavior of the repository by using AOP.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to configure the transactional behavior of the repository\nin Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic TransactionProxyFactoryBean baseProxy() {\n\tTransactionProxyFactoryBean transactionProxyFactoryBean = new TransactionProxyFactoryBean();\n\tProperties transactionAttributes = new Properties();\n\ttransactionAttributes.setProperty(\"*\", \"PROPAGATION_REQUIRED\");\n\ttransactionProxyFactoryBean.setTransactionAttributes(transactionAttributes);\n\ttransactionProxyFactoryBean.setTarget(jobRepository());\n\ttransactionProxyFactoryBean.setTransactionManager(transactionManager());\n\treturn transactionProxyFactoryBean;\n}\n----\n\nXML::\n+\nThe following example shows how to configure the transactional behavior of the repository\nin XML:\n+\n.XML Configuration\n[source, xml]\n----\n<aop:config>\n <aop:advisor\n pointcut=\"execution(* org.springframework.batch.core..*Repository+.*(..))\"/>\n <advice-ref=\"txAdvice\" />\n</aop:config>\n\n<tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\">\n <tx:attributes>\n <tx:method name=\"*\" />\n </tx:attributes>\n</tx:advice>\n----\n+\nYou can use the preceding fragment nearly as is, with almost no changes. Remember also to\ninclude the appropriate namespace declarations and to make sure `spring-tx` and `spring-aop`\n(or the whole of Spring) are on the classpath.\n====\n\n[[repositoryTablePrefix]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc", "title": "configuring-repository", "heading": "Transaction Configuration for the JobRepository", "heading_level": 2, "file_order": 5, "section_index": 4, "content_hash": "e4a57d85e277a4e880e3ba334ad5cc70a7b8c3e6882d850ba37d4cc6541c85d9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc"}}
{"id": "sha256:db2ba773b8568a9aacc790fe7375144a0eeb48de591429ef6c1782cca7309b9f", "content": "Another modifiable property of the `JobRepository` is the table prefix of the meta-data\ntables. By default, they are all prefaced with `BATCH_`. `BATCH_JOB_EXECUTION` and\n`BATCH_STEP_EXECUTION` are two examples. However, there are potential reasons to modify this\nprefix. If the schema names need to be prepended to the table names or if more than one\nset of metadata tables is needed within the same schema, the table prefix needs to\nbe changed.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to change the table prefix in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Configuration\n@EnableBatchProcessing\n@EnableJdbcJobRepository(tablePrefix = \"SYSTEM.TEST_\")\npublic class MyJobConfiguration {\n\n // job definition\n\n}\n----\n\nXML::\n+\nThe following example shows how to change the table prefix in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job-repository id=\"jobRepository\"\n table-prefix=\"SYSTEM.TEST_\" />\n----\n\n====\n\nGiven the preceding changes, every query to the metadata tables is prefixed with\n`SYSTEM.TEST_`. `BATCH_JOB_EXECUTION` is referred to as `SYSTEM.TEST_JOB_EXECUTION`.\n\nNOTE: Only the table prefix is configurable. The table and column names are not.\n\n[[nonStandardDatabaseTypesInRepository]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc", "title": "configuring-repository", "heading": "Changing the Table Prefix", "heading_level": 2, "file_order": 5, "section_index": 5, "content_hash": "db2ba773b8568a9aacc790fe7375144a0eeb48de591429ef6c1782cca7309b9f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc"}}
{"id": "sha256:ca83be1207c865547670603b5263486709397d7703f3390c127fc36229b685a9", "content": "If you use a database platform that is not in the list of supported platforms, you\nmay be able to use one of the supported types, if the SQL variant is close enough. To do\nthis, you can use the raw `JdbcJobRepositoryFactoryBean` instead of the namespace shortcut and\nuse it to set the database type to the closest match.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to use `JdbcJobRepositoryFactoryBean` to set the database type\nto the closest match in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic JobRepository jobRepository() throws Exception {\n JdbcJobRepositoryFactoryBean factory = new JdbcJobRepositoryFactoryBean();\n factory.setDataSource(dataSource);\n factory.setDatabaseType(\"db2\");\n factory.setTransactionManager(transactionManager);\n return factory.getObject();\n}\n----\n\nXML::\n+\nThe following example shows how to use `JdbcJobRepositoryFactoryBean` to set the database type\nto the closest match in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"jobRepository\" class=\"org...JdbcJobRepositoryFactoryBean\">\n <property name=\"databaseType\" value=\"db2\"/>\n <property name=\"dataSource\" ref=\"dataSource\"/>\n</bean>\n----\n\n====\n\nIf the database type is not specified, the `JdbcJobRepositoryFactoryBean` tries to\nauto-detect the database type from the `DataSource`.\nThe major differences between platforms are\nmainly accounted for by the strategy for incrementing primary keys, so\nit is often necessary to override the\n`incrementerFactory` as well (by using one of the standard\nimplementations from the Spring Framework).\n\nIf even that does not work or if you are not using an RDBMS, the\nonly option may be to implement the various `Dao`\ninterfaces that the `SimpleJobRepository` depends\non and wire one up manually in the normal Spring way.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc", "title": "configuring-repository", "heading": "Non-standard Database Types in a Repository", "heading_level": 2, "file_order": 5, "section_index": 6, "content_hash": "ca83be1207c865547670603b5263486709397d7703f3390c127fc36229b685a9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/configuring-repository.adoc"}}
{"id": "sha256:b60551e29b51f059a89d97b3590b11501fc1c39f11cb1b87d7c1e989e0053fcc", "content": "[[runningAJob]]\n\nAt a minimum, launching a batch job requires two things: the\n`Job` to be launched and a\n`JobOperator`. Both can be contained within the same\ncontext or different contexts. For example, if you launch jobs from the\ncommand line, a new JVM is instantiated for each `Job`. Thus, every\njob has its own `JobOperator`. However, if\nyou run from within a web container that is within the scope of an\n`HttpRequest`, there is usually one\n`JobOperator` (configured for asynchronous job\nlaunching) that multiple requests invoke to launch their jobs.\n\n[[runningJobsFromCommandLine]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/running.adoc", "title": "running", "heading": "running", "heading_level": 1, "file_order": 6, "section_index": 0, "content_hash": "b60551e29b51f059a89d97b3590b11501fc1c39f11cb1b87d7c1e989e0053fcc", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/running.adoc"}}
{"id": "sha256:dbb8c3f34144f7efd3c0fdf9ecb2fccb8e5a52de6977bb4553da2c52ebeeb5d5", "content": "If you want to run your jobs from an enterprise\nscheduler, the command line is the primary interface. This is because\nmost schedulers (with the exception of Quartz, unless using\n`NativeJob`) work directly with operating system\nprocesses, primarily kicked off with shell scripts. There are many ways\nto launch a Java process besides a shell script, such as Perl, Ruby, or\neven build tools, such as Ant or Maven. However, because most people\nare familiar with shell scripts, this example focuses on them.\n\n[[commandLineJobOperator]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/running.adoc", "title": "running", "heading": "Running Jobs from the Command Line", "heading_level": 2, "file_order": 6, "section_index": 1, "content_hash": "dbb8c3f34144f7efd3c0fdf9ecb2fccb8e5a52de6977bb4553da2c52ebeeb5d5", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/running.adoc"}}
{"id": "sha256:b8e0403b8c6f3dbdaedf336a9fb229599e0bd4dd6b64b3c73f7edf9b525d7941", "content": "Because the script launching the job must kick off a Java\nVirtual Machine, there needs to be a class with a `main` method to act\nas the primary entry point. Spring Batch provides an implementation\nthat serves this purpose:\n`CommandLineJobOperator`. Note\nthat this is just one way to bootstrap your application. There are\nmany ways to launch a Java process, and this class should in no way be\nviewed as definitive. The `CommandLineJobOperator`\nperforms four tasks:\n\n* Load the appropriate `ApplicationContext`.\n* Parse command line arguments into `JobParameters`.\n* Locate the appropriate job based on arguments.\n* Use the `JobOperator` provided in the application context to launch the job.\n\nAll of these tasks are accomplished with only the arguments passed in.\nThe following table describes the required arguments:\n\n.CommandLineJobOperator arguments\n|===============\n|`jobClass`|The fully qualified name of the job configuration class used to\ncreate an `ApplicationContext`. This file\nshould contain everything needed to run the complete\n`Job`, including a `JobOperator`, a `JobRepository` and a `JobRegistry` populated with the jobs to operate.\n|`operation`|The name of the operation to execute on the job. Can be one of [`start`, `startNextInstance`, `stop`, `restart`, `abandon`]\n|`jobName` or `jobExecutionId`|Depending on the operation, this can be the name of the job to start or the execution ID of the job to stop, restart, abandon or recover.\n|===============\n\nWhen starting a job, all arguments after these are considered to be job parameters, are turned into a `JobParameters` object,\nand must be in the format of `name=value,type,identifying`. In the case of stopping, restarting, abandoning or recovering a job, the `jobExecutionId` is\nexpected as the 4th argument, and all remaining arguments are ignored.\n\nThe following example shows a date passed as a job parameter to a job defined in Java:\n\n[source]\n----\n<bash$ java CommandLineJobOperator io.spring.EndOfDayJobConfiguration start endOfDay schedule.date=2007-05-05,java.time.LocalDate\n----\n\nBy default, the `CommandLineJobOperator` uses a `DefaultJobParametersConverter` that implicitly converts\nkey/value pairs to identifying job parameters. However, you can explicitly specify\nwhich job parameters are identifying and which are not by suffixing them with `true` or `false`, respectively.\n\nIn the following example, `schedule.date` is an identifying job parameter, while `vendor.id` is not:\n\n[source]\n----\n<bash$ java CommandLineJobOperator io.spring.EndOfDayJobConfiguration start endOfDay \\\n schedule.date=2007-05-05,java.time.LocalDate,true \\\n vendor.id=123,java.lang.Long,false\n----\n\nYou can override this behavior by setting a custom `JobParametersConverter` on the `CommandLineJobOperator`.\n\n[[exitCodes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/running.adoc", "title": "running", "heading": "The CommandLineJobOperator", "heading_level": 3, "file_order": 6, "section_index": 2, "content_hash": "b8e0403b8c6f3dbdaedf336a9fb229599e0bd4dd6b64b3c73f7edf9b525d7941", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/running.adoc"}}
{"id": "sha256:a67abf5cbe7912a4e6868eff9bafff4a5c2f0f2ecceb5b2fbc2196abee34e12f", "content": "When launching a batch job from the command-line, an enterprise\nscheduler is often used. Most schedulers are fairly dumb and work only\nat the process level. This means that they only know about some\noperating system process (such as a shell script that they invoke).\nIn this scenario, the only way to communicate back to the scheduler\nabout the success or failure of a job is through return codes. A\nreturn code is a number that is returned to a scheduler by the process\nto indicate the result of the run. In the simplest case, 0 is\nsuccess and 1 is failure. However, there may be more complex\nscenarios, such as \"`If job A returns 4, kick off job B, and, if it returns 5, kick\noff job C.`\" This type of behavior is configured at the scheduler level,\nbut it is important that a processing framework such as Spring Batch\nprovide a way to return a numeric representation of the exit code\nfor a particular batch job. In Spring Batch, this is encapsulated\nwithin an `ExitStatus`, which is covered in more\ndetail in Chapter 5. For the purposes of discussing exit codes, the\nonly important thing to know is that an\n`ExitStatus` has an exit code property that is\nset by the framework (or the developer) and is returned as part of the\n`JobExecution` returned from the `JobOperator`. The\n`CommandLineJobOperator` converts this string value\nto a number by using the `ExitCodeMapper`\ninterface:\n\n[source, java]\n----\npublic interface ExitCodeMapper {\n\n int intValue(String exitCode);\n\n}\n----\n\nThe essential contract of an `ExitCodeMapper` is that, given a string exit\ncode, a number representation will be returned. The default implementation\nused by the job runner is the `SimpleJvmExitCodeMapper`\nthat returns 0 for completion, 1 for generic errors, and 2 for any job\nrunner errors such as not being able to find a\n`Job` in the provided context. If anything more\ncomplex than the three values above is needed, a custom\nimplementation of the `ExitCodeMapper` interface\nmust be supplied by setting it on the `CommandLineJobOperator`.\n\n[[runningJobsFromWebContainer]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/running.adoc", "title": "running", "heading": "Exit Codes", "heading_level": 3, "file_order": 6, "section_index": 3, "content_hash": "a67abf5cbe7912a4e6868eff9bafff4a5c2f0f2ecceb5b2fbc2196abee34e12f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/running.adoc"}}
{"id": "sha256:329605ccc564082be3438df4be7e0ece3c511c4f9c8e00c7ad268552245aa5fa", "content": "Historically, offline processing (such as batch jobs) has been\nlaunched from the command-line, as described earlier. However, there are\nmany cases where launching from an `HttpRequest` is\na better option. Many such use cases include reporting, ad-hoc job\nrunning, and web application support. Because a batch job (by definition)\nis long running, the most important concern is to launch the\njob asynchronously:\n\n.Asynchronous Job Launcher Sequence From Web Container\nimage::launch-from-request.png[Async Job Launcher Sequence from web container, scaledwidth=\"60%\"]\n\nThe controller in this case is a Spring MVC controller. See the\nSpring Framework Reference Guide for more about https://docs.spring.io/spring/docs/current/spring-framework-reference/web.html#mvc[Spring MVC].\nThe controller launches a `Job` by using a\n`JobOperator` that has been configured to launch\nxref:job/running.adoc#runningJobsFromWebContainer[asynchronously], which\nimmediately returns a `JobExecution`. The\n`Job` is likely still running. However, this\nnonblocking behavior lets the controller return immediately, which\nis required when handling an `HttpRequest`. The following listing\nshows an example:\n\n[source, java]\n----\n@Controller\npublic class JobOperatorController {\n\n @Autowired\n JobOperator jobOperator;\n\n @Autowired\n Job job;\n\n @RequestMapping(\"/jobOperator.html\")\n public void handle() throws Exception{\n jobOperator.start(job, new JobParameters());\n }\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job/running.adoc", "title": "running", "heading": "Running Jobs from within a Web Container", "heading_level": 2, "file_order": 6, "section_index": 4, "content_hash": "329605ccc564082be3438df4be7e0ece3c511c4f9c8e00c7ad268552245aa5fa", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job/running.adoc"}}
{"id": "sha256:4181fcf577796acc98ff2d370d0b958dad13c09c0a31a21a0ad0c65c3eba97dc", "content": "[[fieldSet]]\n\nWhen working with flat files in Spring Batch, regardless of whether it is for input or\noutput, one of the most important classes is the `FieldSet`. Many architectures and\nlibraries contain abstractions for helping you read in from a file, but they usually\nreturn a `String` or an array of `String` objects. This really only gets you halfway\nthere. A `FieldSet` is Spring Batch's abstraction for enabling the binding of fields from\na file resource. It allows developers to work with file input in much the same way as\nthey would work with database input. A `FieldSet` is conceptually similar to a JDBC\n`ResultSet`. A `FieldSet` requires only one argument: a `String` array of tokens.\nOptionally, you can also configure the names of the fields so that the fields may be\naccessed either by index or name as patterned after `ResultSet`, as shown in the following\nexample:\n\n[source, java]\n----\nString[] tokens = new String[]{\"foo\", \"1\", \"true\"};\nFieldSet fs = new DefaultFieldSet(tokens);\nString name = fs.readString(0);\nint value = fs.readInt(1);\nboolean booleanValue = fs.readBoolean(2);\n----\n\nThere are many more options on the `FieldSet` interface, such as `Date`, long,\n`BigDecimal`, and so on. The biggest advantage of the `FieldSet` is that it provides\nconsistent parsing of flat file input. Rather than each batch job parsing differently in\npotentially unexpected ways, it can be consistent, both when handling errors caused by a\nformat exception, or when doing simple data conversions.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/field-set.adoc", "title": "field-set", "heading": "field-set", "heading_level": 1, "file_order": 7, "section_index": 0, "content_hash": "4181fcf577796acc98ff2d370d0b958dad13c09c0a31a21a0ad0c65c3eba97dc", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/field-set.adoc"}}
{"id": "sha256:e8ef64c90b4ddca238d104d9e20506025465c71c4093080c4456e26549a6f0d6", "content": "[[flatFileItemReader]]\n\nA flat file is any type of file that contains at most two-dimensional (tabular) data.\nReading flat files in the Spring Batch framework is facilitated by the class called\n`FlatFileItemReader`, which provides basic functionality for reading and parsing flat\nfiles. The two most important required dependencies of `FlatFileItemReader` are\n`Resource` and `LineMapper`. The `LineMapper` interface is explored more in the next\nsections. The resource property represents a Spring Core `Resource`. Documentation\nexplaining how to create beans of this type can be found in\nlink:$$https://docs.spring.io/spring/docs/current/spring-framework-reference/core.html#resources$$[Spring\nFramework, Chapter 5. Resources]. Therefore, this guide does not go into the details of\ncreating `Resource` objects beyond showing the following simple example:\n\n[source, java]\n----\nResource resource = new FileSystemResource(\"resources/trades.csv\");\n----\n\nIn complex batch environments, the directory structures are often managed by the Enterprise Application Integration (EAI)\ninfrastructure, where drop zones for external interfaces are established for moving files\nfrom FTP locations to batch processing locations and vice versa. File moving utilities\nare beyond the scope of the Spring Batch architecture, but it is not unusual for batch\njob streams to include file moving utilities as steps in the job stream. The batch\narchitecture only needs to know how to locate the files to be processed. Spring Batch\nbegins the process of feeding the data into the pipe from this starting point. However,\nlink:$$https://projects.spring.io/spring-integration/$$[Spring Integration] provides many\nof these types of services.\n\nThe other properties in `FlatFileItemReader` let you further specify how your data is\ninterpreted, as described in the following table:\n\n.`FlatFileItemReader` Properties\n[options=\"header\"]\n|===============\n|Property|Type|Description\n|comments|String[]|Specifies line prefixes that indicate comment rows.\n|encoding|String|Specifies what text encoding to use. The default value is `UTF-8`.\n|lineMapper|`LineMapper`|Converts a `String` to an `Object` representing the item.\n|linesToSkip|int|Number of lines to ignore at the top of the file.\n|recordSeparatorPolicy|RecordSeparatorPolicy|Used to determine where the line endings are\nand do things like continue over a line ending if inside a quoted string.\n|resource|`Resource`|The resource from which to read.\n|skippedLinesCallback|LineCallbackHandler|Interface that passes the raw line content of\nthe lines in the file to be skipped. If `linesToSkip` is set to 2, then this interface is\ncalled twice.\n|strict|boolean|In strict mode, the reader throws an exception on `ExecutionContext` if\nthe input resource does not exist. Otherwise, it logs the problem and continues.\n|===============\n\n[[lineMapper]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "file-item-reader", "heading_level": 1, "file_order": 8, "section_index": 0, "content_hash": "e8ef64c90b4ddca238d104d9e20506025465c71c4093080c4456e26549a6f0d6", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:53350397f8c76244699673d651d2a163347604e9bc62c8baea813e2a4c10c865", "content": "As with `RowMapper`, which takes a low-level construct such as `ResultSet` and returns\nan `Object`, flat file processing requires the same construct to convert a `String` line\ninto an `Object`, as shown in the following interface definition:\n\n[source, java]\n----\npublic interface LineMapper<T> {\n\n T mapLine(String line, int lineNumber) throws Exception;\n\n}\n----\n\nThe basic contract is that, given the current line and the line number with which it is\nassociated, the mapper should return a resulting domain object. This is similar to\n`RowMapper`, in that each line is associated with its line number, just as each row in a\n`ResultSet` is tied to its row number. This allows the line number to be tied to the\nresulting domain object for identity comparison or for more informative logging. However,\nunlike `RowMapper`, the `LineMapper` is given a raw line which, as discussed above, only\ngets you halfway there. The line must be tokenized into a `FieldSet`, which can then be\nmapped to an object, as described later in this document.\n\n[[lineTokenizer]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "`LineMapper`", "heading_level": 2, "file_order": 8, "section_index": 1, "content_hash": "53350397f8c76244699673d651d2a163347604e9bc62c8baea813e2a4c10c865", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:73495439daae8ce0c0ca67f2ffa929a82e37cfaf752db6992cc6233203a73d35", "content": "An abstraction for turning a line of input into a `FieldSet` is necessary because there\ncan be many formats of flat file data that need to be converted to a `FieldSet`. In\nSpring Batch, this interface is the `LineTokenizer`:\n\n[source, java]\n----\npublic interface LineTokenizer {\n\n FieldSet tokenize(String line);\n\n}\n----\n\nThe contract of a `LineTokenizer` is such that, given a line of input (in theory the\n`String` could encompass more than one line), a `FieldSet` representing the line is\nreturned. This `FieldSet` can then be passed to a `FieldSetMapper`. Spring Batch contains\nthe following `LineTokenizer` implementations:\n\n* `DelimitedLineTokenizer`: Used for files where fields in a record are separated by a\ndelimiter. The most common delimiter is a comma, but pipes or semicolons are often used\nas well.\n* `FixedLengthTokenizer`: Used for files where fields in a record are each a \"fixed\nwidth\". The width of each field must be defined for each record type.\n* `PatternMatchingCompositeLineTokenizer`: Determines which `LineTokenizer` among a list of\ntokenizers should be used on a particular line by checking against a pattern.\n\n[[fieldSetMapper]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "`LineTokenizer`", "heading_level": 2, "file_order": 8, "section_index": 2, "content_hash": "73495439daae8ce0c0ca67f2ffa929a82e37cfaf752db6992cc6233203a73d35", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:9673ca4fd0baa1f8eb9aea3a4ac2d690f4291950b900e466206ab889a892111f", "content": "The `FieldSetMapper` interface defines a single method, `mapFieldSet`, which takes a\n`FieldSet` object and maps its contents to an object. This object may be a custom DTO, a\ndomain object, or an array, depending on the needs of the job. The `FieldSetMapper` is\nused in conjunction with the `LineTokenizer` to translate a line of data from a resource\ninto an object of the desired type, as shown in the following interface definition:\n\n[source, java]\n----\npublic interface FieldSetMapper<T> {\n\n T mapFieldSet(FieldSet fieldSet) throws BindException;\n\n}\n----\n\nThe pattern used is the same as the `RowMapper` used by `JdbcTemplate`.\n\n[[defaultLineMapper]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "`FieldSetMapper`", "heading_level": 2, "file_order": 8, "section_index": 3, "content_hash": "9673ca4fd0baa1f8eb9aea3a4ac2d690f4291950b900e466206ab889a892111f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:925c2fc61df4bb77450987bfd66d8c3595d3ed31a35799c0eca8ae949a05fb3c", "content": "Now that the basic interfaces for reading in flat files have been defined, it becomes\nclear that three basic steps are required:\n\n. Read one line from the file.\n. Pass the `String` line into the `LineTokenizer#tokenize()` method to retrieve a\n`FieldSet`.\n. Pass the `FieldSet` returned from tokenizing to a `FieldSetMapper`, returning the\nresult from the `ItemReader#read()` method.\n\nThe two interfaces described above represent two separate tasks: converting a line into a\n`FieldSet` and mapping a `FieldSet` to a domain object. Because the input of a\n`LineTokenizer` matches the input of the `LineMapper` (a line), and the output of a\n`FieldSetMapper` matches the output of the `LineMapper`, a default implementation that\nuses both a `LineTokenizer` and a `FieldSetMapper` is provided. The `DefaultLineMapper`,\nshown in the following class definition, represents the behavior most users need:\n\n[source, java]\n----\n\npublic class DefaultLineMapper<T> implements LineMapper<>, InitializingBean {\n\n private LineTokenizer tokenizer;\n\n private FieldSetMapper<T> fieldSetMapper;\n\n public T mapLine(String line, int lineNumber) throws Exception {\n return fieldSetMapper.mapFieldSet(tokenizer.tokenize(line));\n }\n\n public void setLineTokenizer(LineTokenizer tokenizer) {\n this.tokenizer = tokenizer;\n }\n\n public void setFieldSetMapper(FieldSetMapper<T> fieldSetMapper) {\n this.fieldSetMapper = fieldSetMapper;\n }\n}\n----\n\nThe above functionality is provided in a default implementation, rather than being built\ninto the reader itself (as was done in previous versions of the framework) to allow users\ngreater flexibility in controlling the parsing process, especially if access to the raw\nline is needed.\n\n[[simpleDelimitedFileReadingExample]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "`DefaultLineMapper`", "heading_level": 2, "file_order": 8, "section_index": 4, "content_hash": "925c2fc61df4bb77450987bfd66d8c3595d3ed31a35799c0eca8ae949a05fb3c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:3321ddbd10663e78c8a0f230eea20dbb9563cef1521c2a02f9ed7dda58dbace7", "content": "The following example illustrates how to read a flat file with an actual domain scenario.\nThis particular batch job reads in football players from the following file:\n\n----\nID,lastName,firstName,position,birthYear,debutYear\n\"AbduKa00,Abdul-Jabbar,Karim,rb,1974,1996\",\n\"AbduRa00,Abdullah,Rabih,rb,1975,1999\",\n\"AberWa00,Abercrombie,Walter,rb,1959,1982\",\n\"AbraDa00,Abramowicz,Danny,wr,1945,1967\",\n\"AdamBo00,Adams,Bob,te,1946,1969\",\n\"AdamCh00,Adams,Charlie,wr,1979,2003\"\n----\n\nThe contents of this file are mapped to the following\n`Player` domain object:\n\n[source, java]\n----\npublic class Player implements Serializable {\n\n private String ID;\n private String lastName;\n private String firstName;\n private String position;\n private int birthYear;\n private int debutYear;\n\n public String toString() {\n return \"PLAYER:ID=\" + ID + \",Last Name=\" + lastName +\n \",First Name=\" + firstName + \",Position=\" + position +\n \",Birth Year=\" + birthYear + \",DebutYear=\" +\n debutYear;\n }\n\n // setters and getters...\n}\n----\n\nTo map a `FieldSet` into a `Player` object, a `FieldSetMapper` that returns players needs\nto be defined, as shown in the following example:\n\n[source, java]\n----\nprotected static class PlayerFieldSetMapper implements FieldSetMapper<Player> {\n public Player mapFieldSet(FieldSet fieldSet) {\n Player player = new Player();\n\n player.setID(fieldSet.readString(0));\n player.setLastName(fieldSet.readString(1));\n player.setFirstName(fieldSet.readString(2));\n player.setPosition(fieldSet.readString(3));\n player.setBirthYear(fieldSet.readInt(4));\n player.setDebutYear(fieldSet.readInt(5));\n\n return player;\n }\n}\n----\n\nThe file can then be read by correctly constructing a `FlatFileItemReader` and calling\n`read`, as shown in the following example:\n\n[source, java]\n----\nFlatFileItemReader<Player> itemReader = new FlatFileItemReader<>();\nitemReader.setResource(new FileSystemResource(\"resources/players.csv\"));\nDefaultLineMapper<Player> lineMapper = new DefaultLineMapper<>();\nlineMapper.setLineTokenizer(new DelimitedLineTokenizer());\nlineMapper.setFieldSetMapper(new PlayerFieldSetMapper());\nitemReader.setLineMapper(lineMapper);\nitemReader.open(new ExecutionContext());\nPlayer player = itemReader.read();\n----\n\nEach call to `read` returns a new\n `Player` object from each line in the file. When the end of the file is\n reached, `null` is returned.\n\n[[mappingFieldsByName]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "Simple Delimited File Reading Example", "heading_level": 2, "file_order": 8, "section_index": 5, "content_hash": "3321ddbd10663e78c8a0f230eea20dbb9563cef1521c2a02f9ed7dda58dbace7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:0f488d7866bb5129b57632e227dc1ffa295dc5dd31e3a871919d43f4f6b7b96f", "content": "There is one additional piece of functionality that is allowed by both\n`DelimitedLineTokenizer` and `FixedLengthTokenizer` and that is similar in function to a\nJDBC `ResultSet`. The names of the fields can be injected into either of these\n`LineTokenizer` implementations to increase the readability of the mapping function.\nFirst, the column names of all fields in the flat file are injected into the tokenizer,\nas shown in the following example:\n\n[source, java]\n----\ntokenizer.setNames(new String[] {\"ID\", \"lastName\", \"firstName\", \"position\", \"birthYear\", \"debutYear\"});\n----\n\nA `FieldSetMapper` can use this information as follows:\n\n[source, java]\n----\npublic class PlayerMapper implements FieldSetMapper<Player> {\n public Player mapFieldSet(FieldSet fs) {\n\n if (fs == null) {\n return null;\n }\n\n Player player = new Player();\n player.setID(fs.readString(\"ID\"));\n player.setLastName(fs.readString(\"lastName\"));\n player.setFirstName(fs.readString(\"firstName\"));\n player.setPosition(fs.readString(\"position\"));\n player.setDebutYear(fs.readInt(\"debutYear\"));\n player.setBirthYear(fs.readInt(\"birthYear\"));\n\n return player;\n }\n}\n----\n\n[[beanWrapperFieldSetMapper]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "Mapping Fields by Name", "heading_level": 2, "file_order": 8, "section_index": 6, "content_hash": "0f488d7866bb5129b57632e227dc1ffa295dc5dd31e3a871919d43f4f6b7b96f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:9a251629dd5e23d801c6fe8548c86615ea0baf5d1595bee757679a2e13340a27", "content": "For many, having to write a specific `FieldSetMapper` is equally as cumbersome as writing\na specific `RowMapper` for a `JdbcTemplate`. Spring Batch makes this easier by providing\na `FieldSetMapper` that automatically maps fields by matching a field name with a setter\non the object using the JavaBean specification.\n\n[tabs]\n====\nJava::\n+\nAgain using the football example, the `BeanWrapperFieldSetMapper` configuration looks like\nthe following snippet in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FieldSetMapper fieldSetMapper() {\n\tBeanWrapperFieldSetMapper fieldSetMapper = new BeanWrapperFieldSetMapper();\n\n\tfieldSetMapper.setPrototypeBeanName(\"player\");\n\n\treturn fieldSetMapper;\n}\n\n@Bean\n@Scope(\"prototype\")\npublic Player player() {\n\treturn new Player();\n}\n----\n\nXML::\n+\nAgain using the football example, the `BeanWrapperFieldSetMapper` configuration looks like\nthe following snippet in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"fieldSetMapper\"\n class=\"org.springframework.batch.infrastructure.item.file.mapping.BeanWrapperFieldSetMapper\">\n <property name=\"prototypeBeanName\" value=\"player\" />\n</bean>\n\n<bean id=\"player\"\n class=\"org.springframework.batch.samples.domain.Player\"\n scope=\"prototype\" />\n----\n\n====\n\nFor each entry in the `FieldSet`, the mapper looks for a corresponding setter on a new\ninstance of the `Player` object (for this reason, prototype scope is required) in the\nsame way the Spring container looks for setters matching a property name. Each available\nfield in the `FieldSet` is mapped, and the resultant `Player` object is returned, with no\ncode required.\n\n[[fixedLengthFileFormats]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "Automapping FieldSets to Domain Objects", "heading_level": 2, "file_order": 8, "section_index": 7, "content_hash": "9a251629dd5e23d801c6fe8548c86615ea0baf5d1595bee757679a2e13340a27", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:9952af1f15d67317953a78a5b7f20644b5aea5c9d48b9b8c4ef1a2d08a246edd", "content": "So far, only delimited files have been discussed in much detail. However, they represent\nonly half of the file reading picture. Many organizations that use flat files use fixed\nlength formats. An example fixed length file follows:\n\n----\nUK21341EAH4121131.11customer1\nUK21341EAH4221232.11customer2\nUK21341EAH4321333.11customer3\nUK21341EAH4421434.11customer4\nUK21341EAH4521535.11customer5\n----\n\nWhile this looks like one large field, it actually represent 4 distinct fields:\n\n. ISIN: Unique identifier for the item being ordered - 12 characters long.\n. Quantity: Number of the item being ordered - 3 characters long.\n. Price: Price of the item - 5 characters long.\n. Customer: ID of the customer ordering the item - 9 characters long.\n\nWhen configuring the `FixedLengthLineTokenizer`, each of these lengths must be provided\nin the form of ranges.\n\n[tabs]\n=====\nJava::\n+\nThe following example shows how to define ranges for the `FixedLengthLineTokenizer` in\nJava:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FixedLengthTokenizer fixedLengthTokenizer() {\n\tFixedLengthTokenizer tokenizer = new FixedLengthTokenizer();\n\n\ttokenizer.setNames(\"ISIN\", \"Quantity\", \"Price\", \"Customer\");\n\ttokenizer.setColumns(new Range(1, 12),\n new Range(13, 15),\n new Range(16, 20),\n new Range(21, 29));\n\n\treturn tokenizer;\n}\n----\n\nXML::\n+\nThe following example shows how to define ranges for the `FixedLengthLineTokenizer` in\nXML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"fixedLengthLineTokenizer\"\n class=\"org.springframework.batch.infrastructure.item.file.transform.FixedLengthTokenizer\">\n <property name=\"names\" value=\"ISIN,Quantity,Price,Customer\" />\n <property name=\"columns\" value=\"1-12, 13-15, 16-20, 21-29\" />\n</bean>\n----\n+\nBecause the `FixedLengthLineTokenizer` uses the same `LineTokenizer` interface as\ndiscussed earlier, it returns the same `FieldSet` as if a delimiter had been used. This\nallows the same approaches to be used in handling its output, such as using the\n`BeanWrapperFieldSetMapper`.\n+\n[NOTE]\n====\nSupporting the preceding syntax for ranges requires that a specialized property editor,\n`RangeArrayPropertyEditor`, be configured in the `ApplicationContext`. However, this bean\nis automatically declared in an `ApplicationContext` where the batch namespace is used.\n====\n\n=====\n\nBecause the `FixedLengthLineTokenizer` uses the same `LineTokenizer` interface as\ndiscussed above, it returns the same `FieldSet` as if a delimiter had been used. This\nlets the same approaches be used in handling its output, such as using the\n`BeanWrapperFieldSetMapper`.\n\n[[prefixMatchingLineMapper]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "Fixed Length File Formats", "heading_level": 2, "file_order": 8, "section_index": 8, "content_hash": "9952af1f15d67317953a78a5b7f20644b5aea5c9d48b9b8c4ef1a2d08a246edd", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:25bf5c70dd199b4771e24e4bbeac770ecfc0c5d71b4caad96a3b0c5d6a696138", "content": "All of the file reading examples up to this point have all made a key assumption for\nsimplicity's sake: all of the records in a file have the same format. However, this may\nnot always be the case. It is very common that a file might have records with different\nformats that need to be tokenized differently and mapped to different objects. The\nfollowing excerpt from a file illustrates this:\n\n----\nUSER;Smith;Peter;;T;20014539;F\nLINEA;1044391041ABC037.49G201XX1383.12H\nLINEB;2134776319DEF422.99M005LI\n----\n\nIn this file we have three types of records, \"USER\", \"LINEA\", and \"LINEB\". A \"USER\" line\ncorresponds to a `User` object. \"LINEA\" and \"LINEB\" both correspond to `Line` objects,\nthough a \"LINEA\" has more information than a \"LINEB\".\n\nThe `ItemReader` reads each line individually, but we must specify different\n`LineTokenizer` and `FieldSetMapper` objects so that the `ItemWriter` receives the\ncorrect items. The `PatternMatchingCompositeLineMapper` makes this easy by allowing maps\nof patterns to `LineTokenizers` and patterns to `FieldSetMappers` to be configured.\n\n[tabs]\n====\nJava::\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic PatternMatchingCompositeLineMapper orderFileLineMapper() {\n\tPatternMatchingCompositeLineMapper lineMapper =\n new PatternMatchingCompositeLineMapper();\n\n\tMap<String, LineTokenizer> tokenizers = new HashMap<>(3);\n\ttokenizers.put(\"USER*\", userTokenizer());\n\ttokenizers.put(\"LINEA*\", lineATokenizer());\n\ttokenizers.put(\"LINEB*\", lineBTokenizer());\n\n\tlineMapper.setTokenizers(tokenizers);\n\n\tMap<String, FieldSetMapper> mappers = new HashMap<>(2);\n\tmappers.put(\"USER*\", userFieldSetMapper());\n\tmappers.put(\"LINE*\", lineFieldSetMapper());\n\n\tlineMapper.setFieldSetMappers(mappers);\n\n\treturn lineMapper;\n}\n----\n\nXML::\n+\nThe following example shows how to define ranges for the `FixedLengthLineTokenizer` in\nXML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"orderFileLineMapper\"\n class=\"org.spr...PatternMatchingCompositeLineMapper\">\n <property name=\"tokenizers\">\n <map>\n <entry key=\"USER*\" value-ref=\"userTokenizer\" />\n <entry key=\"LINEA*\" value-ref=\"lineATokenizer\" />\n <entry key=\"LINEB*\" value-ref=\"lineBTokenizer\" />\n </map>\n </property>\n <property name=\"fieldSetMappers\">\n <map>\n <entry key=\"USER*\" value-ref=\"userFieldSetMapper\" />\n <entry key=\"LINE*\" value-ref=\"lineFieldSetMapper\" />\n </map>\n </property>\n</bean>\n----\n\n====\n\nIn this example, \"LINEA\" and \"LINEB\" have separate `LineTokenizer` instances, but they both use\nthe same `FieldSetMapper`.\n\nThe `PatternMatchingCompositeLineMapper` uses the `PatternMatcher#match` method\nin order to select the correct delegate for each line. The `PatternMatcher` allows for\ntwo wildcard characters with special meaning: the question mark (\"?\") matches exactly one\ncharacter, while the asterisk (\"\\*\") matches zero or more characters. Note that, in the\npreceding configuration, all patterns end with an asterisk, making them effectively\nprefixes to lines. The `PatternMatcher` always matches the most specific pattern\npossible, regardless of the order in the configuration. So if \"LINE*\" and \"LINEA*\" were\nboth listed as patterns, \"LINEA\" would match pattern \"LINEA*\", while \"LINEB\" would match\npattern \"LINE*\". Additionally, a single asterisk (\"*\") can serve as a default by matching\nany line not matched by any other pattern.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to match a line not matched by any other pattern in Java:\n+\n.Java Configuration\n[source, java]\n----\n...\ntokenizers.put(\"*\", defaultLineTokenizer());\n...\n----\n\nXML::\n+\nThe following example shows how to match a line not matched by any other pattern in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<entry key=\"*\" value-ref=\"defaultLineTokenizer\" />\n----\n\n====\n\nThere is also a `PatternMatchingCompositeLineTokenizer` that can be used for tokenization\nalone.\n\nIt is also common for a flat file to contain records that each span multiple lines. To\nhandle this situation, a more complex strategy is required. A demonstration of this\ncommon pattern can be found in the `multiLineRecords` sample.\n\n[[exceptionHandlingInFlatFiles]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "Multiple Record Types within a Single File", "heading_level": 2, "file_order": 8, "section_index": 9, "content_hash": "25bf5c70dd199b4771e24e4bbeac770ecfc0c5d71b4caad96a3b0c5d6a696138", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:13645b88dbe1f89b755a3a4aa8e0096d70802c9a2b6d2dfafef8bb2a7ef385d3", "content": "There are many scenarios when tokenizing a line may cause exceptions to be thrown. Many\nflat files are imperfect and contain incorrectly formatted records. Many users choose to\nskip these erroneous lines while logging the issue, the original line, and the line\nnumber. These logs can later be inspected manually or by another batch job. For this\nreason, Spring Batch provides a hierarchy of exceptions for handling parse exceptions:\n`FlatFileParseException` and `FlatFileFormatException`. `FlatFileParseException` is\nthrown by the `FlatFileItemReader` when any errors are encountered while trying to read a\nfile. `FlatFileFormatException` is thrown by implementations of the `LineTokenizer`\ninterface and indicates a more specific error encountered while tokenizing.\n\n[[incorrectTokenCountException]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "Exception Handling in Flat Files", "heading_level": 2, "file_order": 8, "section_index": 10, "content_hash": "13645b88dbe1f89b755a3a4aa8e0096d70802c9a2b6d2dfafef8bb2a7ef385d3", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:b4ba3b5e7475982359b57cdd77faf80a242994133eefc3e6d22ca824e8663b35", "content": "Both `DelimitedLineTokenizer` and `FixedLengthLineTokenizer` have the ability to specify\ncolumn names that can be used for creating a `FieldSet`. However, if the number of column\nnames does not match the number of columns found while tokenizing a line, the `FieldSet`\ncannot be created, and an `IncorrectTokenCountException` is thrown, which contains the\nnumber of tokens encountered, and the number expected, as shown in the following example:\n\n[source, java]\n----\ntokenizer.setNames(new String[] {\"A\", \"B\", \"C\", \"D\"});\n\ntry {\n tokenizer.tokenize(\"a,b,c\");\n}\ncatch (IncorrectTokenCountException e) {\n assertEquals(4, e.getExpectedCount());\n assertEquals(3, e.getActualCount());\n}\n----\n\nBecause the tokenizer was configured with 4 column names but only 3 tokens were found in\nthe file, an `IncorrectTokenCountException` was thrown.\n\n[[incorrectLineLengthException]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "`IncorrectTokenCountException`", "heading_level": 3, "file_order": 8, "section_index": 11, "content_hash": "b4ba3b5e7475982359b57cdd77faf80a242994133eefc3e6d22ca824e8663b35", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:682866380edcb74d6a4e1b06beb2eaedcca207d08c8461f6345454feee92544c", "content": "Files formatted in a fixed-length format have additional requirements when parsing\nbecause, unlike a delimited format, each column must strictly adhere to its predefined\nwidth. If the total line length does not equal the widest value of this column, an\nexception is thrown, as shown in the following example:\n\n[source, java]\n----\ntokenizer.setColumns(new Range[] { new Range(1, 5),\n new Range(6, 10),\n new Range(11, 15) });\ntry {\n tokenizer.tokenize(\"12345\");\n fail(\"Expected IncorrectLineLengthException\");\n}\ncatch (IncorrectLineLengthException ex) {\n assertEquals(15, ex.getExpectedLength());\n assertEquals(5, ex.getActualLength());\n}\n----\n\nThe configured ranges for the tokenizer above are: 1-5, 6-10, and 11-15. Consequently,\nthe total length of the line is 15. However, in the preceding example, a line of length 5\nwas passed in, causing an `IncorrectLineLengthException` to be thrown. Throwing an\nexception here rather than only mapping the first column allows the processing of the\nline to fail earlier and with more information than it would contain if it failed while\ntrying to read in column 2 in a `FieldSetMapper`. However, there are scenarios where the\nlength of the line is not always constant. For this reason, validation of line length can\nbe turned off via the 'strict' property, as shown in the following example:\n\n[source, java]\n----\ntokenizer.setColumns(new Range[] { new Range(1, 5), new Range(6, 10) });\ntokenizer.setStrict(false);\nFieldSet tokens = tokenizer.tokenize(\"12345\");\nassertEquals(\"12345\", tokens.readString(0));\nassertEquals(\"\", tokens.readString(1));\n----\n\nThe preceding example is almost identical to the one before it, except that\n`tokenizer.setStrict(false)` was called. This setting tells the tokenizer to not enforce\nline lengths when tokenizing the line. A `FieldSet` is now correctly created and\nreturned. However, it contains only empty tokens for the remaining values.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc", "title": "file-item-reader", "heading": "`IncorrectLineLengthException`", "heading_level": 3, "file_order": 8, "section_index": 12, "content_hash": "682866380edcb74d6a4e1b06beb2eaedcca207d08c8461f6345454feee92544c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-reader.adoc"}}
{"id": "sha256:e82e4add7744ab468d85d17ef15cba4b5f5228b4a2b03a08a1fb9309b89da6e1", "content": "[[flatFileItemWriter]]\n\nWriting out to flat files has the same problems and issues that reading in from a file\nmust overcome. A step must be able to write either delimited or fixed length formats in a\ntransactional manner.\n\n[[lineAggregator]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "file-item-writer", "heading_level": 1, "file_order": 9, "section_index": 0, "content_hash": "e82e4add7744ab468d85d17ef15cba4b5f5228b4a2b03a08a1fb9309b89da6e1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:06041f822b4a65a24e515f0948e11277aa7e5af80953ea583c8098bf1b0dd156", "content": "Just as the `LineTokenizer` interface is necessary to take an item and turn it into a\n`String`, file writing must have a way to aggregate multiple fields into a single string\nfor writing to a file. In Spring Batch, this is the `LineAggregator`, shown in the\nfollowing interface definition:\n\n[source, java]\n----\npublic interface LineAggregator<T> {\n\n public String aggregate(T item);\n\n}\n----\n\nThe `LineAggregator` is the logical opposite of `LineTokenizer`. `LineTokenizer` takes a\n`String` and returns a `FieldSet`, whereas `LineAggregator` takes an `item` and returns a\n`String`.\n\n[[PassThroughLineAggregator]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "`LineAggregator`", "heading_level": 2, "file_order": 9, "section_index": 1, "content_hash": "06041f822b4a65a24e515f0948e11277aa7e5af80953ea583c8098bf1b0dd156", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:132c5bcb59a75d9dab0f5bdc50d83566b1da04d8d706ecaa8ac4b6ebca6f7c4c", "content": "The most basic implementation of the `LineAggregator` interface is the\n`PassThroughLineAggregator`, which assumes that the object is already a string or that\nits string representation is acceptable for writing, as shown in the following code:\n\n[source, java]\n----\npublic class PassThroughLineAggregator<T> implements LineAggregator<T> {\n\n public String aggregate(T item) {\n return item.toString();\n }\n}\n----\n\nThe preceding implementation is useful if direct control of creating the string is\nrequired but the advantages of a `FlatFileItemWriter`, such as transaction and restart\nsupport, are necessary.\n\n[[SimplifiedFileWritingExample]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "`PassThroughLineAggregator`", "heading_level": 3, "file_order": 9, "section_index": 2, "content_hash": "132c5bcb59a75d9dab0f5bdc50d83566b1da04d8d706ecaa8ac4b6ebca6f7c4c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:a7e7eaf6466bd302ff26792987b8eb4ceda8d027421e2b103c9803b4c8ebc2dc", "content": "Now that the `LineAggregator` interface and its most basic implementation,\n`PassThroughLineAggregator`, have been defined, the basic flow of writing can be\nexplained:\n\n. The object to be written is passed to the `LineAggregator` in order to obtain a\n`String`.\n. The returned `String` is written to the configured file.\n\nThe following excerpt from the `FlatFileItemWriter` expresses this in code:\n\n[source, java]\n----\npublic void write(T item) throws Exception {\n write(lineAggregator.aggregate(item) + LINE_SEPARATOR);\n}\n----\n\n[tabs]\n====\nJava::\n+\nIn Java, a simple example of configuration might look like the following:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FlatFileItemWriter itemWriter() {\n\treturn new FlatFileItemWriterBuilder<Foo>()\n .name(\"itemWriter\")\n .resource(new FileSystemResource(\"target/test-outputs/output.txt\"))\n .lineAggregator(new PassThroughLineAggregator<>())\n .build();\n}\n----\n\nXML::\n+\nIn XML, a simple example of configuration might look like the following:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"itemWriter\" class=\"org.spr...FlatFileItemWriter\">\n <property name=\"resource\" value=\"file:target/test-outputs/output.txt\" />\n <property name=\"lineAggregator\">\n <bean class=\"org.spr...PassThroughLineAggregator\"/>\n </property>\n</bean>\n----\n\n====\n\n[[FieldExtractor]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "Simplified File Writing Example", "heading_level": 2, "file_order": 9, "section_index": 3, "content_hash": "a7e7eaf6466bd302ff26792987b8eb4ceda8d027421e2b103c9803b4c8ebc2dc", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:c6c38165a9fa531be0bfbe3ea8a37a2d3069a668dc1819109e5e3d48bfb2f5fd", "content": "The preceding example may be useful for the most basic uses of a writing to a file.\nHowever, most users of the `FlatFileItemWriter` have a domain object that needs to be\nwritten out and, thus, must be converted into a line. In file reading, the following was\nrequired:\n\n. Read one line from the file.\n. Pass the line into the `LineTokenizer#tokenize()` method, in order to retrieve a\n`FieldSet`.\n. Pass the `FieldSet` returned from tokenizing to a `FieldSetMapper`, returning the\nresult from the `ItemReader#read()` method.\n\nFile writing has similar but inverse steps:\n\n. Pass the item to be written to the writer.\n. Convert the fields on the item into an array.\n. Aggregate the resulting array into a line.\n\nBecause there is no way for the framework to know which fields from the object need to\nbe written out, a `FieldExtractor` must be written to accomplish the task of turning the\nitem into an array, as shown in the following interface definition:\n\n[source, java]\n----\npublic interface FieldExtractor<T> {\n\n Object[] extract(T item);\n\n}\n----\n\nImplementations of the `FieldExtractor` interface should create an array from the fields\nof the provided object, which can then be written out with a delimiter between the\nelements or as part of a fixed-width line.\n\n[[PassThroughFieldExtractor]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "`FieldExtractor`", "heading_level": 2, "file_order": 9, "section_index": 4, "content_hash": "c6c38165a9fa531be0bfbe3ea8a37a2d3069a668dc1819109e5e3d48bfb2f5fd", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:cafcb37c8ccc4e79fbeed86c1ed4285f51ba67ce54dfc37c7832ae6fc3a52c9c", "content": "There are many cases where a collection, such as an array, `Collection`, or `FieldSet`,\nneeds to be written out. \"Extracting\" an array from one of these collection types is very\nstraightforward. To do so, convert the collection to an array. Therefore, the\n`PassThroughFieldExtractor` should be used in this scenario. It should be noted that, if\nthe object passed in is not a type of collection, then the `PassThroughFieldExtractor`\nreturns an array containing solely the item to be extracted.\n\n[[BeanWrapperFieldExtractor]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "`PassThroughFieldExtractor`", "heading_level": 3, "file_order": 9, "section_index": 5, "content_hash": "cafcb37c8ccc4e79fbeed86c1ed4285f51ba67ce54dfc37c7832ae6fc3a52c9c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:06cea4b142848857b439d678e4e43b7e3622172c4347bc8fa6c6d6279544906f", "content": "As with the `BeanWrapperFieldSetMapper` described in the file reading section, it is\noften preferable to configure how to convert a domain object to an object array, rather\nthan writing the conversion yourself. The `BeanWrapperFieldExtractor` provides this\nfunctionality, as shown in the following example:\n\n[source, java]\n----\nBeanWrapperFieldExtractor<Name> extractor = new BeanWrapperFieldExtractor<>();\nextractor.setNames(new String[] { \"first\", \"last\", \"born\" });\n\nString first = \"Alan\";\nString last = \"Turing\";\nint born = 1912;\n\nName n = new Name(first, last, born);\nObject[] values = extractor.extract(n);\n\nassertEquals(first, values[0]);\nassertEquals(last, values[1]);\nassertEquals(born, values[2]);\n----\n\nThis extractor implementation has only one required property: the names of the fields to\nmap. Just as the `BeanWrapperFieldSetMapper` needs field names to map fields on the\n`FieldSet` to setters on the provided object, the `BeanWrapperFieldExtractor` needs names\nto map to getters for creating an object array. It is worth noting that the order of the\nnames determines the order of the fields within the array.\n\n[[delimitedFileWritingExample]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "`BeanWrapperFieldExtractor`", "heading_level": 3, "file_order": 9, "section_index": 6, "content_hash": "06cea4b142848857b439d678e4e43b7e3622172c4347bc8fa6c6d6279544906f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:9c9e21fa57a53024d28482df6c8ae5d2c8b13e781a8c6008a2cb9caa2473cd79", "content": "The most basic flat file format is one in which all fields are separated by a delimiter.\nThis can be accomplished using a `DelimitedLineAggregator`. The following example writes\nout a simple domain object that represents a credit to a customer account:\n\n[source, java]\n----\npublic class CustomerCredit {\n\n private int id;\n private String name;\n private BigDecimal credit;\n\n //getters and setters removed for clarity\n}\n----\n\nBecause a domain object is being used, an implementation of the `FieldExtractor`\ninterface must be provided, along with the delimiter to use.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to use the `FieldExtractor` with a delimiter in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FlatFileItemWriter<CustomerCredit> itemWriter(Resource outputResource) throws Exception {\n\tBeanWrapperFieldExtractor<CustomerCredit> fieldExtractor = new BeanWrapperFieldExtractor<>();\n\tfieldExtractor.setNames(new String[] {\"name\", \"credit\"});\n\tfieldExtractor.afterPropertiesSet();\n\n\tDelimitedLineAggregator<CustomerCredit> lineAggregator = new DelimitedLineAggregator<>();\n\tlineAggregator.setDelimiter(\",\");\n\tlineAggregator.setFieldExtractor(fieldExtractor);\n\n\treturn new FlatFileItemWriterBuilder<CustomerCredit>()\n .name(\"customerCreditWriter\")\n .resource(outputResource)\n .lineAggregator(lineAggregator)\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to use the `FieldExtractor` with a delimiter in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"itemWriter\" class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemWriter\">\n <property name=\"resource\" ref=\"outputResource\" />\n <property name=\"lineAggregator\">\n <bean class=\"org.spr...DelimitedLineAggregator\">\n <property name=\"delimiter\" value=\",\"/>\n <property name=\"fieldExtractor\">\n <bean class=\"org.spr...BeanWrapperFieldExtractor\">\n <property name=\"names\" value=\"name,credit\"/>\n </bean>\n </property>\n </bean>\n </property>\n</bean>\n----\n\n====\n\nIn the previous example, the `BeanWrapperFieldExtractor` described earlier in this\nchapter is used to turn the name and credit fields within `CustomerCredit` into an object\narray, which is then written out with commas between each field.\n\n[tabs]\n====\nJava::\n+\nIt is also possible to use the `FlatFileItemWriterBuilder.DelimitedBuilder` to\nautomatically create the `BeanWrapperFieldExtractor` and `DelimitedLineAggregator`\nas shown in the following example:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FlatFileItemWriter<CustomerCredit> itemWriter(Resource outputResource) throws Exception {\n\treturn new FlatFileItemWriterBuilder<CustomerCredit>()\n .name(\"customerCreditWriter\")\n .resource(outputResource)\n .delimited()\n .delimiter(\"|\")\n .names(new String[] {\"name\", \"credit\"})\n .build();\n}\n----\n\nXML::\n+\n+\nThere is no XML equivalent of using `FlatFileItemWriterBuilder`.\n====\n\n[[fixedWidthFileWritingExample]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "Delimited File Writing Example", "heading_level": 2, "file_order": 9, "section_index": 7, "content_hash": "9c9e21fa57a53024d28482df6c8ae5d2c8b13e781a8c6008a2cb9caa2473cd79", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:a02ccdd42ea58baf7edaaea24643c4d002d2aff8e6875523d77b1a84cc546d9a", "content": "Delimited is not the only type of flat file format. Many prefer to use a set width for\neach column to delineate between fields, which is usually referred to as 'fixed width'.\nSpring Batch supports this in file writing with the `FormatterLineAggregator`.\n\n[tabs]\n====\nJava::\n+\nUsing the same `CustomerCredit` domain object described above, it can be configured as\nfollows in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FlatFileItemWriter<CustomerCredit> itemWriter(Resource outputResource) throws Exception {\n\tBeanWrapperFieldExtractor<CustomerCredit> fieldExtractor = new BeanWrapperFieldExtractor<>();\n\tfieldExtractor.setNames(new String[] {\"name\", \"credit\"});\n\tfieldExtractor.afterPropertiesSet();\n\n\tFormatterLineAggregator<CustomerCredit> lineAggregator = new FormatterLineAggregator<>();\n\tlineAggregator.setFormat(\"%-9s%-2.0f\");\n\tlineAggregator.setFieldExtractor(fieldExtractor);\n\n\treturn new FlatFileItemWriterBuilder<CustomerCredit>()\n .name(\"customerCreditWriter\")\n .resource(outputResource)\n .lineAggregator(lineAggregator)\n .build();\n}\n----\n\nXML::\n+\nUsing the same `CustomerCredit` domain object described above, it can be configured as\nfollows in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"itemWriter\" class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemWriter\">\n <property name=\"resource\" ref=\"outputResource\" />\n <property name=\"lineAggregator\">\n <bean class=\"org.spr...FormatterLineAggregator\">\n <property name=\"fieldExtractor\">\n <bean class=\"org.spr...BeanWrapperFieldExtractor\">\n <property name=\"names\" value=\"name,credit\" />\n </bean>\n </property>\n <property name=\"format\" value=\"%-9s%-2.0f\" />\n </bean>\n </property>\n</bean>\n----\n\n====\n\nMost of the preceding example should look familiar. However, the value of the format\nproperty is new.\n\n[tabs]\n====\nJava::\n+\nThe following example shows the format property in Java:\n+\n[source, java]\n----\n...\nFormatterLineAggregator<CustomerCredit> lineAggregator = new FormatterLineAggregator<>();\nlineAggregator.setFormat(\"%-9s%-2.0f\");\n...\n----\n\nXML::\n+\nThe following example shows the format property in XML:\n+\n[source, xml]\n----\n<property name=\"format\" value=\"%-9s%-2.0f\" />\n----\n\n====\n\nThe underlying implementation is built using the same\n`Formatter` added as part of Java 5. The Java\n`Formatter` is based on the\n`printf` functionality of the C programming\nlanguage. Most details on how to configure a formatter can be found in\nthe Javadoc of link:$$https://docs.oracle.com/javase/8/docs/api/java/util/Formatter.html$$[Formatter].\n\n[tabs]\n====\nJava::\n+\nIt is also possible to use the `FlatFileItemWriterBuilder.FormattedBuilder` to\nautomatically create the `BeanWrapperFieldExtractor` and `FormatterLineAggregator`\nas shown in following example:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FlatFileItemWriter<CustomerCredit> itemWriter(Resource outputResource) throws Exception {\n\treturn new FlatFileItemWriterBuilder<CustomerCredit>()\n .name(\"customerCreditWriter\")\n .resource(outputResource)\n .formatted()\n .format(\"%-9s%-2.0f\")\n .names(new String[] {\"name\", \"credit\"})\n .build();\n}\n----\n\nXML::\n+\n\n====\n\n[[handlingFileCreation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "Fixed Width File Writing Example", "heading_level": 2, "file_order": 9, "section_index": 8, "content_hash": "a02ccdd42ea58baf7edaaea24643c4d002d2aff8e6875523d77b1a84cc546d9a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:5cf95edc58985cfec2d89cab39b86accf9eb152e3d9248e30016f61a40a67957", "content": "`FlatFileItemReader` has a very simple relationship with file resources. When the reader\nis initialized, it opens the file (if it exists), and throws an exception if it does not.\nFile writing isn't quite so simple. At first glance, it seems like a similar\nstraightforward contract should exist for `FlatFileItemWriter`: If the file already\nexists, throw an exception, and, if it does not, create it and start writing. However,\npotentially restarting a `Job` can cause issues. In normal restart scenarios, the\ncontract is reversed: If the file exists, start writing to it from the last known good\nposition, and, if it does not, throw an exception. However, what happens if the file name\nfor this job is always the same? In this case, you would want to delete the file if it\nexists, unless it's a restart. Because of this possibility, the `FlatFileItemWriter`\ncontains the property, `shouldDeleteIfExists`. Setting this property to true causes an\nexisting file with the same name to be deleted when the writer is opened.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc", "title": "file-item-writer", "heading": "Handling File Creation", "heading_level": 2, "file_order": 9, "section_index": 9, "content_hash": "5cf95edc58985cfec2d89cab39b86accf9eb152e3d9248e30016f61a40a67957", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files/file-item-writer.adoc"}}
{"id": "sha256:c52a1666f9f1dd0f7fd5314aed9de585ee1e441f5be662b6dd3662cfe3073916", "content": "[[customReadersWriters]]\n\nSo far, this chapter has discussed the basic contracts of reading and writing in Spring\nBatch and some common implementations for doing so. However, these are all fairly\ngeneric, and there are many potential scenarios that may not be covered by out-of-the-box\nimplementations. This section shows, by using a simple example, how to create a custom\n`ItemReader` and `ItemWriter` implementation and implement their contracts correctly. The\n`ItemReader` also implements `ItemStream`, in order to illustrate how to make a reader or\nwriter restartable.\n\n[[customReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc", "title": "custom", "heading": "custom", "heading_level": 1, "file_order": 10, "section_index": 0, "content_hash": "c52a1666f9f1dd0f7fd5314aed9de585ee1e441f5be662b6dd3662cfe3073916", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc"}}
{"id": "sha256:52e0ca5860a76e41f4ae53e56921d0db1bf9075ce6dc0d2809c8f687e4a2d7aa", "content": "For the purpose of this example, we create a simple `ItemReader` implementation that\nreads from a provided list. We start by implementing the most basic contract of\n`ItemReader`, the `read` method, as shown in the following code:\n\n[source, java]\n----\npublic class CustomItemReader<T> implements ItemReader<T> {\n\n List<T> items;\n\n public CustomItemReader(List<T> items) {\n this.items = items;\n }\n\n public T read() throws Exception, UnexpectedInputException,\n NonTransientResourceException, ParseException {\n\n if (!items.isEmpty()) {\n return items.remove(0);\n }\n return null;\n }\n}\n----\n\nThe preceding class takes a list of items and returns them one at a time, removing each\nfrom the list. When the list is empty, it returns `null`, thus satisfying the most basic\nrequirements of an `ItemReader`, as illustrated in the following test code:\n\n[source, java]\n----\nList<String> items = new ArrayList<>();\nitems.add(\"1\");\nitems.add(\"2\");\nitems.add(\"3\");\n\nItemReader itemReader = new CustomItemReader<>(items);\nassertEquals(\"1\", itemReader.read());\nassertEquals(\"2\", itemReader.read());\nassertEquals(\"3\", itemReader.read());\nassertNull(itemReader.read());\n----\n\n[[restartableReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc", "title": "custom", "heading": "Custom `ItemReader` Example", "heading_level": 2, "file_order": 10, "section_index": 1, "content_hash": "52e0ca5860a76e41f4ae53e56921d0db1bf9075ce6dc0d2809c8f687e4a2d7aa", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc"}}
{"id": "sha256:68f65db16b1d5418294e4e20a35ec6132bd33ebb772615c0b79e3b26bde483e4", "content": "The final challenge is to make the `ItemReader` restartable. Currently, if processing is\ninterrupted and begins again, the `ItemReader` must start at the beginning. This is\nactually valid in many scenarios, but it is sometimes preferable that a batch job\nrestarts where it left off. The key discriminant is often whether the reader is stateful\nor stateless. A stateless reader does not need to worry about restartability, but a\nstateful one has to try to reconstitute its last known state on restart. For this reason,\nwe recommend that you keep custom readers stateless if possible, so you need not worry\nabout restartability.\n\nIf you do need to store state, then the `ItemStream` interface should be used:\n\n[source, java]\n----\npublic class CustomItemReader<T> implements ItemReader<T>, ItemStream {\n\n List<T> items;\n int currentIndex = 0;\n private static final String CURRENT_INDEX = \"current.index\";\n\n public CustomItemReader(List<T> items) {\n this.items = items;\n }\n\n public T read() throws Exception, UnexpectedInputException,\n ParseException, NonTransientResourceException {\n\n if (currentIndex < items.size()) {\n return items.get(currentIndex++);\n }\n\n return null;\n }\n\n public void open(ExecutionContext executionContext) throws ItemStreamException {\n if (executionContext.containsKey(CURRENT_INDEX)) {\n currentIndex = new Long(executionContext.getLong(CURRENT_INDEX)).intValue();\n }\n else {\n currentIndex = 0;\n }\n }\n\n public void update(ExecutionContext executionContext) throws ItemStreamException {\n executionContext.putLong(CURRENT_INDEX, new Long(currentIndex).longValue());\n }\n\n public void close() throws ItemStreamException {}\n}\n----\n\nOn each call to the `ItemStream` `update` method, the current index of the `ItemReader`\nis stored in the provided `ExecutionContext` with a key of 'current.index'. When the\n`ItemStream` `open` method is called, the `ExecutionContext` is checked to see if it\ncontains an entry with that key. If the key is found, then the current index is moved to\nthat location. This is a fairly trivial example, but it still meets the general contract:\n\n[source, java]\n----\nExecutionContext executionContext = new ExecutionContext();\n((ItemStream)itemReader).open(executionContext);\nassertEquals(\"1\", itemReader.read());\n((ItemStream)itemReader).update(executionContext);\n\nList<String> items = new ArrayList<>();\nitems.add(\"1\");\nitems.add(\"2\");\nitems.add(\"3\");\nitemReader = new CustomItemReader<>(items);\n\n((ItemStream)itemReader).open(executionContext);\nassertEquals(\"2\", itemReader.read());\n----\n\nMost `ItemReaders` have much more sophisticated restart logic. The\n`JdbcCursorItemReader`, for example, stores the row ID of the last processed row in the\ncursor.\n\nIt is also worth noting that the key used within the `ExecutionContext` should not be\ntrivial. That is because the same `ExecutionContext` is used for all `ItemStreams` within\na `Step`. In most cases, simply prepending the key with the class name should be enough\nto guarantee uniqueness. However, in the rare cases where two of the same type of\n`ItemStream` are used in the same step (which can happen if two files are needed for\noutput), a more unique name is needed. For this reason, many of the Spring Batch\n`ItemReader` and `ItemWriter` implementations have a `setName()` property that lets this\nkey name be overridden.\n\n[[customWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc", "title": "custom", "heading": "Making the `ItemReader` Restartable", "heading_level": 3, "file_order": 10, "section_index": 2, "content_hash": "68f65db16b1d5418294e4e20a35ec6132bd33ebb772615c0b79e3b26bde483e4", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc"}}
{"id": "sha256:228e2085ee15b8bb11b709cf25ff7cc3e6cfadb942d1ab283da6947613592219", "content": "Implementing a Custom `ItemWriter` is similar in many ways to the `ItemReader` example\nabove but differs in enough ways as to warrant its own example. However, adding\nrestartability is essentially the same, so it is not covered in this example. As with the\n`ItemReader` example, a `List` is used in order to keep the example as simple as\npossible:\n\n[source, java]\n----\npublic class CustomItemWriter<T> implements ItemWriter<T> {\n\n List<T> output = TransactionAwareProxyFactory.createTransactionalList();\n\n public void write(Chunk<? extends T> items) throws Exception {\n output.addAll(items);\n }\n\n public List<T> getOutput() {\n return output;\n }\n}\n----\n\n[[restartableWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc", "title": "custom", "heading": "Custom `ItemWriter` Example", "heading_level": 2, "file_order": 10, "section_index": 3, "content_hash": "228e2085ee15b8bb11b709cf25ff7cc3e6cfadb942d1ab283da6947613592219", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc"}}
{"id": "sha256:422652871aadf41f3ad9aa3a4b80b81cee43bfa436845229d10df915b57a84f7", "content": "To make the `ItemWriter` restartable, we would follow the same process as for the\n`ItemReader`, adding and implementing the `ItemStream` interface to synchronize the\nexecution context. In the example, we might have to count the number of items processed\nand add that as a footer record. If we needed to do that, we could implement\n`ItemStream` in our `ItemWriter` so that the counter was reconstituted from the execution\ncontext if the stream was re-opened.\n\nIn many realistic cases, custom `ItemWriters` also delegate to another writer that itself\nis restartable (for example, when writing to a file), or else it writes to a\ntransactional resource and so does not need to be restartable, because it is stateless.\nWhen you have a stateful writer you should probably be sure to implement `ItemStream` as\nwell as `ItemWriter`. Remember also that the client of the writer needs to be aware of\nthe `ItemStream`, so you may need to register it as a stream in the configuration.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc", "title": "custom", "heading": "Making the `ItemWriter` Restartable", "heading_level": 3, "file_order": 10, "section_index": 4, "content_hash": "422652871aadf41f3ad9aa3a4b80b81cee43bfa436845229d10df915b57a84f7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/custom.adoc"}}
{"id": "sha256:be8dd66819583a3ff862060a8391fa9c0b5c5fb0f7c12f0d28e6cee282e695ac", "content": "[[database]]\n\nLike most enterprise application styles, a database is the central storage mechanism for\nbatch. However, batch differs from other application styles due to the sheer size of the\ndatasets with which the system must work. If a SQL statement returns 1 million rows, the\nresult set probably holds all returned results in memory until all rows have been read.\nSpring Batch provides two types of solutions for this problem:\n\n* xref:readers-and-writers/database.adoc#cursorBasedItemReaders[Cursor-based `ItemReader` Implementations]\n* xref:readers-and-writers/database.adoc#pagingItemReaders[Paging `ItemReader` Implementations]\n\n[[cursorBasedItemReaders]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc", "title": "database", "heading": "database", "heading_level": 1, "file_order": 11, "section_index": 0, "content_hash": "be8dd66819583a3ff862060a8391fa9c0b5c5fb0f7c12f0d28e6cee282e695ac", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc"}}
{"id": "sha256:259c078d1e2d61727413a06e698cc992319eafbe6b02307fa60f9039b05deefa", "content": "Using a database cursor is generally the default approach of most batch developers,\nbecause it is the database's solution to the problem of 'streaming' relational data. The\nJava `ResultSet` class is essentially an object oriented mechanism for manipulating a\ncursor. A `ResultSet` maintains a cursor to the current row of data. Calling `next` on a\n`ResultSet` moves this cursor to the next row. The Spring Batch cursor-based `ItemReader`\nimplementation opens a cursor on initialization and moves the cursor forward one row for\nevery call to `read`, returning a mapped object that can be used for processing. The\n`close` method is then called to ensure all resources are freed up. The Spring core\n`JdbcTemplate` gets around this problem by using the callback pattern to completely map\nall rows in a `ResultSet` and close before returning control back to the method caller.\nHowever, in batch, this must wait until the step is complete. The following image shows a\ngeneric diagram of how a cursor-based `ItemReader` works. Note that, while the example\nuses SQL (because SQL is so widely known), any technology could implement the basic\napproach.\n\n.Cursor Example\nimage::cursorExample.png[Cursor Example, scaledwidth=\"60%\"]\n\nThis example illustrates the basic pattern. Given a 'FOO' table, which has three columns:\n`ID`, `NAME`, and `BAR`, select all rows with an ID greater than 1 but less than 7. This\nputs the beginning of the cursor (row 1) on ID 2. The result of this row should be a\ncompletely mapped `Foo` object. Calling `read()` again moves the cursor to the next row,\nwhich is the `Foo` with an ID of 3. The results of these reads are written out after each\n`read`, allowing the objects to be garbage collected (assuming no instance variables are\nmaintaining references to them).\n\n[[JdbcCursorItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc", "title": "database", "heading": "Cursor-based `ItemReader` Implementations", "heading_level": 2, "file_order": 11, "section_index": 1, "content_hash": "259c078d1e2d61727413a06e698cc992319eafbe6b02307fa60f9039b05deefa", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc"}}
{"id": "sha256:644820c125c818ff961fd84bacdd028fe4c8bd3878b097d2c3613ee0d9d73d3b", "content": "`JdbcCursorItemReader` is the JDBC implementation of the cursor-based technique. It works\ndirectly with a `ResultSet` and requires an SQL statement to run against a connection\nobtained from a `DataSource`. The following database schema is used as an example:\n\n[source, sql]\n----\nCREATE TABLE CUSTOMER (\n ID BIGINT IDENTITY PRIMARY KEY,\n NAME VARCHAR(45),\n CREDIT FLOAT\n);\n----\n\nMany people prefer to use a domain object for each row, so the following example uses an\nimplementation of the `RowMapper` interface to map a `CustomerCredit` object:\n\n[source, java]\n----\npublic class CustomerCreditRowMapper implements RowMapper<CustomerCredit> {\n\n public static final String ID_COLUMN = \"id\";\n public static final String NAME_COLUMN = \"name\";\n public static final String CREDIT_COLUMN = \"credit\";\n\n public CustomerCredit mapRow(ResultSet rs, int rowNum) throws SQLException {\n CustomerCredit customerCredit = new CustomerCredit();\n\n customerCredit.setId(rs.getInt(ID_COLUMN));\n customerCredit.setName(rs.getString(NAME_COLUMN));\n customerCredit.setCredit(rs.getBigDecimal(CREDIT_COLUMN));\n\n return customerCredit;\n }\n}\n----\n\nBecause `JdbcCursorItemReader` shares key interfaces with `JdbcTemplate`, it is useful to\nsee an example of how to read in this data with `JdbcTemplate`, in order to contrast it\nwith the `ItemReader`. For the purposes of this example, assume there are 1,000 rows in\nthe `CUSTOMER` database. The first example uses `JdbcTemplate`:\n\n[source, java]\n----\nJdbcTemplate jdbcTemplate = new JdbcTemplate(dataSource);\nList customerCredits = jdbcTemplate.query(\"SELECT ID, NAME, CREDIT from CUSTOMER\",\n new CustomerCreditRowMapper());\n----\n\nAfter running the preceding code snippet, the `customerCredits` list contains 1,000\n`CustomerCredit` objects. In the query method, a connection is obtained from the\n`DataSource`, the provided SQL is run against it, and the `mapRow` method is called for\neach row in the `ResultSet`. Contrast this with the approach of the\n`JdbcCursorItemReader`, shown in the following example:\n\n[source, java]\n----\nJdbcCursorItemReader itemReader = new JdbcCursorItemReader();\nitemReader.setDataSource(dataSource);\nitemReader.setSql(\"SELECT ID, NAME, CREDIT from CUSTOMER\");\nitemReader.setRowMapper(new CustomerCreditRowMapper());\nint counter = 0;\nExecutionContext executionContext = new ExecutionContext();\nitemReader.open(executionContext);\nObject customerCredit = new Object();\nwhile(customerCredit != null){\n customerCredit = itemReader.read();\n counter++;\n}\nitemReader.close();\n----\n\nAfter running the preceding code snippet, the counter equals 1,000. If the code above had\nput the returned `customerCredit` into a list, the result would have been exactly the\nsame as with the `JdbcTemplate` example. However, the big advantage of the `ItemReader`\nis that it allows items to be 'streamed'. The `read` method can be called once, the item\ncan be written out by an `ItemWriter`, and then the next item can be obtained with\n`read`. This allows item reading and writing to be done in 'chunks' and committed\nperiodically, which is the essence of high performance batch processing. Furthermore, it\nis easily configured for injection into a Spring Batch `Step`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to inject an `ItemReader` into a `Step` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic JdbcCursorItemReader<CustomerCredit> itemReader() {\n\treturn new JdbcCursorItemReaderBuilder<CustomerCredit>()\n .dataSource(this.dataSource)\n .name(\"creditReader\")\n .sql(\"select ID, NAME, CREDIT from CUSTOMER\")\n .rowMapper(new CustomerCreditRowMapper())\n .build();\n\n}\n----\n\nXML::\n+\nThe following example shows how to inject an `ItemReader` into a `Step` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"itemReader\" class=\"org.spr...JdbcCursorItemReader\">\n <property name=\"dataSource\" ref=\"dataSource\"/>\n <property name=\"sql\" value=\"select ID, NAME, CREDIT from CUSTOMER\"/>\n <property name=\"rowMapper\">\n <bean class=\"org.springframework.batch.samples.domain.CustomerCreditRowMapper\"/>\n </property>\n</bean>\n----\n\n====\n\n[[JdbcCursorItemReaderProperties]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc", "title": "database", "heading": "`JdbcCursorItemReader`", "heading_level": 3, "file_order": 11, "section_index": 2, "content_hash": "644820c125c818ff961fd84bacdd028fe4c8bd3878b097d2c3613ee0d9d73d3b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc"}}
{"id": "sha256:e741000067527f6ccbac6669cfaf756945bc0ebbc1763625e0da663fda13b9e7", "content": "Because there are so many varying options for opening a cursor in Java, there are many\nproperties on the `JdbcCursorItemReader` that can be set, as described in the following\ntable:\n\n.JdbcCursorItemReader Properties\n\n|===============\n|ignoreWarnings|Determines whether or not SQLWarnings are logged or cause an exception.\nThe default is `true` (meaning that warnings are logged).\n|fetchSize|Gives the JDBC driver a hint as to the number of rows that should be fetched\nfrom the database when more rows are needed by the `ResultSet` object used by the\n`ItemReader`. By default, no hint is given.\n|maxRows|Sets the limit for the maximum number of rows the underlying `ResultSet` can\nhold at any one time.\n|queryTimeout|Sets the number of seconds the driver waits for a `Statement` object to\nrun. If the limit is exceeded, a `DataAccessException` is thrown. (Consult your driver\nvendor documentation for details).\n|verifyCursorPosition|Because the same `ResultSet` held by the `ItemReader` is passed to\nthe `RowMapper`, it is possible for users to call `ResultSet.next()` themselves, which\ncould cause issues with the reader's internal count. Setting this value to `true` causes\nan exception to be thrown if the cursor position is not the same after the `RowMapper`\ncall as it was before.\n|saveState|Indicates whether or not the reader's state should be saved in the\n`ExecutionContext` provided by `ItemStream#update(ExecutionContext)`. The default is\n`true`.\n|driverSupportsAbsolute|Indicates whether the JDBC driver supports\nsetting the absolute row on a `ResultSet`. It is recommended that this is set to `true`\nfor JDBC drivers that support `ResultSet.absolute()`, as it may improve performance,\nespecially if a step fails while working with a large data set. Defaults to `false`.\n|setUseSharedExtendedConnection| Indicates whether the connection\nused for the cursor should be used by all other processing, thus sharing the same\ntransaction. If this is set to `false`, then the cursor is opened with its own connection\nand does not participate in any transactions started for the rest of the step processing.\nIf you set this flag to `true` then you must wrap the DataSource in an\n`ExtendedConnectionDataSourceProxy` to prevent the connection from being closed and\nreleased after each commit. When you set this option to `true`, the statement used to\nopen the cursor is created with both 'READ_ONLY' and 'HOLD_CURSORS_OVER_COMMIT' options.\nThis allows holding the cursor open over transaction start and commits performed in the\nstep processing. To use this feature, you need a database that supports this and a JDBC\ndriver supporting JDBC 3.0 or later. Defaults to `false`.\n|===============\n\n[[StoredProcedureItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc", "title": "database", "heading": "Additional Properties", "heading_level": 4, "file_order": 11, "section_index": 3, "content_hash": "e741000067527f6ccbac6669cfaf756945bc0ebbc1763625e0da663fda13b9e7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc"}}
{"id": "sha256:9287c06db84bb7009d972d16227c3a1e6d6da1d11379a78380b1a2f9ecf7ae78", "content": "Sometimes it is necessary to obtain the cursor data by using a stored procedure. The\n`StoredProcedureItemReader` works like the `JdbcCursorItemReader`, except that, instead\nof running a query to obtain a cursor, it runs a stored procedure that returns a cursor.\nThe stored procedure can return the cursor in three different ways:\n\n* As a returned `ResultSet` (used by SQL Server, Sybase, DB2, Derby, and MySQL).\n* As a ref-cursor returned as an out parameter (used by Oracle and PostgreSQL).\n* As the return value of a stored function call.\n\n[tabs]\n====\nJava::\n+\nThe following Java example configuration uses the same 'customer credit' example as\nearlier examples:\n+\n.Java Configuration\n[source, xml]\n----\n@Bean\npublic StoredProcedureItemReader reader(DataSource dataSource) {\n\tStoredProcedureItemReader reader = new StoredProcedureItemReader();\n\n\treader.setDataSource(dataSource);\n\treader.setProcedureName(\"sp_customer_credit\");\n\treader.setRowMapper(new CustomerCreditRowMapper());\n\n\treturn reader;\n}\n----\n\nXML::\n+\nThe following XML example configuration uses the same 'customer credit' example as earlier\nexamples:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"reader\" class=\"o.s.batch.item.database.StoredProcedureItemReader\">\n <property name=\"dataSource\" ref=\"dataSource\"/>\n <property name=\"procedureName\" value=\"sp_customer_credit\"/>\n <property name=\"rowMapper\">\n <bean class=\"org.springframework.batch.samples.domain.CustomerCreditRowMapper\"/>\n </property>\n</bean>\n----\n====\n\nThe preceding example relies on the stored procedure to provide a `ResultSet` as a\nreturned result (option 1 from earlier).\n\nIf the stored procedure returned a `ref-cursor` (option 2), then we would need to provide\nthe position of the out parameter that is the returned `ref-cursor`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to work with the first parameter being a ref-cursor in\nJava:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic StoredProcedureItemReader reader(DataSource dataSource) {\n\tStoredProcedureItemReader reader = new StoredProcedureItemReader();\n\n\treader.setDataSource(dataSource);\n\treader.setProcedureName(\"sp_customer_credit\");\n\treader.setRowMapper(new CustomerCreditRowMapper());\n\treader.setRefCursorPosition(1);\n\n\treturn reader;\n}\n----\n\nXML::\n+\nThe following example shows how to work with the first parameter being a ref-cursor in\nXML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"reader\" class=\"o.s.batch.item.database.StoredProcedureItemReader\">\n <property name=\"dataSource\" ref=\"dataSource\"/>\n <property name=\"procedureName\" value=\"sp_customer_credit\"/>\n <property name=\"refCursorPosition\" value=\"1\"/>\n <property name=\"rowMapper\">\n <bean class=\"org.springframework.batch.samples.domain.CustomerCreditRowMapper\"/>\n </property>\n</bean>\n----\n====\n\nIf the cursor was returned from a stored function (option 3), we would need to set the\nproperty \"[maroon]#function#\" to `true`. It defaults to `false`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows property to `true` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic StoredProcedureItemReader reader(DataSource dataSource) {\n\tStoredProcedureItemReader reader = new StoredProcedureItemReader();\n\n\treader.setDataSource(dataSource);\n\treader.setProcedureName(\"sp_customer_credit\");\n\treader.setRowMapper(new CustomerCreditRowMapper());\n\treader.setFunction(true);\n\n\treturn reader;\n}\n----\n\nXML::\n+\nThe following example shows property to `true` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"reader\" class=\"o.s.batch.item.database.StoredProcedureItemReader\">\n <property name=\"dataSource\" ref=\"dataSource\"/>\n <property name=\"procedureName\" value=\"sp_customer_credit\"/>\n <property name=\"function\" value=\"true\"/>\n <property name=\"rowMapper\">\n <bean class=\"org.springframework.batch.samples.domain.CustomerCreditRowMapper\"/>\n </property>\n</bean>\n----\n====\n\nIn all of these cases, we need to define a `RowMapper` as well as a `DataSource` and the\nactual procedure name.\n\nIf the stored procedure or function takes in parameters, then they must be declared and\nset by using the `parameters` property. The following example, for Oracle, declares three\nparameters. The first one is the `out` parameter that returns the ref-cursor, and the\nsecond and third are in parameters that takes a value of type `INTEGER`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to work with parameters in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic StoredProcedureItemReader reader(DataSource dataSource) {\n\tList<SqlParameter> parameters = new ArrayList<>();\n\tparameters.add(new SqlOutParameter(\"newId\", OracleTypes.CURSOR));\n\tparameters.add(new SqlParameter(\"amount\", Types.INTEGER);\n\tparameters.add(new SqlParameter(\"custId\", Types.INTEGER);\n\n\tStoredProcedureItemReader reader = new StoredProcedureItemReader();\n\n\treader.setDataSource(dataSource);\n\treader.setProcedureName(\"spring.cursor_func\");\n\treader.setParameters(parameters);\n\treader.setRefCursorPosition(1);\n\treader.setRowMapper(rowMapper());\n\treader.setPreparedStatementSetter(parameterSetter());\n\n\treturn reader;\n}\n----\n\nXML::\n+\nThe following example shows how to work with parameters in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"reader\" class=\"o.s.batch.item.database.StoredProcedureItemReader\">\n <property name=\"dataSource\" ref=\"dataSource\"/>\n <property name=\"procedureName\" value=\"spring.cursor_func\"/>\n <property name=\"parameters\">\n <list>\n <bean class=\"org.springframework.jdbc.core.SqlOutParameter\">\n <constructor-arg index=\"0\" value=\"newid\"/>\n <constructor-arg index=\"1\">\n <util:constant static-field=\"oracle.jdbc.OracleTypes.CURSOR\"/>\n </constructor-arg>\n </bean>\n <bean class=\"org.springframework.jdbc.core.SqlParameter\">\n <constructor-arg index=\"0\" value=\"amount\"/>\n <constructor-arg index=\"1\">\n <util:constant static-field=\"java.sql.Types.INTEGER\"/>\n </constructor-arg>\n </bean>\n <bean class=\"org.springframework.jdbc.core.SqlParameter\">\n <constructor-arg index=\"0\" value=\"custid\"/>\n <constructor-arg index=\"1\">\n <util:constant static-field=\"java.sql.Types.INTEGER\"/>\n </constructor-arg>\n </bean>\n </list>\n </property>\n <property name=\"refCursorPosition\" value=\"1\"/>\n <property name=\"rowMapper\" ref=\"rowMapper\"/>\n <property name=\"preparedStatementSetter\" ref=\"parameterSetter\"/>\n</bean>\n----\n\n====\n\nIn addition to the parameter declarations, we need to specify a `PreparedStatementSetter`\nimplementation that sets the parameter values for the call. This works the same as for\nthe `JdbcCursorItemReader` above. All the additional properties listed in\nxref:readers-and-writers/database.adoc#JdbcCursorItemReaderProperties[Additional Properties] apply to the `StoredProcedureItemReader` as well.\n\n[[pagingItemReaders]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc", "title": "database", "heading": "`StoredProcedureItemReader`", "heading_level": 3, "file_order": 11, "section_index": 4, "content_hash": "9287c06db84bb7009d972d16227c3a1e6d6da1d11379a78380b1a2f9ecf7ae78", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc"}}
{"id": "sha256:3a216ac5b1b29a2b44effd86c1805281644fe7e5003d3ad4281de12089585dab", "content": "An alternative to using a database cursor is running multiple queries where each query\nfetches a portion of the results. We refer to this portion as a page. Each query must\nspecify the starting row number and the number of rows that we want returned in the page.\n\n[[JdbcPagingItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc", "title": "database", "heading": "Paging `ItemReader` Implementations", "heading_level": 2, "file_order": 11, "section_index": 5, "content_hash": "3a216ac5b1b29a2b44effd86c1805281644fe7e5003d3ad4281de12089585dab", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc"}}
{"id": "sha256:b04770f1360f827d6bbc16ea79138bc46cb0225b7d635f3cea0dd9e22294843a", "content": "One implementation of a paging `ItemReader` is the `JdbcPagingItemReader`. The\n`JdbcPagingItemReader` needs a `PagingQueryProvider` responsible for providing the SQL\nqueries used to retrieve the rows making up a page. Since each database has its own\nstrategy for providing paging support, we need to use a different `PagingQueryProvider`\nfor each supported database type. There is also the `SqlPagingQueryProviderFactoryBean`\nthat auto-detects the database that is being used and determine the appropriate\n`PagingQueryProvider` implementation. This simplifies the configuration and is the\nrecommended best practice.\n\nThe `SqlPagingQueryProviderFactoryBean` requires that you specify a `select` clause and a\n`from` clause. You can also provide an optional `where` clause. These clauses and the\nrequired `sortKey` are used to build an SQL statement.\n\nNOTE: It is important to have a unique key constraint on the `sortKey` to guarantee that\n no data is lost between executions.\n\nAfter the reader has been opened, it passes back one item per call to `read` in the same\nbasic fashion as any other `ItemReader`. The paging happens behind the scenes when\nadditional rows are needed.\n\n[tabs]\n====\nJava::\n+\nThe following Java example configuration uses a similar 'customer credit' example as the\ncursor-based `ItemReaders` shown previously:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic JdbcPagingItemReader itemReader(DataSource dataSource, PagingQueryProvider queryProvider) {\n\tMap<String, Object> parameterValues = new HashMap<>();\n\tparameterValues.put(\"status\", \"NEW\");\n\n\treturn new JdbcPagingItemReaderBuilder<CustomerCredit>()\n .name(\"creditReader\")\n .dataSource(dataSource)\n .queryProvider(queryProvider)\n .parameterValues(parameterValues)\n .rowMapper(customerCreditMapper())\n .pageSize(1000)\n .build();\n}\n\n@Bean\npublic SqlPagingQueryProviderFactoryBean queryProvider() {\n\tSqlPagingQueryProviderFactoryBean provider = new SqlPagingQueryProviderFactoryBean();\n\n\tprovider.setSelectClause(\"select id, name, credit\");\n\tprovider.setFromClause(\"from customer\");\n\tprovider.setWhereClause(\"where status=:status\");\n\tprovider.setSortKey(\"id\");\n\n\treturn provider;\n}\n----\n\nXML::\n+\nThe following XML example configuration uses a similar 'customer credit' example as the\ncursor-based `ItemReaders` shown previously:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"itemReader\" class=\"org.spr...JdbcPagingItemReader\">\n <property name=\"dataSource\" ref=\"dataSource\"/>\n <property name=\"queryProvider\">\n <bean class=\"org.spr...SqlPagingQueryProviderFactoryBean\">\n <property name=\"selectClause\" value=\"select id, name, credit\"/>\n <property name=\"fromClause\" value=\"from customer\"/>\n <property name=\"whereClause\" value=\"where status=:status\"/>\n <property name=\"sortKey\" value=\"id\"/>\n </bean>\n </property>\n <property name=\"parameterValues\">\n <map>\n <entry key=\"status\" value=\"NEW\"/>\n </map>\n </property>\n <property name=\"pageSize\" value=\"1000\"/>\n <property name=\"rowMapper\" ref=\"customerMapper\"/>\n</bean>\n----\n\n====\n\nThis configured `ItemReader` returns `CustomerCredit` objects using the `RowMapper`,\nwhich must be specified. The 'pageSize' property determines the number of entities read\nfrom the database for each query run.\n\nThe 'parameterValues' property can be used to specify a `Map` of parameter values for the\nquery. If you use named parameters in the `where` clause, the key for each entry should\nmatch the name of the named parameter. If you use a traditional '?' placeholder, then the\nkey for each entry should be the number of the placeholder, starting with 1.\n\n[[JpaPagingItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc", "title": "database", "heading": "`JdbcPagingItemReader`", "heading_level": 3, "file_order": 11, "section_index": 6, "content_hash": "b04770f1360f827d6bbc16ea79138bc46cb0225b7d635f3cea0dd9e22294843a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc"}}
{"id": "sha256:ad44031d8eea28140b88b31001a47dee412d511bf7d489f718caf036ae15f5b7", "content": "Another implementation of a paging `ItemReader` is the `JpaPagingItemReader`. JPA does\nnot have a concept similar to the Hibernate `StatelessSession`, so we have to use other\nfeatures provided by the JPA specification. Since JPA supports paging, this is a natural\nchoice when it comes to using JPA for batch processing. After each page is read, the\nentities become detached and the persistence context is cleared, to allow the entities to\nbe garbage collected once the page is processed.\n\nThe `JpaPagingItemReader` lets you declare a JPQL statement and pass in a\n`EntityManagerFactory`. It then passes back one item per call to read in the same basic\nfashion as any other `ItemReader`. The paging happens behind the scenes when additional\nentities are needed.\n\n[tabs]\n====\nJava::\n+\nThe following Java example configuration uses the same 'customer credit' example as the\nJDBC reader shown previously:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic JpaPagingItemReader itemReader() {\n\treturn new JpaPagingItemReaderBuilder<CustomerCredit>()\n .name(\"creditReader\")\n .entityManagerFactory(entityManagerFactory())\n .queryString(\"select c from CustomerCredit c\")\n .pageSize(1000)\n .build();\n}\n----\n\nXML::\n+\nThe following XML example configuration uses the same 'customer credit' example as the\nJDBC reader shown previously:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"itemReader\" class=\"org.spr...JpaPagingItemReader\">\n <property name=\"entityManagerFactory\" ref=\"entityManagerFactory\"/>\n <property name=\"queryString\" value=\"select c from CustomerCredit c\"/>\n <property name=\"pageSize\" value=\"1000\"/>\n</bean>\n----\n\n====\n\nThis configured `ItemReader` returns `CustomerCredit` objects in the exact same manner as\ndescribed for the `JdbcPagingItemReader` above, assuming the `CustomerCredit` object has the\ncorrect JPA annotations or ORM mapping file. The 'pageSize' property determines the\nnumber of entities read from the database for each query execution.\n\n[[databaseItemWriters]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc", "title": "database", "heading": "`JpaPagingItemReader`", "heading_level": 3, "file_order": 11, "section_index": 7, "content_hash": "ad44031d8eea28140b88b31001a47dee412d511bf7d489f718caf036ae15f5b7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc"}}
{"id": "sha256:18d082d834a02595e25a3b836bf0aa4552a886debaf210a24ff9cd52b21e3872", "content": "While both flat files and XML files have a specific `ItemWriter` instance, there is no exact equivalent\nin the database world. This is because transactions provide all the needed functionality.\n`ItemWriter` implementations are necessary for files because they must act as if they're transactional,\nkeeping track of written items and flushing or clearing at the appropriate times.\nDatabases have no need for this functionality, since the write is already contained in a\ntransaction. Users can create their own DAOs that implement the `ItemWriter` interface or\nuse one from a custom `ItemWriter` that's written for generic processing concerns. Either\nway, they should work without any issues. One thing to look out for is the performance\nand error handling capabilities that are provided by batching the outputs. This is most\ncommon when using hibernate as an `ItemWriter` but could have the same issues when using\nJDBC batch mode. Batching database output does not have any inherent flaws, assuming we\nare careful to flush and there are no errors in the data. However, any errors while\nwriting can cause confusion, because there is no way to know which individual item caused\nan exception or even if any individual item was responsible, as illustrated in the\nfollowing image:\n\n.Error On Flush\nimage::errorOnFlush.png[Error On Flush, scaledwidth=\"60%\"]\n\nIf items are buffered before being written, any errors are not thrown until the buffer is\nflushed just before a commit. For example, assume that 20 items are written per chunk,\nand the 15th item throws a `DataIntegrityViolationException`. As far as the `Step`\nis concerned, all 20 item are written successfully, since there is no way to know that an\nerror occurs until they are actually written. Once `Session#flush()` is called, the\nbuffer is emptied and the exception is hit. At this point, there is nothing the `Step`\ncan do. The transaction must be rolled back. Normally, this exception might cause the\nitem to be skipped (depending upon the skip/retry policies), and then it is not written\nagain. However, in the batched scenario, there is no way to know which item caused the\nissue. The whole buffer was being written when the failure happened. The only way to\nsolve this issue is to flush after each item, as shown in the following image:\n\n.Error On Write\nimage::errorOnWrite.png[Error On Write, scaledwidth=\"60%\"]\n\nThis is a common use case, especially when using Hibernate, and the simple guideline for\nimplementations of `ItemWriter` is to flush on each call to `write()`. Doing so allows\nfor items to be skipped reliably, with Spring Batch internally taking care of the\ngranularity of the calls to `ItemWriter` after an error.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc", "title": "database", "heading": "Database ItemWriters", "heading_level": 2, "file_order": 11, "section_index": 8, "content_hash": "18d082d834a02595e25a3b836bf0aa4552a886debaf210a24ff9cd52b21e3872", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/database.adoc"}}
{"id": "sha256:a786ae34671f4c4eec6947a5a8573eb910d5a7746498443b30e54cea4299eed8", "content": "[[delegatePatternAndRegistering]]\n\nNote that the `CompositeItemWriter` is an example of the delegation pattern, which is\ncommon in Spring Batch. The delegates themselves might implement callback interfaces,\nsuch as `StepListener`. If they do and if they are being used in conjunction with Spring\nBatch Core as part of a `Step` in a `Job`, then they almost certainly need to be\nregistered manually with the `Step`. A reader, writer, or processor that is directly\nwired into the `Step` gets registered automatically if it implements `ItemStream` or a\n`StepListener` interface. However, because the delegates are not known to the `Step`,\nthey need to be injected as listeners or streams (or both if appropriate).\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to inject a delegate as a stream in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job ioSampleJob(JobRepository jobRepository, Step step1) {\n\treturn new JobBuilder(\"ioSampleJob\", jobRepository)\n .start(step1)\n .build();\n}\n\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(2).transactionManager(transactionManager)\n .reader(fooReader())\n .processor(fooProcessor())\n .writer(compositeItemWriter())\n .stream(barWriter())\n .build();\n}\n\n@Bean\npublic CustomCompositeItemWriter compositeItemWriter() {\n\n\tCustomCompositeItemWriter writer = new CustomCompositeItemWriter();\n\n\twriter.setDelegate(barWriter());\n\n\treturn writer;\n}\n\n@Bean\npublic BarWriter barWriter() {\n\treturn new BarWriter();\n}\n----\n\nXML::\n+\nThe following example shows how to inject a delegate as a stream in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"ioSampleJob\">\n <step name=\"step1\">\n <tasklet>\n <chunk reader=\"fooReader\" processor=\"fooProcessor\" writer=\"compositeItemWriter\"\n commit-interval=\"2\">\n <streams>\n <stream ref=\"barWriter\" />\n </streams>\n </chunk>\n </tasklet>\n </step>\n</job>\n\n<bean id=\"compositeItemWriter\" class=\"...CustomCompositeItemWriter\">\n <property name=\"delegate\" ref=\"barWriter\" />\n</bean>\n\n<bean id=\"barWriter\" class=\"...BarWriter\" />\n----\n\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/delegate-pattern-registering.adoc", "title": "delegate-pattern-registering", "heading": "delegate-pattern-registering", "heading_level": 1, "file_order": 12, "section_index": 0, "content_hash": "a786ae34671f4c4eec6947a5a8573eb910d5a7746498443b30e54cea4299eed8", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/delegate-pattern-registering.adoc"}}
{"id": "sha256:4cd3ec92b86bdf5e9f718c5cfff02073cf2025f40a9fde6b6fb1e18424366abf", "content": "[[flatFiles]]\n\nOne of the most common mechanisms for interchanging bulk data has always been the flat\nfile. Unlike XML, which has an agreed upon standard for defining how it is structured\n(XSD), anyone reading a flat file must understand ahead of time exactly how the file is\nstructured. In general, all flat files fall into two types: delimited and fixed length.\nDelimited files are those in which fields are separated by a delimiter, such as a comma.\nFixed Length files have fields that are a set length.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files.adoc", "title": "flat-files", "heading": "flat-files", "heading_level": 1, "file_order": 13, "section_index": 0, "content_hash": "4cd3ec92b86bdf5e9f718c5cfff02073cf2025f40a9fde6b6fb1e18424366abf", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/flat-files.adoc"}}
{"id": "sha256:6b84b521d722e7300701f11c5a8cb9365617a7ef1731e12bf040e6d9ef307ffa", "content": "[[itemReaderAndWriterImplementations]]\n\nIn this section, we will introduce you to readers and writers that have not already been\ndiscussed in the previous sections.\n\n[[decorators]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "item-reader-writer-implementations", "heading_level": 1, "file_order": 14, "section_index": 0, "content_hash": "6b84b521d722e7300701f11c5a8cb9365617a7ef1731e12bf040e6d9ef307ffa", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:4082982be859ee925b9ccf837982733ebbf4755ce9c568e1a7cf5a5722c9df64", "content": "In some cases, you might need specialized behavior to be appended to a pre-existing\n`ItemReader` or `ItemWriter` implementation.\nFor this purpose, Spring Batch offers the following out-of-the-box decorators:\n\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#synchronizedItemStreamReader[`SynchronizedItemStreamReader`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#singleItemPeekableItemReader[`SingleItemPeekableItemReader`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#synchronizedItemStreamWriter[`SynchronizedItemStreamWriter`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#multiResourceItemWriter[`MultiResourceItemWriter`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#classifierCompositeItemWriter[`ClassifierCompositeItemWriter`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#classifierCompositeItemProcessor[`ClassifierCompositeItemProcessor`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#mappingItemWriter[`MappingItemWriter`]\n\n[[synchronizedItemStreamReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "Decorators", "heading_level": 2, "file_order": 14, "section_index": 1, "content_hash": "4082982be859ee925b9ccf837982733ebbf4755ce9c568e1a7cf5a5722c9df64", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:0b60f0329290c2b917dc9e72678d0025fcb5e57687e5ba495cf7c8f43dccf93e", "content": "When using an `ItemReader` that is not thread safe, Spring Batch offers the\n`SynchronizedItemStreamReader` decorator, which can be used to make the `ItemReader`\nthread safe. Spring Batch provides a `SynchronizedItemStreamReaderBuilder` to construct\nan instance of the `SynchronizedItemStreamReader`.\n\nFor example, the `FlatFileItemReader` is *not* thread-safe and cannot be used in\na multi-threaded step. This reader can be decorated with a `SynchronizedItemStreamReader`\nin order to use it safely in a multi-threaded step. Here is an example of how to decorate\nsuch a reader:\n\n[source, java]\n----\n@Bean\npublic SynchronizedItemStreamReader<Person> itemReader() {\n\tFlatFileItemReader<Person> flatFileItemReader = new FlatFileItemReaderBuilder<Person>()\n // set reader properties\n .build();\n\n\treturn new SynchronizedItemStreamReaderBuilder<Person>()\n .delegate(flatFileItemReader)\n .build();\n}\n----\n\n[[singleItemPeekableItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`SynchronizedItemStreamReader`", "heading_level": 3, "file_order": 14, "section_index": 2, "content_hash": "0b60f0329290c2b917dc9e72678d0025fcb5e57687e5ba495cf7c8f43dccf93e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:2d72b49c6d4a5a8a1dd30767ee98898ba0e83fa990a9bfd025fad0d3894e6a11", "content": "Spring Batch includes a decorator that adds a peek method to an `ItemReader`. This peek\nmethod lets the user peek one item ahead. Repeated calls to the peek returns the same\nitem, and this is the next item returned from the `read` method. Spring Batch provides a\n`SingleItemPeekableItemReaderBuilder` to construct an instance of the\n`SingleItemPeekableItemReader`.\n\nNOTE: SingleItemPeekableItemReader's peek method is not thread-safe, because it would not\nbe possible to honor the peek in multiple threads. Only one of the threads that peeked\nwould get that item in the next call to read.\n\n[[synchronizedItemStreamWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`SingleItemPeekableItemReader`", "heading_level": 3, "file_order": 14, "section_index": 3, "content_hash": "2d72b49c6d4a5a8a1dd30767ee98898ba0e83fa990a9bfd025fad0d3894e6a11", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:a892e2484144befa18b8fc3ac1f63bcf5424680936c5577a2c6aeca64f69d63e", "content": "When using an `ItemWriter` that is not thread safe, Spring Batch offers the\n`SynchronizedItemStreamWriter` decorator, which can be used to make the `ItemWriter`\nthread safe. Spring Batch provides a `SynchronizedItemStreamWriterBuilder` to construct\nan instance of the `SynchronizedItemStreamWriter`.\n\nFor example, the `FlatFileItemWriter` is *not* thread-safe and cannot be used in\na multi-threaded step. This writer can be decorated with a `SynchronizedItemStreamWriter`\nin order to use it safely in a multi-threaded step. Here is an example of how to decorate\nsuch a writer:\n\n[source, java]\n----\n@Bean\npublic SynchronizedItemStreamWriter<Person> itemWriter() {\n\tFlatFileItemWriter<Person> flatFileItemWriter = new FlatFileItemWriterBuilder<Person>()\n // set writer properties\n .build();\n\n\treturn new SynchronizedItemStreamWriterBuilder<Person>()\n .delegate(flatFileItemWriter)\n .build();\n}\n----\n\n[[multiResourceItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`SynchronizedItemStreamWriter`", "heading_level": 3, "file_order": 14, "section_index": 4, "content_hash": "a892e2484144befa18b8fc3ac1f63bcf5424680936c5577a2c6aeca64f69d63e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:d9f4fd40921d7799594985443fd607d595c2c47e9d778af9c52d2963d06e71d4", "content": "The `MultiResourceItemWriter` wraps a `ResourceAwareItemWriterItemStream` and creates a new\noutput resource when the count of items written in the current resource exceeds the\n`itemCountLimitPerResource`. Spring Batch provides a `MultiResourceItemWriterBuilder` to\nconstruct an instance of the `MultiResourceItemWriter`.\n\n[[classifierCompositeItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`MultiResourceItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 5, "content_hash": "d9f4fd40921d7799594985443fd607d595c2c47e9d778af9c52d2963d06e71d4", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:0adc346edc323a4b5fad10bd579cb609afdb4e0da177f3a3a86f8fd323ef8698", "content": "The `ClassifierCompositeItemWriter` calls one of a collection of `ItemWriter`\nimplementations for each item, based on a router pattern implemented through the provided\n`Classifier`. The implementation is thread-safe if all delegates are thread-safe. Spring\nBatch provides a `ClassifierCompositeItemWriterBuilder` to construct an instance of the\n`ClassifierCompositeItemWriter`.\n\n[[classifierCompositeItemProcessor]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`ClassifierCompositeItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 6, "content_hash": "0adc346edc323a4b5fad10bd579cb609afdb4e0da177f3a3a86f8fd323ef8698", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:41f877990148893505ef37a9af746da9f70e0253f846fa90f0a7f52717308b8a", "content": "The `ClassifierCompositeItemProcessor` is an `ItemProcessor` that calls one of a\ncollection of `ItemProcessor` implementations, based on a router pattern implemented\nthrough the provided `Classifier`. Spring Batch provides a\n`ClassifierCompositeItemProcessorBuilder` to construct an instance of the\n`ClassifierCompositeItemProcessor`.\n\n[[mappingItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`ClassifierCompositeItemProcessor`", "heading_level": 3, "file_order": 14, "section_index": 7, "content_hash": "41f877990148893505ef37a9af746da9f70e0253f846fa90f0a7f52717308b8a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:a908360f960e0c72e4c90c35b1a3441c0c0c97ccd8337ff32fcfb695db0467e1", "content": "The `MappingItemWriter` adapts an `ItemWriter` accepting items of a given type to one accepting\nitems of another type by applying a mapping function to each item before writing.\nThread-safety is guaranteed as long as the downstream item writer is thread-safe, and state\nmanagement is honored with a downstream `ItemStream` item writer.\n\nThis item writer is most useful when used in combination with a `CompositeItemWriter`, where the\nmapping function in front of the downstream writer can be a getter of the input item or a more\ncomplex transformation logic, effectively allowing deconstruction patterns.\n\n[[messagingReadersAndWriters]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`MappingItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 8, "content_hash": "a908360f960e0c72e4c90c35b1a3441c0c0c97ccd8337ff32fcfb695db0467e1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:b3de1d60ffe88bcdc701398d9f08bfecf48789beabf58913022c7bf5122bbab7", "content": "Spring Batch offers the following readers and writers for commonly used messaging systems:\n\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#amqpItemReader[`AmqpItemReader`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#amqpItemWriter[`AmqpItemWriter`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#jmsItemReader[`JmsItemReader`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#jmsItemWriter[`JmsItemWriter`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#kafkaItemReader[`KafkaItemReader`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#kafkaItemWriter[`KafkaItemWriter`]\n\n[[amqpItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "Messaging Readers And Writers", "heading_level": 2, "file_order": 14, "section_index": 9, "content_hash": "b3de1d60ffe88bcdc701398d9f08bfecf48789beabf58913022c7bf5122bbab7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:6989d0f904cd30ffde14d53f7cb843c1bea843a79d8f75ba8240db333dd864d1", "content": "The `AmqpItemReader` is an `ItemReader` that uses an `AmqpTemplate` to receive or convert\nmessages from an exchange. Spring Batch provides a `AmqpItemReaderBuilder` to construct\nan instance of the `AmqpItemReader`.\n\n[[amqpItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`AmqpItemReader`", "heading_level": 3, "file_order": 14, "section_index": 10, "content_hash": "6989d0f904cd30ffde14d53f7cb843c1bea843a79d8f75ba8240db333dd864d1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:b1105b827f00e24ec50018cc3183173cd02d3edfab3601b15e903e536c1d053d", "content": "The `AmqpItemWriter` is an `ItemWriter` that uses an `AmqpTemplate` to send messages to\nan AMQP exchange. Messages are sent to the nameless exchange if the name not specified in\nthe provided `AmqpTemplate`. Spring Batch provides an `AmqpItemWriterBuilder` to\nconstruct an instance of the `AmqpItemWriter`.\n\n[[jmsItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`AmqpItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 11, "content_hash": "b1105b827f00e24ec50018cc3183173cd02d3edfab3601b15e903e536c1d053d", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:a7b78fddf9fce9ceae4aa72c2ebc78b9e44946de7261cf24d667ee48850926d0", "content": "The `JmsItemReader` is an `ItemReader` for JMS that uses a `JmsTemplate`. The template\nshould have a default destination, which is used to provide items for the `read()`\nmethod. Spring Batch provides a `JmsItemReaderBuilder` to construct an instance of the\n`JmsItemReader`.\n\n[[jmsItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`JmsItemReader`", "heading_level": 3, "file_order": 14, "section_index": 12, "content_hash": "a7b78fddf9fce9ceae4aa72c2ebc78b9e44946de7261cf24d667ee48850926d0", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:7ca4efba14bc96e96ebaf99a8535e6157fe7dbac812bbbc3957deb6c2e711f29", "content": "The `JmsItemWriter` is an `ItemWriter` for JMS that uses a `JmsTemplate`. The template\nshould have a default destination, which is used to send items in `write(List)`. Spring\nBatch provides a `JmsItemWriterBuilder` to construct an instance of the `JmsItemWriter`.\n\n[[kafkaItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`JmsItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 13, "content_hash": "7ca4efba14bc96e96ebaf99a8535e6157fe7dbac812bbbc3957deb6c2e711f29", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:d172dd93ed01da695c0e01634d4c25310a62148c1e2dd1aee07c5f28f14d0e16", "content": "The `KafkaItemReader` is an `ItemReader` for an Apache Kafka topic. It can be configured\nto read messages from multiple partitions of the same topic. It stores message offsets\nin the execution context to support restart capabilities. Spring Batch provides a\n`KafkaItemReaderBuilder` to construct an instance of the `KafkaItemReader`.\n\n[[kafkaItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`KafkaItemReader`", "heading_level": 3, "file_order": 14, "section_index": 14, "content_hash": "d172dd93ed01da695c0e01634d4c25310a62148c1e2dd1aee07c5f28f14d0e16", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:23a7196250973c8faf1043843e2d0465ca00461d5adc5ec09ffd30a1a048f137", "content": "The `KafkaItemWriter` is an `ItemWriter` for Apache Kafka that uses a `KafkaTemplate` to\nsend events to a default topic. Spring Batch provides a `KafkaItemWriterBuilder` to\nconstruct an instance of the `KafkaItemWriter`.\n\n[[databaseReaders]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`KafkaItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 15, "content_hash": "23a7196250973c8faf1043843e2d0465ca00461d5adc5ec09ffd30a1a048f137", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:1db4a3f85a348fd6db95adde1b3548cf26256174e8685bc1de51ba64ab122138", "content": "Spring Batch offers the following database readers:\n\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#mongoPagingItemReader[`MongoPagingItemReader`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#mongoCursorItemReader[`MongoCursorItemReader`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#repositoryItemReader[`RepositoryItemReader`]\n\n[[mongoPagingItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "Database Readers", "heading_level": 2, "file_order": 14, "section_index": 16, "content_hash": "1db4a3f85a348fd6db95adde1b3548cf26256174e8685bc1de51ba64ab122138", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:55d524515179e405a176a21bfd2c3da006d5fab94e97ee8de6c8446463d20a04", "content": "The `MongoPagingItemReader` is an `ItemReader` that reads documents from MongoDB by using a\npaging technique. Spring Batch provides a `MongoPagingItemReaderBuilder` to construct an\ninstance of the `MongoPagingItemReader`.\n\n[[mongoCursorItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`MongoPagingItemReader`", "heading_level": 3, "file_order": 14, "section_index": 17, "content_hash": "55d524515179e405a176a21bfd2c3da006d5fab94e97ee8de6c8446463d20a04", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:bde4b50a14957ff024b18de9690772813ecaab5abdc8007a83d96fe9652a7936", "content": "The `MongoCursorItemReader` is an `ItemReader` that reads documents from MongoDB by using a\nstreaming technique. Spring Batch provides a `MongoCursorItemReaderBuilder` to construct an\ninstance of the `MongoCursorItemReader`.\n\n[[repositoryItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`MongoCursorItemReader`", "heading_level": 3, "file_order": 14, "section_index": 18, "content_hash": "bde4b50a14957ff024b18de9690772813ecaab5abdc8007a83d96fe9652a7936", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:6f94240d74c2fc1c192547b6de88c03dbc4d603efec4cdc7a21e20050f54c056", "content": "The `RepositoryItemReader` is an `ItemReader` that reads records by using a\n`PagingAndSortingRepository`. Spring Batch provides a `RepositoryItemReaderBuilder` to\nconstruct an instance of the `RepositoryItemReader`.\n\n[[databaseWriters]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`RepositoryItemReader`", "heading_level": 3, "file_order": 14, "section_index": 19, "content_hash": "6f94240d74c2fc1c192547b6de88c03dbc4d603efec4cdc7a21e20050f54c056", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:4392bf217cdc5b19ada23227a8c6ecd99d104c505361b2d15c45a68c4fc3aed1", "content": "Spring Batch offers the following database writers:\n\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#mongoItemWriter[`MongoItemWriter`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#repositoryItemWriter[`RepositoryItemWriter`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#jdbcBatchItemWriter[`JdbcBatchItemWriter`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#jpaItemWriter[`JpaItemWriter`]\n\n[[mongoItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "Database Writers", "heading_level": 2, "file_order": 14, "section_index": 20, "content_hash": "4392bf217cdc5b19ada23227a8c6ecd99d104c505361b2d15c45a68c4fc3aed1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:e68b8d4f910bc8fac73359ef81cd8e91ef67473406b5dc413a1a085ff6d4bf13", "content": "The `MongoItemWriter` is an `ItemWriter` implementation that writes to a MongoDB store\nusing an implementation of Spring Data's `MongoOperations`. Spring Batch provides a\n`MongoItemWriterBuilder` to construct an instance of the `MongoItemWriter`.\n\n[[repositoryItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`MongoItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 21, "content_hash": "e68b8d4f910bc8fac73359ef81cd8e91ef67473406b5dc413a1a085ff6d4bf13", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:d9079e33596dbc5434b531c01731b23b3f0cf0ffe86228263d5237a794e74a4a", "content": "The `RepositoryItemWriter` is an `ItemWriter` wrapper for a `CrudRepository` from Spring\nData. Spring Batch provides a `RepositoryItemWriterBuilder` to construct an instance of\nthe `RepositoryItemWriter`.\n\n[[jdbcBatchItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`RepositoryItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 22, "content_hash": "d9079e33596dbc5434b531c01731b23b3f0cf0ffe86228263d5237a794e74a4a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:8c50437b88248761a56d494ee7617bde6f27756e394eab5cf06a75e386530c5d", "content": "The `JdbcBatchItemWriter` is an `ItemWriter` that uses the batching features from\n`NamedParameterJdbcTemplate` to execute a batch of statements for all items provided.\nSpring Batch provides a `JdbcBatchItemWriterBuilder` to construct an instance of the\n`JdbcBatchItemWriter`.\n\n[[jpaItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`JdbcBatchItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 23, "content_hash": "8c50437b88248761a56d494ee7617bde6f27756e394eab5cf06a75e386530c5d", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:fff7a40d705061dd8114eeb3b30b7539d653719bcc529abf99e31b7a8b70079d", "content": "The `JpaItemWriter` is an `ItemWriter` that uses a JPA `EntityManagerFactory` to merge\nany entities that are not part of the persistence context. Spring Batch provides a\n`JpaItemWriterBuilder` to construct an instance of the `JpaItemWriter`.\n\n[[specializedReaders]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`JpaItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 24, "content_hash": "fff7a40d705061dd8114eeb3b30b7539d653719bcc529abf99e31b7a8b70079d", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:4396129742157f51509ec5796336c22b077e96736d1083b1b29f2a4ba1d43669", "content": "Spring Batch offers the following specialized readers:\n\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#ldifReader[`LdifReader`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#mappingLdifReader[`MappingLdifReader`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#avroItemReader[`AvroItemReader`]\n\n[[ldifReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "Specialized Readers", "heading_level": 2, "file_order": 14, "section_index": 25, "content_hash": "4396129742157f51509ec5796336c22b077e96736d1083b1b29f2a4ba1d43669", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:fce3807efff648c239969d73d2d2b303007f3afa089dfbdfa58b6faf1c8a8030", "content": "The `LdifReader` reads LDIF (LDAP Data Interchange Format) records from a `Resource`,\nparses them, and returns a `LdapAttribute` object for each `read` executed. Spring Batch\nprovides a `LdifReaderBuilder` to construct an instance of the `LdifReader`.\n\n[[mappingLdifReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`LdifReader`", "heading_level": 3, "file_order": 14, "section_index": 26, "content_hash": "fce3807efff648c239969d73d2d2b303007f3afa089dfbdfa58b6faf1c8a8030", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:5240e18d5abe467584e9a2fe0bfb009c775cc34c6f0043b7ee7229fc336c6b08", "content": "The `MappingLdifReader` reads LDIF (LDAP Data Interchange Format) records from a\n`Resource`, parses them then maps each LDIF record to a POJO (Plain Old Java Object).\nEach read returns a POJO. Spring Batch provides a `MappingLdifReaderBuilder` to construct\nan instance of the `MappingLdifReader`.\n\n[[avroItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`MappingLdifReader`", "heading_level": 3, "file_order": 14, "section_index": 27, "content_hash": "5240e18d5abe467584e9a2fe0bfb009c775cc34c6f0043b7ee7229fc336c6b08", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:5a21307024812d70b13495b372f2ca9ab5459b8a2f57ead71979c11f14bb99dc", "content": "The `AvroItemReader` reads serialized Avro data from a Resource.\nEach read returns an instance of the type specified by a Java class or Avro Schema.\nThe reader may be optionally configured for input that embeds an Avro schema or not.\nSpring Batch provides an `AvroItemReaderBuilder` to construct an instance of the `AvroItemReader`.\n\n[[specializedWriters]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`AvroItemReader`", "heading_level": 3, "file_order": 14, "section_index": 28, "content_hash": "5a21307024812d70b13495b372f2ca9ab5459b8a2f57ead71979c11f14bb99dc", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:1a40ee62126e65c45faeb33483f57401796406829bb670d7865247f7459d8868", "content": "Spring Batch offers the following specialized writers:\n\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#simpleMailMessageItemWriter[`SimpleMailMessageItemWriter`]\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#avroItemWriter[`AvroItemWriter`]\n\n[[simpleMailMessageItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "Specialized Writers", "heading_level": 2, "file_order": 14, "section_index": 29, "content_hash": "1a40ee62126e65c45faeb33483f57401796406829bb670d7865247f7459d8868", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:6c82ecfaf68691032ac7e34d8a1135e9fdbf9c3e2bdcee6d3bda21d4981249a4", "content": "The `SimpleMailMessageItemWriter` is an `ItemWriter` that can send mail messages. It\ndelegates the actual sending of messages to an instance of `MailSender`. Spring Batch\nprovides a `SimpleMailMessageItemWriterBuilder` to construct an instance of the\n`SimpleMailMessageItemWriter`.\n\n[[avroItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`SimpleMailMessageItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 30, "content_hash": "6c82ecfaf68691032ac7e34d8a1135e9fdbf9c3e2bdcee6d3bda21d4981249a4", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:da41698f3874bc8f08092ae6d4b60f4739d72673499c513d5531ebecb8628442", "content": "The `AvroItemWrite` serializes Java objects to a WriteableResource according to the given type or Schema.\nThe writer may be optionally configured to embed an Avro schema in the output or not.\nSpring Batch provides an `AvroItemWriterBuilder` to construct an instance of the `AvroItemWriter`.\n\n[[specializedProcessors]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`AvroItemWriter`", "heading_level": 3, "file_order": 14, "section_index": 31, "content_hash": "da41698f3874bc8f08092ae6d4b60f4739d72673499c513d5531ebecb8628442", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:5f53ec834e463ad747397dafa1e925ca55e4073d1bf7ec0349dd2e4dd3ce2af2", "content": "Spring Batch offers the following specialized processors:\n\n* xref:readers-and-writers/item-reader-writer-implementations.adoc#scriptItemProcessor[`ScriptItemProcessor`]\n\n[[scriptItemProcessor]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "Specialized Processors", "heading_level": 2, "file_order": 14, "section_index": 32, "content_hash": "5f53ec834e463ad747397dafa1e925ca55e4073d1bf7ec0349dd2e4dd3ce2af2", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:593d0d44593cc9b3e0c9d31a1541f5dde8733127b7f7520e5471c45be7b42ca6", "content": "The `ScriptItemProcessor` is an `ItemProcessor` that passes the current item to process\nto the provided script and the result of the script is returned by the processor. Spring\nBatch provides a `ScriptItemProcessorBuilder` to construct an instance of the\n`ScriptItemProcessor`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc", "title": "item-reader-writer-implementations", "heading": "`ScriptItemProcessor`", "heading_level": 3, "file_order": 14, "section_index": 33, "content_hash": "593d0d44593cc9b3e0c9d31a1541f5dde8733127b7f7520e5471c45be7b42ca6", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader-writer-implementations.adoc"}}
{"id": "sha256:d1e8920862e1c901591524bdea82b0fb4750b72ec4119b5d9b663c90d4b752d6", "content": "[[itemReader]]\n\nAlthough a simple concept, an `ItemReader` is the means for providing data from many\ndifferent types of input. The most general examples include:\n\n* Flat File: Flat-file item readers read lines of data from a flat file that typically\ndescribes records with fields of data defined by fixed positions in the file or delimited\nby some special character (such as a comma).\n\n* XML: XML `ItemReaders` process XML independently of technologies used for parsing,\nmapping and validating objects. Input data allows for the validation of an XML file\nagainst an XSD schema.\n\n* Database: A database resource is accessed to return resultsets which can be mapped to\nobjects for processing. The default SQL `ItemReader` implementations invoke a `RowMapper`\nto return objects, keep track of the current row if restart is required, store basic\nstatistics, and provide some transaction enhancements that are explained later.\n\nThere are many more possibilities, but we focus on the basic ones for this chapter. A\ncomplete list of all available `ItemReader` implementations can be found in\nxref:appendix.adoc#listOfReadersAndWriters[Appendix A].\n\n`ItemReader` is a basic interface for generic\ninput operations, as shown in the following interface definition:\n\n[source, java]\n----\npublic interface ItemReader<T> {\n\n T read() throws Exception;\n\n}\n----\n\nThe `read` method defines the most essential contract of the `ItemReader`. Calling it\nreturns one item or `null` if no more items are left. An item might represent a line in a\nfile, a row in a database, or an element in an XML file. It is generally expected that\nthese are mapped to a usable domain object (such as `Trade`, `Foo`, or others), but there\nis no requirement in the contract to do so.\n\nIt is expected that implementations of the `ItemReader` interface are forward only.\nHowever, if the underlying resource is transactional (such as a JMS queue) then calling\n`read` may return the same logical item on subsequent calls in a rollback scenario. It is\nalso worth noting that a lack of items to process by an `ItemReader` does not cause an\nexception to be thrown. For example, a database `ItemReader` that is configured with a\nquery that returns 0 results returns `null` on the first invocation of `read`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader.adoc", "title": "item-reader", "heading": "item-reader", "heading_level": 1, "file_order": 15, "section_index": 0, "content_hash": "d1e8920862e1c901591524bdea82b0fb4750b72ec4119b5d9b663c90d4b752d6", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-reader.adoc"}}
{"id": "sha256:48ba63e8edda998925281adea7e5772ab8229ff569acce75845163619a5fa326", "content": "[[itemStream]]\n\nBoth `ItemReaders` and `ItemWriters` serve their individual purposes well, but there is a\ncommon concern among both of them that necessitates another interface. In general, as\npart of the scope of a batch job, readers and writers need to be opened, closed, and\nrequire a mechanism for persisting state. The `ItemStream` interface serves that purpose,\nas shown in the following example:\n\n[source, java]\n----\npublic interface ItemStream {\n\n void open(ExecutionContext executionContext) throws ItemStreamException;\n\n void update(ExecutionContext executionContext) throws ItemStreamException;\n\n void close() throws ItemStreamException;\n}\n----\n\nBefore describing each method, we should mention the `ExecutionContext`. Clients of an\n`ItemReader` that also implement `ItemStream` should call `open` before any calls to\n`read`, in order to open any resources such as files or to obtain connections. A similar\nrestriction applies to an `ItemWriter` that implements `ItemStream`. As mentioned in\nChapter 2, if expected data is found in the `ExecutionContext`, it may be used to start\nthe `ItemReader` or `ItemWriter` at a location other than its initial state. Conversely,\n`close` is called to ensure that any resources allocated during open are released safely.\n`update` is called primarily to ensure that any state currently being held is loaded into\nthe provided `ExecutionContext`. This method is called before committing, to ensure that\nthe current state is persisted in the database before commit.\n\nIn the special case where the client of an `ItemStream` is a `Step` (from the Spring\nBatch Core), an `ExecutionContext` is created for each StepExecution to allow users to\nstore the state of a particular execution, with the expectation that it is returned if\nthe same `JobInstance` is started again. For those familiar with Quartz, the semantics\nare very similar to a Quartz `JobDataMap`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-stream.adoc", "title": "item-stream", "heading": "item-stream", "heading_level": 1, "file_order": 16, "section_index": 0, "content_hash": "48ba63e8edda998925281adea7e5772ab8229ff569acce75845163619a5fa326", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-stream.adoc"}}
{"id": "sha256:228a8d917c6736a072ddda7f52dc0ee55b71f70ea47f2dc80efb7cd461f8d25f", "content": "[[itemWriter]]\n\n`ItemWriter` is similar in functionality to an `ItemReader` but with inverse operations.\nResources still need to be located, opened, and closed but they differ in that an\n`ItemWriter` writes out, rather than reading in. In the case of databases or queues,\nthese operations may be inserts, updates, or sends. The format of the serialization of\nthe output is specific to each batch job.\n\nAs with `ItemReader`,\n`ItemWriter` is a fairly generic interface, as shown in the following interface definition:\n\n[source, java]\n----\npublic interface ItemWriter<T> {\n\n void write(Chunk<? extends T> items) throws Exception;\n\n}\n----\n\nAs with `read` on `ItemReader`, `write` provides the basic contract of `ItemWriter`. It\nattempts to write out the list of items passed in as long as it is open. Because it is\ngenerally expected that items are 'batched' together into a chunk and then output, the\ninterface accepts a list of items, rather than an item by itself. After writing out the\nlist, any flushing that may be necessary can be performed before returning from the write\nmethod. For example, if writing to a Hibernate DAO, multiple calls to write can be made,\none for each item. The writer can then call `flush` on the hibernate session before\nreturning.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-writer.adoc", "title": "item-writer", "heading": "item-writer", "heading_level": 1, "file_order": 17, "section_index": 0, "content_hash": "228a8d917c6736a072ddda7f52dc0ee55b71f70ea47f2dc80efb7cd461f8d25f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/item-writer.adoc"}}
{"id": "sha256:3078d04dd0316c1e9ce6069adf9b1d0ed64fad2082d391706f7ac047f3bbfd07", "content": "[[jsonReadingWriting]]\n\nSpring Batch provides support for reading and Writing JSON resources in the following format:\n\n[source, json]\n----\n[\n {\n \"isin\": \"123\",\n \"quantity\": 1,\n \"price\": 1.2,\n \"customer\": \"foo\"\n },\n {\n \"isin\": \"456\",\n \"quantity\": 2,\n \"price\": 1.4,\n \"customer\": \"bar\"\n }\n]\n----\n\nIt is assumed that the JSON resource is an array of JSON objects corresponding to\nindividual items. Spring Batch is not tied to any particular JSON library.\n\n[[JsonItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/json-reading-writing.adoc", "title": "json-reading-writing", "heading": "json-reading-writing", "heading_level": 1, "file_order": 18, "section_index": 0, "content_hash": "3078d04dd0316c1e9ce6069adf9b1d0ed64fad2082d391706f7ac047f3bbfd07", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/json-reading-writing.adoc"}}
{"id": "sha256:745dd0af1ea9b69b4182e4e3dc155ed2317c8631b2c3d2b12ff6975ad2a0238e", "content": "The `JsonItemReader` delegates JSON parsing and binding to implementations of the\n`org.springframework.batch.infrastructure.item.json.JsonObjectReader` interface. This interface\nis intended to be implemented by using a streaming API to read JSON objects\nin chunks. Two implementations are currently provided:\n\n* link:$$https://github.com/FasterXML/jackson$$[Jackson] through the `org.springframework.batch.infrastructure.item.json.JacksonJsonObjectReader`\n* link:$$https://github.com/google/gson$$[Gson] through the `org.springframework.batch.infrastructure.item.json.GsonJsonObjectReader`\n\nTo be able to process JSON records, the following is needed:\n\n* `Resource`: A Spring Resource that represents the JSON file to read.\n* `JsonObjectReader`: A JSON object reader to parse and bind JSON objects to items\n\nThe following example shows how to define a `JsonItemReader` that works with the\nprevious JSON resource `org/springframework/batch/infrastructure/item/json/trades.json` and a\n`JsonObjectReader` based on Jackson:\n\n[source, java]\n----\n@Bean\npublic JsonItemReader<Trade> jsonItemReader() {\n return new JsonItemReaderBuilder<Trade>()\n .jsonObjectReader(new JacksonJsonObjectReader<>(Trade.class))\n .resource(new ClassPathResource(\"trades.json\"))\n .name(\"tradeJsonItemReader\")\n .build();\n}\n----\n\n[[jsonfileitemwriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/json-reading-writing.adoc", "title": "json-reading-writing", "heading": "`JsonItemReader`", "heading_level": 2, "file_order": 18, "section_index": 1, "content_hash": "745dd0af1ea9b69b4182e4e3dc155ed2317c8631b2c3d2b12ff6975ad2a0238e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/json-reading-writing.adoc"}}
{"id": "sha256:baac5cc4d671032c985909fd407afc853a9d3cb6a6ead5cff50090bc99637104", "content": "The `JsonFileItemWriter` delegates the marshalling of items to the\n`org.springframework.batch.infrastructure.item.json.JsonObjectMarshaller` interface. The contract\nof this interface is to take an object and marshall it to a JSON `String`.\nTwo implementations are currently provided:\n\n* link:$$https://github.com/FasterXML/jackson$$[Jackson] through the `org.springframework.batch.infrastructure.item.json.JacksonJsonObjectMarshaller`\n* link:$$https://github.com/google/gson$$[Gson] through the `org.springframework.batch.infrastructure.item.json.GsonJsonObjectMarshaller`\n\nTo be able to write JSON records, the following is needed:\n\n* `Resource`: A Spring `Resource` that represents the JSON file to write\n* `JsonObjectMarshaller`: A JSON object marshaller to marshall objects to JSON format\n\nThe following example shows how to define a `JsonFileItemWriter`:\n\n[source, java]\n----\n@Bean\npublic JsonFileItemWriter<Trade> jsonFileItemWriter() {\n return new JsonFileItemWriterBuilder<Trade>()\n .jsonObjectMarshaller(new JacksonJsonObjectMarshaller<>())\n .resource(new ClassPathResource(\"trades.json\"))\n .name(\"tradeJsonFileItemWriter\")\n .build();\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/json-reading-writing.adoc", "title": "json-reading-writing", "heading": "`JsonFileItemWriter`", "heading_level": 2, "file_order": 18, "section_index": 2, "content_hash": "baac5cc4d671032c985909fd407afc853a9d3cb6a6ead5cff50090bc99637104", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/json-reading-writing.adoc"}}
{"id": "sha256:69ffccd0b12fdde3280aaa7ae74ca95a8c4e21031fb1819ce3e3d72fbf319ca9", "content": "[[multiFileInput]]\n\nIt is a common requirement to process multiple files within a single `Step`. Assuming the\nfiles all have the same formatting, the `MultiResourceItemReader` supports this type of\ninput for both XML and flat file processing. Consider the following files in a directory:\n\n----\nfile-1.txt file-2.txt ignored.txt\n----\n\nfile-1.txt and file-2.txt are formatted the same and, for business reasons, should be\nprocessed together. The `MultiResourceItemReader` can be used to read in both files by\nusing wildcards.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to read files with wildcards in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic MultiResourceItemReader multiResourceReader(@Value(\"classpath:data/input/file-*.txt\") Resource[] resources) {\n\treturn new MultiResourceItemReaderBuilder<Foo>()\n .delegate(flatFileItemReader())\n .resources(resources)\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to read files with wildcards in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"multiResourceReader\" class=\"org.spr...MultiResourceItemReader\">\n <property name=\"resources\" value=\"classpath:data/input/file-*.txt\" />\n <property name=\"delegate\" ref=\"flatFileItemReader\" />\n</bean>\n----\n\n====\n\nThe referenced delegate is a simple `FlatFileItemReader`. The above configuration reads\ninput from both files, handling rollback and restart scenarios. It should be noted that,\nas with any `ItemReader`, adding extra input (in this case a file) could cause potential\nissues when restarting. It is recommended that batch jobs work with their own individual\ndirectories until completed successfully.\n\nNOTE: Input resources are ordered by using `MultiResourceItemReader#setComparator(Comparator)`\n to make sure resource ordering is preserved between job runs in restart scenario.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/multi-file-input.adoc", "title": "multi-file-input", "heading": "multi-file-input", "heading_level": 1, "file_order": 19, "section_index": 0, "content_hash": "69ffccd0b12fdde3280aaa7ae74ca95a8c4e21031fb1819ce3e3d72fbf319ca9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/multi-file-input.adoc"}}
{"id": "sha256:e8fbc2af820c413c9a5b184f89b2faeffbc17697f73fd2f9a95e4aec8b27aa9f", "content": "[[process-indicator]]\n\nBy default, all of the `ItemReader` and `ItemWriter` implementations store their current\nstate in the `ExecutionContext` before it is committed. However, this may not always be\nthe desired behavior. For example, many developers choose to make their database readers\n'rerunnable' by using a process indicator. An extra column is added to the input data to\nindicate whether or not it has been processed. When a particular record is being read (or\nwritten) the processed flag is flipped from `false` to `true`. The SQL statement can then\ncontain an extra statement in the `where` clause, such as `where PROCESSED_IND = false`,\nthereby ensuring that only unprocessed records are returned in the case of a restart. In\nthis scenario, it is preferable to not store any state, such as the current row number,\nsince it is irrelevant upon restart. For this reason, all readers and writers include the\n'saveState' property.\n\n[tabs]\n====\nJava::\n+\nThe following bean definition shows how to prevent state persistence in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic JdbcCursorItemReader playerSummarizationSource(DataSource dataSource) {\n\treturn new JdbcCursorItemReaderBuilder<PlayerSummary>()\n .dataSource(dataSource)\n .rowMapper(new PlayerSummaryMapper())\n .saveState(false)\n .sql(\"SELECT games.player_id, games.year_no, SUM(COMPLETES),\"\n + \"SUM(ATTEMPTS), SUM(PASSING_YARDS), SUM(PASSING_TD),\"\n + \"SUM(INTERCEPTIONS), SUM(RUSHES), SUM(RUSH_YARDS),\"\n + \"SUM(RECEPTIONS), SUM(RECEPTIONS_YARDS), SUM(TOTAL_TD)\"\n + \"from games, players where players.player_id =\"\n + \"games.player_id group by games.player_id, games.year_no\")\n .build();\n\n}\n----\n\nXML::\n+\nThe following bean definition shows how to prevent state persistence in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"playerSummarizationSource\" class=\"org.spr...JdbcCursorItemReader\">\n <property name=\"dataSource\" ref=\"dataSource\" />\n <property name=\"rowMapper\">\n <bean class=\"org.springframework.batch.samples.PlayerSummaryMapper\" />\n </property>\n <property name=\"saveState\" value=\"false\" />\n <property name=\"sql\">\n <value>\n SELECT games.player_id, games.year_no, SUM(COMPLETES),\n SUM(ATTEMPTS), SUM(PASSING_YARDS), SUM(PASSING_TD),\n SUM(INTERCEPTIONS), SUM(RUSHES), SUM(RUSH_YARDS),\n SUM(RECEPTIONS), SUM(RECEPTIONS_YARDS), SUM(TOTAL_TD)\n from games, players where players.player_id =\n games.player_id group by games.player_id, games.year_no\n </value>\n </property>\n</bean>\n----\n\n====\n\nThe `ItemReader` configured above does not make any entries in the `ExecutionContext` for\nany executions in which it participates.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/process-indicator.adoc", "title": "process-indicator", "heading": "process-indicator", "heading_level": 1, "file_order": 20, "section_index": 0, "content_hash": "e8fbc2af820c413c9a5b184f89b2faeffbc17697f73fd2f9a95e4aec8b27aa9f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/process-indicator.adoc"}}
{"id": "sha256:418856052d884db6c26e820dadd86dbf70f397e6df48892ca0ec9201d3947c20", "content": "[[reusingExistingServices]]\n\nBatch systems are often used in conjunction with other application styles. The most\ncommon is an online system, but it may also support integration or even a thick client\napplication by moving necessary bulk data that each application style uses. For this\nreason, it is common that many users want to reuse existing DAOs or other services within\ntheir batch jobs. The Spring container itself makes this fairly easy by allowing any\nnecessary class to be injected. However, there may be cases where the existing service\nneeds to act as an `ItemReader` or `ItemWriter`, either to satisfy the dependency of\nanother Spring Batch class or because it truly is the main `ItemReader` for a step. It is\nfairly trivial to write an adapter class for each service that needs wrapping, but\nbecause it is such a common concern, Spring Batch provides implementations:\n`ItemReaderAdapter` and `ItemWriterAdapter`. Both classes implement the standard Spring\nmethod by invoking the delegate pattern and are fairly simple to set up.\n\n[tabs]\n====\nJava::\n+\nThe following Java example uses the `ItemReaderAdapter`:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic ItemReaderAdapter itemReader() {\n\tItemReaderAdapter reader = new ItemReaderAdapter();\n\n\treader.setTargetObject(fooService());\n\treader.setTargetMethod(\"generateFoo\");\n\n\treturn reader;\n}\n\n@Bean\npublic FooService fooService() {\n\treturn new FooService();\n}\n----\n\nXML::\n+\nThe following XML example uses the `ItemReaderAdapter`:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"itemReader\" class=\"org.springframework.batch.infrastructure.item.adapter.ItemReaderAdapter\">\n <property name=\"targetObject\" ref=\"fooService\" />\n <property name=\"targetMethod\" value=\"generateFoo\" />\n</bean>\n\n<bean id=\"fooService\" class=\"org.springframework.batch.infrastructure.item.sample.FooService\" />\n----\n\n====\n\nOne important point to note is that the contract of the `targetMethod` must be the same\nas the contract for `read`: When exhausted, it returns `null`. Otherwise, it returns an\n`Object`. Anything else prevents the framework from knowing when processing should end,\neither causing an infinite loop or incorrect failure, depending upon the implementation\nof the `ItemWriter`.\n\n[tabs]\n====\nJava::\n+\nThe following Java example uses the `ItemWriterAdapter`:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic ItemWriterAdapter itemWriter() {\n\tItemWriterAdapter writer = new ItemWriterAdapter();\n\n\twriter.setTargetObject(fooService());\n\twriter.setTargetMethod(\"processFoo\");\n\n\treturn writer;\n}\n\n@Bean\npublic FooService fooService() {\n\treturn new FooService();\n}\n----\n\nXML::\n+\nThe following XML example uses the `ItemWriterAdapter`:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"itemWriter\" class=\"org.springframework.batch.infrastructure.item.adapter.ItemWriterAdapter\">\n <property name=\"targetObject\" ref=\"fooService\" />\n <property name=\"targetMethod\" value=\"processFoo\" />\n</bean>\n\n<bean id=\"fooService\" class=\"org.springframework.batch.infrastructure.item.sample.FooService\" />\n----\n\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/reusing-existing-services.adoc", "title": "reusing-existing-services", "heading": "reusing-existing-services", "heading_level": 1, "file_order": 21, "section_index": 0, "content_hash": "418856052d884db6c26e820dadd86dbf70f397e6df48892ca0ec9201d3947c20", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/reusing-existing-services.adoc"}}
{"id": "sha256:fdd5660041cb89c14502f5d0f0b4638a08d0291607927b249eef25e97259afc9", "content": "[[xmlReadingWriting]]\n\nSpring Batch provides transactional infrastructure for both reading XML records and\nmapping them to Java objects as well as writing Java objects as XML records.\n\n[NOTE]\n.Constraints on streaming XML\n====\nThe StAX API is used for I/O, as other standard XML parsing APIs do not fit batch\nprocessing requirements (DOM loads the whole input into memory at once and SAX controls\nthe parsing process by allowing the user to provide only callbacks).\n====\n\nWe need to consider how XML input and output works in Spring Batch. First, there are a\nfew concepts that vary from file reading and writing but are common across Spring Batch\nXML processing. With XML processing, instead of lines of records (`FieldSet` instances) that need\nto be tokenized, it is assumed an XML resource is a collection of 'fragments'\ncorresponding to individual records, as shown in the following image:\n\n.XML Input\nimage::xmlinput.png[XML Input, scaledwidth=\"60%\"]\n\nThe 'trade' tag is defined as the 'root element' in the scenario above. Everything\nbetween '&lt;trade&gt;' and '&lt;/trade&gt;' is considered one 'fragment'. Spring Batch\nuses Object/XML Mapping (OXM) to bind fragments to objects. However, Spring Batch is not\ntied to any particular XML binding technology. Typical use is to delegate to\nlink:$$https://docs.spring.io/spring/docs/current/spring-framework-reference/data-access.html#oxm$$[Spring OXM], which\nprovides uniform abstraction for the most popular OXM technologies. The dependency on\nSpring OXM is optional and you can choose to implement Spring Batch specific interfaces\nif desired. The relationship to the technologies that OXM supports is shown in the\nfollowing image:\n\n.OXM Binding\nimage::oxm-fragments.png[OXM Binding, scaledwidth=\"60%\"]\n\nWith an introduction to OXM and how one can use XML fragments to represent records, we\ncan now more closely examine readers and writers.\n\n[[StaxEventItemReader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/xml-reading-writing.adoc", "title": "xml-reading-writing", "heading": "xml-reading-writing", "heading_level": 1, "file_order": 22, "section_index": 0, "content_hash": "fdd5660041cb89c14502f5d0f0b4638a08d0291607927b249eef25e97259afc9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/xml-reading-writing.adoc"}}
{"id": "sha256:23675ab960de0d92e92cd3dc7fadb4757030d1fa07590abc5f43df517cbbcbde", "content": "The `StaxEventItemReader` configuration provides a typical setup for the processing of\nrecords from an XML input stream. First, consider the following set of XML records that\nthe `StaxEventItemReader` can process:\n\n[source, xml]\n----\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<records>\n <trade xmlns=\"https://springframework.org/batch/sample/io/oxm/domain\">\n <isin>XYZ0001</isin>\n <quantity>5</quantity>\n <price>11.39</price>\n <customer>Customer1</customer>\n </trade>\n <trade xmlns=\"https://springframework.org/batch/sample/io/oxm/domain\">\n <isin>XYZ0002</isin>\n <quantity>2</quantity>\n <price>72.99</price>\n <customer>Customer2c</customer>\n </trade>\n <trade xmlns=\"https://springframework.org/batch/sample/io/oxm/domain\">\n <isin>XYZ0003</isin>\n <quantity>9</quantity>\n <price>99.99</price>\n <customer>Customer3</customer>\n </trade>\n</records>\n----\n\nTo be able to process the XML records, the following is needed:\n\n* Root Element Name: The name of the root element of the fragment that constitutes the\nobject to be mapped. The example configuration demonstrates this with the value of trade.\n* Resource: A Spring Resource that represents the file to read.\n* `Unmarshaller`: An unmarshalling facility provided by Spring OXM for mapping the XML\nfragment to an object.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to define a `StaxEventItemReader` that works with a root\nelement named `trade`, a resource of `data/iosample/input/input.xml`, and an unmarshaller\ncalled `tradeMarshaller` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic StaxEventItemReader itemReader() {\n\treturn new StaxEventItemReaderBuilder<Trade>()\n .name(\"itemReader\")\n .resource(new FileSystemResource(\"org/springframework/batch/infrastructure/item/xml/domain/trades.xml\"))\n .addFragmentRootElements(\"trade\")\n .unmarshaller(tradeMarshaller())\n .build();\n\n}\n----\n\nXML::\n+\nThe following example shows how to define a `StaxEventItemReader` that works with a root\nelement named `trade`, a resource of `data/iosample/input/input.xml`, and an unmarshaller\ncalled `tradeMarshaller` in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"itemReader\" class=\"org.springframework.batch.infrastructure.item.xml.StaxEventItemReader\">\n <property name=\"fragmentRootElementName\" value=\"trade\" />\n <property name=\"resource\" value=\"org/springframework/batch/infrastructure/item/xml/domain/trades.xml\" />\n <property name=\"unmarshaller\" ref=\"tradeMarshaller\" />\n</bean>\n----\n\n====\n\nNote that, in this example, we have chosen to use an `XStreamMarshaller`, which accepts\nan alias passed in as a map with the first key and value being the name of the fragment\n(that is, a root element) and the object type to bind. Then, similar to a `FieldSet`, the\nnames of the other elements that map to fields within the object type are described as\nkey/value pairs in the map. In the configuration file, we can use a Spring configuration\nutility to describe the required alias.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to describe the alias in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic XStreamMarshaller tradeMarshaller() {\n\tMap<String, Class> aliases = new HashMap<>();\n\taliases.put(\"trade\", Trade.class);\n\taliases.put(\"price\", BigDecimal.class);\n\taliases.put(\"isin\", String.class);\n\taliases.put(\"customer\", String.class);\n\taliases.put(\"quantity\", Long.class);\n\n\tXStreamMarshaller marshaller = new XStreamMarshaller();\n\n\tmarshaller.setAliases(aliases);\n\n\treturn marshaller;\n}\n----\n\nXML::\n+\nThe following example shows how to describe the alias in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"tradeMarshaller\"\n class=\"org.springframework.oxm.xstream.XStreamMarshaller\">\n <property name=\"aliases\">\n <util:map id=\"aliases\">\n <entry key=\"trade\"\n value=\"org.springframework.batch.samples.domain.trade.Trade\" />\n <entry key=\"price\" value=\"java.math.BigDecimal\" />\n <entry key=\"isin\" value=\"java.lang.String\" />\n <entry key=\"customer\" value=\"java.lang.String\" />\n <entry key=\"quantity\" value=\"java.lang.Long\" />\n </util:map>\n </property>\n</bean>\n----\n\n====\n\nOn input, the reader reads the XML resource until it recognizes that a new fragment is\nabout to start. By default, the reader matches the element name to recognize that a new\nfragment is about to start. The reader creates a standalone XML document from the\nfragment and passes the document to a deserializer (typically a wrapper around a Spring\nOXM `Unmarshaller`) to map the XML to a Java object.\n\nIn summary, this procedure is analogous to the following Java code, which uses the\ninjection provided by the Spring configuration:\n\n[source, java]\n----\nStaxEventItemReader<Trade> xmlStaxEventItemReader = new StaxEventItemReader<>();\nResource resource = new ByteArrayResource(xmlResource.getBytes());\n\nMap aliases = new HashMap();\naliases.put(\"trade\",\"org.springframework.batch.samples.domain.trade.Trade\");\naliases.put(\"price\",\"java.math.BigDecimal\");\naliases.put(\"customer\",\"java.lang.String\");\naliases.put(\"isin\",\"java.lang.String\");\naliases.put(\"quantity\",\"java.lang.Long\");\nXStreamMarshaller unmarshaller = new XStreamMarshaller();\nunmarshaller.setAliases(aliases);\nxmlStaxEventItemReader.setUnmarshaller(unmarshaller);\nxmlStaxEventItemReader.setResource(resource);\nxmlStaxEventItemReader.setFragmentRootElementName(\"trade\");\nxmlStaxEventItemReader.open(new ExecutionContext());\n\nboolean hasNext = true;\n\nTrade trade = null;\n\nwhile (hasNext) {\n trade = xmlStaxEventItemReader.read();\n if (trade == null) {\n hasNext = false;\n }\n else {\n System.out.println(trade);\n }\n}\n----\n\n[[StaxEventItemWriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/xml-reading-writing.adoc", "title": "xml-reading-writing", "heading": "`StaxEventItemReader`", "heading_level": 2, "file_order": 22, "section_index": 1, "content_hash": "23675ab960de0d92e92cd3dc7fadb4757030d1fa07590abc5f43df517cbbcbde", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/xml-reading-writing.adoc"}}
{"id": "sha256:5f062598d375a42a43ad208739626603364d2fd2ff377a51059aa21fee893ec6", "content": "Output works symmetrically to input. The `StaxEventItemWriter` needs a `Resource`, a\nmarshaller, and a `rootTagName`. A Java object is passed to a marshaller (typically a\nstandard Spring OXM Marshaller) which writes to a `Resource` by using a custom event\nwriter that filters the `StartDocument` and `EndDocument` events produced for each\nfragment by the OXM tools.\n\n[tabs]\n====\nJava::\n+\nThe following Java example uses the `MarshallingEventWriterSerializer`:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic StaxEventItemWriter itemWriter(Resource outputResource) {\n\treturn new StaxEventItemWriterBuilder<Trade>()\n .name(\"tradesWriter\")\n .marshaller(tradeMarshaller())\n .resource(outputResource)\n .rootTagName(\"trade\")\n .overwriteOutput(true)\n .build();\n\n}\n----\n\nXML::\n+\nThe following XML example uses the `MarshallingEventWriterSerializer`:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"itemWriter\" class=\"org.springframework.batch.infrastructure.item.xml.StaxEventItemWriter\">\n <property name=\"resource\" ref=\"outputResource\" />\n <property name=\"marshaller\" ref=\"tradeMarshaller\" />\n <property name=\"rootTagName\" value=\"trade\" />\n <property name=\"overwriteOutput\" value=\"true\" />\n</bean>\n----\n\n====\n\nThe preceding configuration sets up the three required properties and sets the optional\n`overwriteOutput=true` attrbute, mentioned earlier in this chapter for specifying whether\nan existing file can be overwritten.\n\n[tabs]\n====\nJava::\n+\nThe following Java example uses the same marshaller as the one used in the reading example\nshown earlier in the chapter:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic XStreamMarshaller customerCreditMarshaller() {\n\tXStreamMarshaller marshaller = new XStreamMarshaller();\n\n\tMap<String, Class> aliases = new HashMap<>();\n\taliases.put(\"trade\", Trade.class);\n\taliases.put(\"price\", BigDecimal.class);\n\taliases.put(\"isin\", String.class);\n\taliases.put(\"customer\", String.class);\n\taliases.put(\"quantity\", Long.class);\n\n\tmarshaller.setAliases(aliases);\n\n\treturn marshaller;\n}\n----\n\nXML::\n+\nThe following XML example uses the same marshaller as the one used in the reading example\nshown earlier in the chapter:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"customerCreditMarshaller\"\n class=\"org.springframework.oxm.xstream.XStreamMarshaller\">\n <property name=\"aliases\">\n <util:map id=\"aliases\">\n <entry key=\"customer\"\n value=\"org.springframework.batch.samples.domain.trade.Trade\" />\n <entry key=\"price\" value=\"java.math.BigDecimal\" />\n <entry key=\"isin\" value=\"java.lang.String\" />\n <entry key=\"customer\" value=\"java.lang.String\" />\n <entry key=\"quantity\" value=\"java.lang.Long\" />\n </util:map>\n </property>\n</bean>\n----\n\n====\n\nTo summarize with a Java example, the following code illustrates all of the points\ndiscussed, demonstrating the programmatic setup of the required properties:\n\n[source, java]\n----\nFileSystemResource resource = new FileSystemResource(\"data/outputFile.xml\")\n\nMap aliases = new HashMap();\naliases.put(\"trade\",\"org.springframework.batch.samples.domain.trade.Trade\");\naliases.put(\"price\",\"java.math.BigDecimal\");\naliases.put(\"customer\",\"java.lang.String\");\naliases.put(\"isin\",\"java.lang.String\");\naliases.put(\"quantity\",\"java.lang.Long\");\nMarshaller marshaller = new XStreamMarshaller();\nmarshaller.setAliases(aliases);\n\nStaxEventItemWriter staxItemWriter =\n\tnew StaxEventItemWriterBuilder<Trade>()\n .name(\"tradesWriter\")\n .marshaller(marshaller)\n .resource(resource)\n .rootTagName(\"trade\")\n .overwriteOutput(true)\n .build();\n\nstaxItemWriter.afterPropertiesSet();\n\nExecutionContext executionContext = new ExecutionContext();\nstaxItemWriter.open(executionContext);\nTrade trade = new Trade();\ntrade.setPrice(11.39);\ntrade.setIsin(\"XYZ0001\");\ntrade.setQuantity(5L);\ntrade.setCustomer(\"Customer1\");\nstaxItemWriter.write(trade);\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readers-and-writers/xml-reading-writing.adoc", "title": "xml-reading-writing", "heading": "`StaxEventItemWriter`", "heading_level": 2, "file_order": 22, "section_index": 2, "content_hash": "5f062598d375a42a43ad208739626603364d2fd2ff377a51059aa21fee893ec6", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readers-and-writers/xml-reading-writing.adoc"}}
{"id": "sha256:f4bfadb4fcfa83408061e653f5f7c08ab4b060dc2ac082f940eed4e6dd72e174", "content": "[[asynchronous-processors]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/asynchronous-processing.adoc", "title": "asynchronous-processing", "heading": "asynchronous-processing", "heading_level": 1, "file_order": 23, "section_index": 0, "content_hash": "f4bfadb4fcfa83408061e653f5f7c08ab4b060dc2ac082f940eed4e6dd72e174", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/asynchronous-processing.adoc"}}
{"id": "sha256:3328c84c362e9d4487c15878c9cb249a64469dd1c25c65008e986e328d84db19", "content": "Asynchronous Processors help you scale the processing of items. In the asynchronous\nprocessor use case, an `AsyncItemProcessor` serves as a dispatcher, executing the logic of\nthe `ItemProcessor` for an item on a new thread. Once the item completes, the `Future` is\npassed to the `AsyncItemWriter` to be written.\n\nTherefore, you can increase performance by using asynchronous item processing, basically\nletting you implement fork-join scenarios. The `AsyncItemWriter` gathers the results and\nwrites back the chunk as soon as all the results become available.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to configuration the `AsyncItemProcessor` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic AsyncItemProcessor processor(ItemProcessor itemProcessor, TaskExecutor taskExecutor) {\n AsyncItemProcessor asyncItemProcessor = new AsyncItemProcessor();\n asyncItemProcessor.setTaskExecutor(taskExecutor);\n asyncItemProcessor.setDelegate(itemProcessor);\n return asyncItemProcessor;\n}\n----\n\nXML::\n+\nThe following example shows how to configuration the `AsyncItemProcessor` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"processor\"\n class=\"org.springframework.batch.integration.async.AsyncItemProcessor\">\n <property name=\"delegate\">\n <bean class=\"your.ItemProcessor\"/>\n </property>\n <property name=\"taskExecutor\">\n <bean class=\"org.springframework.core.task.SimpleAsyncTaskExecutor\"/>\n </property>\n</bean>\n----\n\n====\n\nThe `delegate` property refers to your `ItemProcessor` bean, and the `taskExecutor`\nproperty refers to the `TaskExecutor` of your choice.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to configure the `AsyncItemWriter` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic AsyncItemWriter writer(ItemWriter itemWriter) {\n AsyncItemWriter asyncItemWriter = new AsyncItemWriter();\n asyncItemWriter.setDelegate(itemWriter);\n return asyncItemWriter;\n}\n----\n\nXML::\n+\nThe following example shows how to configure the `AsyncItemWriter` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"itemWriter\"\n class=\"org.springframework.batch.integration.async.AsyncItemWriter\">\n <property name=\"delegate\">\n <bean id=\"itemWriter\" class=\"your.ItemWriter\"/>\n </property>\n</bean>\n----\n\n====\n\nAgain, the `delegate` property is\nactually a reference to your `ItemWriter` bean.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/asynchronous-processing.adoc", "title": "asynchronous-processing", "heading": "Asynchronous Processors", "heading_level": 2, "file_order": 23, "section_index": 1, "content_hash": "3328c84c362e9d4487c15878c9cb249a64469dd1c25c65008e986e328d84db19", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/asynchronous-processing.adoc"}}
{"id": "sha256:9ae5e2ea78efc058f080875b1bc7f441f2d6e3976ef95be8621b0abf6f4d7b5c", "content": "[[availableAttributesOfTheJobLaunchingGateway]]\n\nThe job-launching gateway has the following attributes that you can set to control a job:\n\n* `id`: Identifies the underlying Spring bean definition, which is an instance of either:\n** `EventDrivenConsumer`\n** `PollingConsumer`\n(The exact implementation depends on whether the component's input channel is a\n`SubscribableChannel` or a `PollableChannel`.)\n* `auto-startup`: Boolean flag to indicate that the endpoint should start automatically on\nstartup. The default is `true`.\n* `request-channel`: The input `MessageChannel` of this endpoint.\n* `reply-channel`: `MessageChannel` to which the resulting `JobExecution` payload is sent.\n* `reply-timeout`: Lets you specify how long (in milliseconds) this gateway waits for the reply message\nto be sent successfully to the reply channel before throwing\nan exception. This attribute applies only when the channel\nmight block (for example, when using a bounded queue channel\nthat is currently full). Also, keep in mind that, when sending to a\n`DirectChannel`, the invocation occurs\nin the sender's thread. Therefore, the failing of the send\noperation may be caused by other components further downstream.\nThe `reply-timeout` attribute maps to the\n`sendTimeout` property of the underlying\n`MessagingTemplate` instance. If not specified, the attribute\ndefaults to -1,\nmeaning that, by default, the `Gateway` waits indefinitely.\n* `job-launcher`: Optional. Accepts a\ncustom\n`JobLauncher`\nbean reference.\nIf not specified, the adapter\nre-uses the instance that is registered under the `id` of\n`jobLauncher`. If no default instance\nexists, an exception is thrown.\n* `order`: Specifies the order of invocation when this endpoint is connected as a subscriber\nto a `SubscribableChannel`.\n\nWhen this `Gateway` is receiving messages from a\n`PollableChannel`, you must either provide\na global default `Poller` or provide a `Poller` sub-element to the\n`Job Launching Gateway`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to provide a poller in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\n@ServiceActivator(inputChannel = \"queueChannel\", poller = @Poller(fixedRate=\"1000\"))\npublic JobLaunchingGateway sampleJobLaunchingGateway() {\n JobLaunchingGateway jobLaunchingGateway = new JobLaunchingGateway(jobLauncher());\n jobLaunchingGateway.setOutputChannel(replyChannel());\n return jobLaunchingGateway;\n}\n----\n\nXML::\n+\nThe following example shows how to provide a poller in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<batch-int:job-launching-gateway request-channel=\"queueChannel\"\n reply-channel=\"replyChannel\" job-launcher=\"jobLauncher\">\n <int:poller fixed-rate=\"1000\">\n</batch-int:job-launching-gateway>\n----\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/available-attributes-of-the-job-launching-gateway.adoc", "title": "available-attributes-of-the-job-launching-gateway", "heading": "available-attributes-of-the-job-launching-gateway", "heading_level": 1, "file_order": 24, "section_index": 0, "content_hash": "9ae5e2ea78efc058f080875b1bc7f441f2d6e3976ef95be8621b0abf6f4d7b5c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/available-attributes-of-the-job-launching-gateway.adoc"}}
{"id": "sha256:12f7497fa15bce3fea9187b9fe833e1af321ed9717747376d686b685ce3217ee", "content": "[[externalizing-batch-process-execution]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/externalizing-execution.adoc", "title": "externalizing-execution", "heading": "externalizing-execution", "heading_level": 1, "file_order": 25, "section_index": 0, "content_hash": "12f7497fa15bce3fea9187b9fe833e1af321ed9717747376d686b685ce3217ee", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/externalizing-execution.adoc"}}
{"id": "sha256:1382696af45dc22053c9c61e55089ed6f3b7334c854ff1d30c1ee08ea408e751", "content": "The integration approaches discussed so far suggest use cases\nwhere Spring Integration wraps Spring Batch like an outer shell.\nHowever, Spring Batch can also use Spring Integration internally.\nBy using this approach, Spring Batch users can delegate the\nprocessing of items or even chunks to outside processes. This\nlets you offload complex processing. Spring Batch Integration\nprovides dedicated support for:\n\n* Remote Chunking\n* Remote Partitioning\n\n[[remote-chunking]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/externalizing-execution.adoc", "title": "externalizing-execution", "heading": "Externalizing Batch Process Execution", "heading_level": 2, "file_order": 25, "section_index": 1, "content_hash": "1382696af45dc22053c9c61e55089ed6f3b7334c854ff1d30c1ee08ea408e751", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/externalizing-execution.adoc"}}
{"id": "sha256:f47efb669783472ad0c8da37fc3d331949885bbc847e1d0e2984efdb545fdfa8", "content": "The following image shows one way that remote chunking works when you use Spring Batch\ntogether with Spring Integration:\n\n.Remote Chunking\nimage::remote-chunking-sbi.png[Remote Chunking, scaledwidth=\"60%\"]\n\nTaking things one step further, you can also externalize the\nchunk processing by using the\n`ChunkMessageChannelItemWriter`\n(provided by Spring Batch Integration), which sends items out\nand collects the result. Once sent, Spring Batch continues the\nprocess of reading and grouping items, without waiting for the results.\nRather, it is the responsibility of the `ChunkMessageChannelItemWriter`\nto gather the results and integrate them back into the Spring Batch process.\n\nWith Spring Integration, you have full\ncontrol over the concurrency of your processes (for instance, by\nusing a `QueueChannel` instead of a\n`DirectChannel`). Furthermore, by relying on\nSpring Integration's rich collection of channel adapters (such as\nJMS and AMQP), you can distribute chunks of a batch job to\nexternal systems for processing.\n\n[tabs]\n====\nJava::\n+\nA job with a step to be remotely chunked might have a configuration similar to the\nfollowing in Java:\n+\n.Java Configuration\n[source, java]\n----\npublic Job chunkJob(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n return new JobBuilder(\"personJob\", jobRepository)\n .start(new StepBuilder(\"step1\", jobRepository)\n .<Person, Person>chunk(200, transactionManager)\n .reader(itemReader())\n .writer(itemWriter())\n .build())\n .build();\n }\n----\n\nXML::\n+\nA job with a step to be remotely chunked might have a configuration similar to the\nfollowing in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"personJob\">\n <step id=\"step1\">\n <tasklet>\n <chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"200\"/>\n </tasklet>\n ...\n </step>\n</job>\n----\n\n====\n\nThe `ItemReader` reference points to the bean you want to use for reading data on the\nmanager. The `ItemWriter` reference points to a special `ItemWriter` (called\n`ChunkMessageChannelItemWriter`), as described earlier. The processor (if any) is left off\nthe manager configuration, as it is configured on the worker. You should check any\nadditional component properties, such as throttle limits and so on, when implementing\nyour use case.\n\n[tabs]\n====\nJava::\n+\nThe following Java configuration provides a basic manager setup:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic org.apache.activemq.ActiveMQConnectionFactory connectionFactory() {\n ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory();\n factory.setBrokerURL(\"tcp://localhost:61616\");\n return factory;\n}\n\n/*\n * Configure outbound flow (requests going to workers)\n */\n@Bean\npublic DirectChannel requests() {\n return new DirectChannel();\n}\n\n@Bean\npublic IntegrationFlow outboundFlow(ActiveMQConnectionFactory connectionFactory) {\n return IntegrationFlow\n .from(requests())\n .handle(Jms.outboundAdapter(connectionFactory).destination(\"requests\"))\n .get();\n}\n\n/*\n * Configure inbound flow (replies coming from workers)\n */\n@Bean\npublic QueueChannel replies() {\n return new QueueChannel();\n}\n\n@Bean\npublic IntegrationFlow inboundFlow(ActiveMQConnectionFactory connectionFactory) {\n return IntegrationFlow\n .from(Jms.messageDrivenChannelAdapter(connectionFactory).destination(\"replies\"))\n .channel(replies())\n .get();\n}\n\n/*\n * Configure the ChunkMessageChannelItemWriter\n */\n@Bean\npublic ItemWriter<Integer> itemWriter() {\n MessagingTemplate messagingTemplate = new MessagingTemplate();\n messagingTemplate.setDefaultChannel(requests());\n messagingTemplate.setReceiveTimeout(2000);\n ChunkMessageChannelItemWriter<Integer> chunkMessageChannelItemWriter\n chunkMessageChannelItemWriter.setMessagingOperations(messagingTemplate);\n chunkMessageChannelItemWriter.setReplyChannel(replies());\n return chunkMessageChannelItemWriter;\n}\n----\n\nXML::\n+\nThe following XML configuration provides a basic manager setup:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"connectionFactory\" class=\"org.apache.activemq.ActiveMQConnectionFactory\">\n <property name=\"brokerURL\" value=\"tcp://localhost:61616\"/>\n</bean>\n\n<int-jms:outbound-channel-adapter id=\"jmsRequests\" destination-name=\"requests\"/>\n\n<bean id=\"messagingTemplate\"\n class=\"org.springframework.integration.core.MessagingTemplate\">\n <property name=\"defaultChannel\" ref=\"requests\"/>\n <property name=\"receiveTimeout\" value=\"2000\"/>\n</bean>\n\n<bean id=\"itemWriter\"\n class=\"org.springframework.batch.integration.chunk.ChunkMessageChannelItemWriter\"\n scope=\"step\">\n <property name=\"messagingOperations\" ref=\"messagingTemplate\"/>\n <property name=\"replyChannel\" ref=\"replies\"/>\n</bean>\n\n<int:channel id=\"replies\">\n <int:queue/>\n</int:channel>\n\n<int-jms:message-driven-channel-adapter id=\"jmsReplies\"\n destination-name=\"replies\"\n channel=\"replies\"/>\n----\n\n====\n\nThe preceding configuration provides us with a number of beans. We\nconfigure our messaging middleware by using ActiveMQ and the\ninbound and outbound JMS adapters provided by Spring Integration. As\nshown, our `itemWriter` bean, which is\nreferenced by our job step, uses the\n`ChunkMessageChannelItemWriter` to write chunks over the\nconfigured middleware.\n\nNow we can move on to the worker configuration, as the following example shows:\n\n[tabs]\n====\nJava::\n+\nThe following example shows the worker configuration in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic org.apache.activemq.ActiveMQConnectionFactory connectionFactory() {\n ActiveMQConnectionFactory factory = new ActiveMQConnectionFactory();\n factory.setBrokerURL(\"tcp://localhost:61616\");\n return factory;\n}\n\n/*\n * Configure inbound flow (requests coming from the manager)\n */\n@Bean\npublic DirectChannel requests() {\n return new DirectChannel();\n}\n\n@Bean\npublic IntegrationFlow inboundFlow(ActiveMQConnectionFactory connectionFactory) {\n return IntegrationFlow\n .from(Jms.messageDrivenChannelAdapter(connectionFactory).destination(\"requests\"))\n .channel(requests())\n .get();\n}\n\n/*\n * Configure outbound flow (replies going to the manager)\n */\n@Bean\npublic DirectChannel replies() {\n return new DirectChannel();\n}\n\n@Bean\npublic IntegrationFlow outboundFlow(ActiveMQConnectionFactory connectionFactory) {\n return IntegrationFlow\n .from(replies())\n .handle(Jms.outboundAdapter(connectionFactory).destination(\"replies\"))\n .get();\n}\n\n/*\n * Configure the ChunkProcessorChunkHandler\n */\n@Bean\n@ServiceActivator(inputChannel = \"requests\", outputChannel = \"replies\")\npublic ChunkProcessorChunkHandler<Integer> chunkProcessorChunkHandler() {\n ChunkProcessor<Integer> chunkProcessor\n ChunkProcessorChunkHandler<Integer> chunkProcessorChunkHandler\n chunkProcessorChunkHandler.setChunkProcessor(chunkProcessor);\n return chunkProcessorChunkHandler;\n}\n----\n\nXML::\n+\nThe following example shows the worker configuration in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"connectionFactory\" class=\"org.apache.activemq.ActiveMQConnectionFactory\">\n <property name=\"brokerURL\" value=\"tcp://localhost:61616\"/>\n</bean>\n\n<int:channel id=\"requests\"/>\n<int:channel id=\"replies\"/>\n\n<int-jms:message-driven-channel-adapter id=\"incomingRequests\"\n destination-name=\"requests\"\n channel=\"requests\"/>\n\n<int-jms:outbound-channel-adapter id=\"outgoingReplies\"\n destination-name=\"replies\"\n channel=\"replies\">\n</int-jms:outbound-channel-adapter>\n\n<int:service-activator id=\"serviceActivator\"\n input-channel=\"requests\"\n output-channel=\"replies\"\n ref=\"chunkProcessorChunkHandler\"\n method=\"handleChunk\"/>\n\n<bean id=\"chunkProcessorChunkHandler\"\n class=\"org.springframework.batch.integration.chunk.ChunkProcessorChunkRequestHandler\">\n <property name=\"chunkProcessor\">\n <bean class=\"org.springframework.batch.core.step.item.SimpleChunkProcessor\">\n <property name=\"itemWriter\">\n <bean class=\"io.spring.sbi.PersonItemWriter\"/>\n </property>\n <property name=\"itemProcessor\">\n <bean class=\"io.spring.sbi.PersonItemProcessor\"/>\n </property>\n </bean>\n </property>\n</bean>\n----\n\n====\n\nMost of these configuration items should look familiar from the\nmanager configuration. Workers do not need access to\nthe Spring Batch `JobRepository` nor\nto the actual job configuration file. The main bean of interest\nis the `chunkProcessorChunkHandler`. The\n`chunkProcessor` property of `ChunkProcessorChunkRequestHandler` takes a\nconfigured `SimpleChunkProcessor`, which is where you would provide a reference to your\n`ItemWriter` (and, optionally, your\n`ItemProcessor`) that will run on the worker\nwhen it receives chunks from the manager.\n\nFor more information, see the section of the \"`Scalability`\" chapter on\nlink:$$https://docs.spring.io/spring-batch/docs/current/reference/html/scalability.html#remoteChunking$$[Remote Chunking].\n\nStarting from version 4.1, Spring Batch Integration introduces the `@EnableBatchIntegration`\nannotation that can be used to simplify a remote chunking setup. This annotation provides\ntwo beans that you can autowire in your application context:\n\n* `RemoteChunkingManagerStepBuilderFactory`: Configures the manager step\n* `RemoteChunkingWorkerBuilder`: Configures the remote worker integration flow\n\nThese APIs take care of configuring a number of components, as the following diagram shows:\n\n.Remote Chunking Configuration\nimage::remote-chunking-config.png[Remote Chunking Configuration, scaledwidth=\"80%\"]\n\nOn the manager side, the `RemoteChunkingManagerStepBuilderFactory` lets you\nconfigure a manager step by declaring:\n\n* The item reader to read items and send them to workers\n* The output channel (\"Outgoing requests\") to send requests to workers\n* The input channel (\"Incoming replies\") to receive replies from workers\n\nYou need not explicitly configure `ChunkMessageChannelItemWriter` and the `MessagingTemplate`.\n(You can still explicitly configure them if find a reason to do so).\n\nOn the worker side, the `RemoteChunkingWorkerBuilder` lets you configure a worker to:\n\n* Listen to requests sent by the manager on the input channel (\"`Incoming requests`\")\n* Call the `handleChunk` method of `ChunkProcessorChunkRequestHandler` for each request\nwith the configured `ItemProcessor` and `ItemWriter`\n* Send replies on the output channel (\"`Outgoing replies`\") to the manager\n\nYou need not explicitly configure the `SimpleChunkProcessor`\nand the `ChunkProcessorChunkRequestHandler`. (You can still explicitly configure them if you find\n a reason to do so).\n\nThe following example shows how to use these APIs:\n\n[source, java]\n----\n@EnableBatchIntegration\n@EnableBatchProcessing\npublic class RemoteChunkingJobConfiguration {\n\n @Configuration\n public static class ManagerConfiguration {\n\n @Autowired\n private RemoteChunkingManagerStepBuilderFactory managerStepBuilderFactory;\n\n @Bean\n public TaskletStep managerStep() {\n return this.managerStepBuilderFactory.get(\"managerStep\")\n .chunk(100)\n .reader(itemReader())\n .outputChannel(requests()) // requests sent to workers\n .inputChannel(replies()) // replies received from workers\n .build();\n }\n\n // Middleware beans setup omitted\n\n }\n\n @Configuration\n public static class WorkerConfiguration {\n\n @Autowired\n private RemoteChunkingWorkerBuilder workerBuilder;\n\n @Bean\n public IntegrationFlow workerFlow() {\n return this.workerBuilder\n .itemProcessor(itemProcessor())\n .itemWriter(itemWriter())\n .inputChannel(requests()) // requests received from the manager\n .outputChannel(replies()) // replies sent to the manager\n .build();\n }\n\n // Middleware beans setup omitted\n\n }\n\n}\n----\n\nYou can find a complete example of a remote chunking job\nlink:$$https://github.com/spring-projects/spring-batch/tree/main/spring-batch-samples#remote-chunking-sample$$[here].\n\n[[remote-partitioning]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/externalizing-execution.adoc", "title": "externalizing-execution", "heading": "Remote Chunking", "heading_level": 3, "file_order": 25, "section_index": 2, "content_hash": "f47efb669783472ad0c8da37fc3d331949885bbc847e1d0e2984efdb545fdfa8", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/externalizing-execution.adoc"}}
{"id": "sha256:2005234f1ce0ea740709f927f192eef57533f1094d105189df3d41ed9479f33e", "content": "The following image shows a typical remote partitioning situation:\n\n.Remote Partitioning\nimage::remote-partitioning.png[Remote Partitioning, scaledwidth=\"60%\"]\n\nRemote Partitioning, on the other hand, is useful when it\nis not the processing of items but rather the associated I/O that\ncauses the bottleneck. With remote partitioning, you can send work\nto workers that execute complete Spring Batch\nsteps. Thus, each worker has its own `ItemReader`, `ItemProcessor`, and\n`ItemWriter`. For this purpose, Spring Batch\nIntegration provides the `MessageChannelPartitionHandler`.\n\nThis implementation of the `PartitionHandler`\ninterface uses `MessageChannel` instances to\nsend instructions to remote workers and receive their responses.\nThis provides a nice abstraction from the transports (such as JMS\nand AMQP) being used to communicate with the remote workers.\n\nThe section of the \"`Scalability`\" chapter that addresses\nxref:scalability.adoc#partitioning[remote partitioning] provides an overview of the concepts and\ncomponents needed to configure remote partitioning and shows an\nexample of using the default\n`TaskExecutorPartitionHandler` to partition\nin separate local threads of execution. For remote partitioning\nto multiple JVMs, two additional components are required:\n\n* A remoting fabric or grid environment\n* A `PartitionHandler` implementation that supports the desired\nremoting fabric or grid environment\n\nSimilar to remote chunking, you can use JMS as the \"`remoting fabric`\". In that case, use\na `MessageChannelPartitionHandler` instance as the `PartitionHandler` implementation,\nas described earlier.\n\n[tabs]\n====\nJava::\n+\nThe following example assumes an existing partitioned job and focuses on the\n`MessageChannelPartitionHandler` and JMS configuration in Java:\n+\n.Java Configuration\n[source, java]\n----\n/*\n * Configuration of the manager side\n */\n@Bean\npublic PartitionHandler partitionHandler() {\n MessageChannelPartitionHandler partitionHandler = new MessageChannelPartitionHandler();\n partitionHandler.setStepName(\"step1\");\n partitionHandler.setGridSize(3);\n partitionHandler.setReplyChannel(outboundReplies());\n MessagingTemplate template = new MessagingTemplate();\n template.setDefaultChannel(outboundRequests());\n template.setReceiveTimeout(100000);\n partitionHandler.setMessagingOperations(template);\n return partitionHandler;\n}\n\n@Bean\npublic QueueChannel outboundReplies() {\n return new QueueChannel();\n}\n\n@Bean\npublic DirectChannel outboundRequests() {\n return new DirectChannel();\n}\n\n@Bean\npublic IntegrationFlow outboundJmsRequests() {\n return IntegrationFlow.from(\"outboundRequests\")\n .handle(Jms.outboundGateway(connectionFactory())\n .requestDestination(\"requestsQueue\"))\n .get();\n}\n\n@Bean\n@ServiceActivator(inputChannel = \"inboundStaging\")\npublic AggregatorFactoryBean partitioningMessageHandler() throws Exception {\n AggregatorFactoryBean aggregatorFactoryBean = new AggregatorFactoryBean();\n aggregatorFactoryBean.setProcessorBean(partitionHandler());\n aggregatorFactoryBean.setOutputChannel(outboundReplies());\n // configure other propeties of the aggregatorFactoryBean\n return aggregatorFactoryBean;\n}\n\n@Bean\npublic DirectChannel inboundStaging() {\n return new DirectChannel();\n}\n\n@Bean\npublic IntegrationFlow inboundJmsStaging() {\n return IntegrationFlow\n .from(Jms.messageDrivenChannelAdapter(connectionFactory())\n .configureListenerContainer(c -> c.subscriptionDurable(false))\n .destination(\"stagingQueue\"))\n .channel(inboundStaging())\n .get();\n}\n\n/*\n * Configuration of the worker side\n */\n@Bean\npublic StepExecutionRequestHandler stepExecutionRequestHandler() {\n StepExecutionRequestHandler stepExecutionRequestHandler = new StepExecutionRequestHandler();\n stepExecutionRequestHandler.setJobExplorer(jobExplorer);\n stepExecutionRequestHandler.setStepLocator(stepLocator());\n return stepExecutionRequestHandler;\n}\n\n@Bean\n@ServiceActivator(inputChannel = \"inboundRequests\", outputChannel = \"outboundStaging\")\npublic StepExecutionRequestHandler serviceActivator() throws Exception {\n return stepExecutionRequestHandler();\n}\n\n@Bean\npublic DirectChannel inboundRequests() {\n return new DirectChannel();\n}\n\npublic IntegrationFlow inboundJmsRequests() {\n return IntegrationFlow\n .from(Jms.messageDrivenChannelAdapter(connectionFactory())\n .configureListenerContainer(c -> c.subscriptionDurable(false))\n .destination(\"requestsQueue\"))\n .channel(inboundRequests())\n .get();\n}\n\n@Bean\npublic DirectChannel outboundStaging() {\n return new DirectChannel();\n}\n\n@Bean\npublic IntegrationFlow outboundJmsStaging() {\n return IntegrationFlow.from(\"outboundStaging\")\n .handle(Jms.outboundGateway(connectionFactory())\n .requestDestination(\"stagingQueue\"))\n .get();\n}\n----\n\nXML::\n+\nThe following example assumes an existing partitioned job and focuses on the\n`MessageChannelPartitionHandler` and JMS configuration in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"partitionHandler\"\n class=\"org.springframework.batch.integration.partition.MessageChannelPartitionHandler\">\n <property name=\"stepName\" value=\"step1\"/>\n <property name=\"gridSize\" value=\"3\"/>\n <property name=\"replyChannel\" ref=\"outbound-replies\"/>\n <property name=\"messagingOperations\">\n <bean class=\"org.springframework.integration.core.MessagingTemplate\">\n <property name=\"defaultChannel\" ref=\"outbound-requests\"/>\n <property name=\"receiveTimeout\" value=\"100000\"/>\n </bean>\n </property>\n</bean>\n\n<int:channel id=\"outbound-requests\"/>\n<int-jms:outbound-channel-adapter destination=\"requestsQueue\"\n channel=\"outbound-requests\"/>\n\n<int:channel id=\"inbound-requests\"/>\n<int-jms:message-driven-channel-adapter destination=\"requestsQueue\"\n channel=\"inbound-requests\"/>\n\n<bean id=\"stepExecutionRequestHandler\"\n class=\"org.springframework.batch.integration.partition.StepExecutionRequestHandler\">\n <property name=\"jobExplorer\" ref=\"jobExplorer\"/>\n <property name=\"stepLocator\" ref=\"stepLocator\"/>\n</bean>\n\n<int:service-activator ref=\"stepExecutionRequestHandler\" input-channel=\"inbound-requests\"\n output-channel=\"outbound-staging\"/>\n\n<int:channel id=\"outbound-staging\"/>\n<int-jms:outbound-channel-adapter destination=\"stagingQueue\"\n channel=\"outbound-staging\"/>\n\n<int:channel id=\"inbound-staging\"/>\n<int-jms:message-driven-channel-adapter destination=\"stagingQueue\"\n channel=\"inbound-staging\"/>\n\n<int:aggregator ref=\"partitionHandler\" input-channel=\"inbound-staging\"\n output-channel=\"outbound-replies\"/>\n\n<int:channel id=\"outbound-replies\">\n <int:queue/>\n</int:channel>\n\n<bean id=\"stepLocator\"\n class=\"org.springframework.batch.integration.partition.BeanFactoryStepLocator\" />\n----\n\n====\n\nYou must also ensure that the partition `handler` attribute maps to the `partitionHandler`\nbean.\n\n[tabs]\n====\nJava::\n+\nThe following example maps the partition `handler` attribute to the `partitionHandler` in\nJava:\n+\n.Java Configuration\n[source, java]\n----\n\tpublic Job personJob(JobRepository jobRepository) {\n return new JobBuilder(\"personJob\", jobRepository)\n .start(new StepBuilder(\"step1.manager\", jobRepository)\n .partitioner(\"step1.worker\", partitioner())\n .partitionHandler(partitionHandler())\n .build())\n .build();\n\t}\n----\n\nXML::\n+\nThe following example maps the partition `handler` attribute to the `partitionHandler` in\nXML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"personJob\">\n <step id=\"step1.manager\">\n <partition partitioner=\"partitioner\" handler=\"partitionHandler\"/>\n ...\n </step>\n</job>\n----\n\n====\n\nYou can find a complete example of a remote partitioning job\nlink:$$https://github.com/spring-projects/spring-batch/tree/main/spring-batch-samples#remote-partitioning-sample$$[here].\n\nYou can use the `@EnableBatchIntegration` annotation to simplify a remote\npartitioning setup. This annotation provides two beans that are useful for remote partitioning:\n\n* `RemotePartitioningManagerStepBuilderFactory`: Configures the manager step\n* `RemotePartitioningWorkerStepBuilderFactory`: Configures the worker step\n\nThese APIs take care of configuring a number of components, as the following diagrams show:\n\n.Remote Partitioning Configuration (with job repository polling)\nimage::remote-partitioning-polling-config.png[Remote Partitioning Configuration (with job repository polling), scaledwidth=\"80%\"]\n\n.Remote Partitioning Configuration (with replies aggregation)\nimage::remote-partitioning-aggregation-config.png[Remote Partitioning Configuration (with replies aggregation), scaledwidth=\"80%\"]\n\nOn the manager side, the `RemotePartitioningManagerStepBuilderFactory` lets you\nconfigure a manager step by declaring:\n\n* The `Partitioner` used to partition data\n* The output channel (\"`Outgoing requests`\") on which to send requests to workers\n* The input channel (\"`Incoming replies`\") on which to receive replies from workers (when configuring replies aggregation)\n* The poll interval and timeout parameters (when configuring job repository polling)\n\nYou need not explicitly configure The `MessageChannelPartitionHandler` and the `MessagingTemplate`.\n(You can still explicitly configured them if you find a reason to do so).\n\nOn the worker side, the `RemotePartitioningWorkerStepBuilderFactory` lets you configure a worker to:\n\n* Listen to requests sent by the manager on the input channel (\"`Incoming requests`\")\n* Call the `handle` method of `StepExecutionRequestHandler` for each request\n* Send replies on the output channel (\"`Outgoing replies`\") to the manager\n\nYou need not explicitly configure the `StepExecutionRequestHandler`.\n(You can explicitly configure it if you find a reason to do so).\n\nThe following example shows how to use these APIs:\n\n[source, java]\n----\n@Configuration\n@EnableBatchProcessing\n@EnableBatchIntegration\npublic class RemotePartitioningJobConfiguration {\n\n @Configuration\n public static class ManagerConfiguration {\n\n @Autowired\n private RemotePartitioningManagerStepBuilderFactory managerStepBuilderFactory;\n\n @Bean\n public Step managerStep() {\n return this.managerStepBuilderFactory\n .get(\"managerStep\")\n .partitioner(\"workerStep\", partitioner())\n .gridSize(10)\n .outputChannel(outgoingRequestsToWorkers())\n .inputChannel(incomingRepliesFromWorkers())\n .build();\n }\n\n // Middleware beans setup omitted\n\n }\n\n @Configuration\n public static class WorkerConfiguration {\n\n @Autowired\n private RemotePartitioningWorkerStepBuilderFactory workerStepBuilderFactory;\n\n @Bean\n public Step workerStep() {\n return this.workerStepBuilderFactory\n .get(\"workerStep\")\n .inputChannel(incomingRequestsFromManager())\n .outputChannel(outgoingRepliesToManager())\n .chunk(100)\n .reader(itemReader())\n .processor(itemProcessor())\n .writer(itemWriter())\n .build();\n }\n\n // Middleware beans setup omitted\n\n }\n\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/externalizing-execution.adoc", "title": "externalizing-execution", "heading": "Remote Partitioning", "heading_level": 3, "file_order": 25, "section_index": 3, "content_hash": "2005234f1ce0ea740709f927f192eef57533f1094d105189df3d41ed9479f33e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/externalizing-execution.adoc"}}
{"id": "sha256:7c2f170fb5d5411df84d7b4024f5c7176ed6d7adae94b8ff4ab1054f091b48ed", "content": "[[launching-batch-jobs-through-messages]]\n\nWhen starting batch jobs by using the core Spring Batch API, you\nbasically have two options:\n\n* From the command line, with the `CommandLineJobOperator`\n* Programmatically, with either `JobOperator.start()`\n\nFor example, you may want to use the\n`CommandLineJobOperator` when invoking batch jobs by\nusing a shell script. Alternatively, you can use the\n`JobOperator` directly (for example, when using\nSpring Batch as part of a web application). However, what about\nmore complex use cases? Maybe you need to poll a remote (S)FTP\nserver to retrieve the data for the Batch Job or your application\nhas to support multiple different data sources simultaneously. For\nexample, you may receive data files not only from the web but also from\nFTP and other sources. Maybe additional transformation of the input files is\nneeded before invoking Spring Batch.\n\nTherefore, it would be much more powerful to execute the batch job\nby using Spring Integration and its numerous adapters. For example,\nyou can use a _File Inbound Channel Adapter_ to\nmonitor a directory in the file-system and start the batch job as\nsoon as the input file arrives. Additionally, you can create Spring\nIntegration flows that use multiple different adapters to easily\ningest data for your batch jobs from multiple sources\nsimultaneously by using only configuration. Implementing all these\nscenarios with Spring Integration is easy, as it allows for\ndecoupled, event-driven execution of the\n`JobOperator`.\n\nSpring Batch Integration provides the\n`JobLaunchingMessageHandler` class that you can\nuse to launch batch jobs. The input for the\n`JobLaunchingMessageHandler` is provided by a\nSpring Integration message, which has a payload of type\n`JobLaunchRequest`. This class is a wrapper around the `Job`\nto be launched and around the `JobParameters` that are\nnecessary to launch the Batch job.\n\nThe following image shows the typical Spring Integration\nmessage flow that is needed to start a Batch job. The\nlink:$$https://www.enterpriseintegrationpatterns.com/toc.html$$[EIP (Enterprise Integration Patterns) website]\nprovides a full overview of messaging icons and their descriptions.\n\n.Launch Batch Job\nimage::launch-batch-job.png[Launch Batch Job, scaledwidth=\"60%\"]\n\n[[transforming-a-file-into-a-joblaunchrequest]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc", "title": "launching-jobs-through-messages", "heading": "launching-jobs-through-messages", "heading_level": 1, "file_order": 26, "section_index": 0, "content_hash": "7c2f170fb5d5411df84d7b4024f5c7176ed6d7adae94b8ff4ab1054f091b48ed", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc"}}
{"id": "sha256:32c92ce943f4d66288b1a4ef6be73560604411b265b2707838c2f4e8c657e58e", "content": "The following example transforms a file into a `JobLaunchRequest`:\n\n[source, java]\n----\npackage io.spring.sbi;\n\nimport org.springframework.batch.core.Job;\nimport org.springframework.batch.core.JobParametersBuilder;\nimport org.springframework.batch.integration.launch.JobLaunchRequest;\nimport org.springframework.integration.annotation.Transformer;\nimport org.springframework.messaging.Message;\n\nimport java.io.File;\n\npublic class FileMessageToJobRequest {\n private Job job;\n private String fileParameterName;\n\n public void setFileParameterName(String fileParameterName) {\n this.fileParameterName = fileParameterName;\n }\n\n public void setJob(Job job) {\n this.job = job;\n }\n\n @Transformer\n public JobLaunchRequest toRequest(Message<File> message) {\n JobParametersBuilder jobParametersBuilder =\n new JobParametersBuilder();\n\n jobParametersBuilder.addString(fileParameterName,\n message.getPayload().getAbsolutePath());\n\n return new JobLaunchRequest(job, jobParametersBuilder.toJobParameters());\n }\n}\n----\n\n[[the-jobexecution-response]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc", "title": "launching-jobs-through-messages", "heading": "Transforming a File into a JobLaunchRequest", "heading_level": 2, "file_order": 26, "section_index": 1, "content_hash": "32c92ce943f4d66288b1a4ef6be73560604411b265b2707838c2f4e8c657e58e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc"}}
{"id": "sha256:2bdbf79526fa510570783d05179e9aa955313b10bd0a4b4a3b3cce6474b0cfd9", "content": "When a batch job is being executed, a\n`JobExecution` instance is returned. You can use this\ninstance to determine the status of an execution. If\na `JobExecution` is able to be created\nsuccessfully, it is always returned, regardless of whether\nor not the actual execution is successful.\n\nThe exact behavior on how the `JobExecution`\ninstance is returned depends on the provided\n`TaskExecutor`. If a\n`synchronous` (single-threaded)\n`TaskExecutor` implementation is used, the\n`JobExecution` response is returned only\n`after` the job completes. When using an\n`asynchronous`\n`TaskExecutor`, the\n`JobExecution` instance is returned\nimmediately. You can then take the `id` of\n`JobExecution` instance\n(with `JobExecution.getJobInstanceId()`) and query the\n`JobRepository` for the job's updated status\nusing the `JobExplorer`. For more\ninformation, see\nxref:job/advanced-meta-data.adoc#queryingRepository[Querying the Repository].\n\n[[spring-batch-integration-configuration]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc", "title": "launching-jobs-through-messages", "heading": "The JobExecution Response", "heading_level": 2, "file_order": 26, "section_index": 2, "content_hash": "2bdbf79526fa510570783d05179e9aa955313b10bd0a4b4a3b3cce6474b0cfd9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc"}}
{"id": "sha256:f105f4be34c930738230f765155fbd92d4dd255065d2c45b39d9f7fc76a60eb2", "content": "Consider a case where someone needs to create a file `inbound-channel-adapter` to listen\nfor CSV files in the provided directory, hand them off to a transformer\n(`FileMessageToJobRequest`), launch the job through the job launching gateway, and\nlog the output of the `JobExecution` with the `logging-channel-adapter`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how that common case can be configured in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FileMessageToJobRequest fileMessageToJobRequest() {\n FileMessageToJobRequest fileMessageToJobRequest = new FileMessageToJobRequest();\n fileMessageToJobRequest.setFileParameterName(\"input.file.name\");\n fileMessageToJobRequest.setJob(personJob());\n return fileMessageToJobRequest;\n}\n\n@Bean\npublic JobLaunchingGateway jobLaunchingGateway() {\n TaskExecutorJobLauncher jobLauncher = new TaskExecutorJobLauncher();\n jobLauncher.setJobRepository(jobRepository);\n jobLauncher.setTaskExecutor(new SyncTaskExecutor());\n JobLaunchingGateway jobLaunchingGateway = new JobLaunchingGateway(jobLauncher);\n\n return jobLaunchingGateway;\n}\n\n@Bean\npublic IntegrationFlow integrationFlow(JobLaunchingGateway jobLaunchingGateway) {\n return IntegrationFlow.from(Files.inboundAdapter(new File(\"/tmp/myfiles\")).\n filter(new SimplePatternFileListFilter(\"*.csv\")),\n c -> c.poller(Pollers.fixedRate(1000).maxMessagesPerPoll(1))).\n transform(fileMessageToJobRequest()).\n handle(jobLaunchingGateway).\n log(LoggingHandler.Level.WARN, \"headers.id + ': ' + payload\").\n get();\n}\n----\n\nXML::\n+\nThe following example shows how that common case can be configured in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<int:channel id=\"inboundFileChannel\"/>\n<int:channel id=\"outboundJobRequestChannel\"/>\n<int:channel id=\"jobLaunchReplyChannel\"/>\n\n<int-file:inbound-channel-adapter id=\"filePoller\"\n channel=\"inboundFileChannel\"\n directory=\"file:/tmp/myfiles/\"\n filename-pattern=\"*.csv\">\n <int:poller fixed-rate=\"1000\"/>\n</int-file:inbound-channel-adapter>\n\n<int:transformer input-channel=\"inboundFileChannel\"\n output-channel=\"outboundJobRequestChannel\">\n <bean class=\"io.spring.sbi.FileMessageToJobRequest\">\n <property name=\"job\" ref=\"personJob\"/>\n <property name=\"fileParameterName\" value=\"input.file.name\"/>\n </bean>\n</int:transformer>\n\n<batch-int:job-launching-gateway request-channel=\"outboundJobRequestChannel\"\n reply-channel=\"jobLaunchReplyChannel\"/>\n\n<int:logging-channel-adapter channel=\"jobLaunchReplyChannel\"/>\n----\n====\n\n[[example-itemreader-configuration]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc", "title": "launching-jobs-through-messages", "heading": "Spring Batch Integration Configuration", "heading_level": 2, "file_order": 26, "section_index": 3, "content_hash": "f105f4be34c930738230f765155fbd92d4dd255065d2c45b39d9f7fc76a60eb2", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc"}}
{"id": "sha256:a50059aa24515fd8da3b893fc44d5c30b469aac72894bbe23fa6559d1b5eef67", "content": "Now that we are polling for files and launching jobs, we need to configure our Spring\nBatch `ItemReader` (for example) to use the files found at the location defined by the job\nparameter called \"input.file.name\", as the following bean configuration shows:\n\n[tabs]\n====\nJava::\n+\nThe following Java example shows the necessary bean configuration:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\n@StepScope\npublic ItemReader sampleReader(@Value(\"#{jobParameters[input.file.name]}\") String resource) {\n...\n FlatFileItemReader flatFileItemReader = new FlatFileItemReader();\n flatFileItemReader.setResource(new FileSystemResource(resource));\n...\n return flatFileItemReader;\n}\n----\n\nXML::\n+\nThe following XML example shows the necessary bean configuration:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"itemReader\" class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemReader\"\n scope=\"step\">\n <property name=\"resource\" value=\"file://#{jobParameters['input.file.name']}\"/>\n ...\n</bean>\n----\n\n====\n\nThe main points of interest in the preceding example are injecting the value of\n`#{jobParameters['input.file.name']}`\nas the Resource property value and setting the `ItemReader` bean\nto have step scope. Setting the bean to have step scope takes advantage of\nthe late binding support, which allows access to the\n`jobParameters` variable.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc", "title": "launching-jobs-through-messages", "heading": "Example ItemReader Configuration", "heading_level": 2, "file_order": 26, "section_index": 4, "content_hash": "a50059aa24515fd8da3b893fc44d5c30b469aac72894bbe23fa6559d1b5eef67", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/launching-jobs-through-messages.adoc"}}
{"id": "sha256:06ee1034cba4475dab126c744cd582ed49eb2dce9e703f71b63cbdbe7562f584", "content": "[[namespace-support]]\n\nDedicated XML namespace support was added to Spring Batch Integration in version 1.3,\nwith the aim to provide an easier configuration\nexperience. To use the namespace, add the following\nnamespace declarations to your Spring XML Application Context\nfile:\n\n[source, xml]\n----\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n xmlns:batch-int=\"http://www.springframework.org/schema/batch-integration\"\n xsi:schemaLocation=\"\n http://www.springframework.org/schema/batch-integration\n https://www.springframework.org/schema/batch-integration/spring-batch-integration.xsd\">\n\n ...\n\n</beans>\n----\n\nThe following example shows a fully configured Spring XML application context file for Spring\nBatch Integration:\n\n[source, xml]\n----\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n xmlns:int=\"http://www.springframework.org/schema/integration\"\n xmlns:batch=\"http://www.springframework.org/schema/batch\"\n xmlns:batch-int=\"http://www.springframework.org/schema/batch-integration\"\n xsi:schemaLocation=\"\n http://www.springframework.org/schema/batch-integration\n https://www.springframework.org/schema/batch-integration/spring-batch-integration.xsd\n http://www.springframework.org/schema/batch\n https://www.springframework.org/schema/batch/spring-batch.xsd\n http://www.springframework.org/schema/beans\n https://www.springframework.org/schema/beans/spring-beans.xsd\n http://www.springframework.org/schema/integration\n https://www.springframework.org/schema/integration/spring-integration.xsd\">\n\n ...\n\n</beans>\n----\n\nAppending version numbers to the referenced XSD file is also\nallowed. However, because a version-less declaration always uses the\nlatest schema, we generally do not recommend appending the version\nnumber to the XSD name. Adding a version number\ncould possibly create issues when updating the Spring Batch\nIntegration dependencies, as they may require more recent versions\nof the XML schema.\n\nWARNING: The `batch-integration` XML namespace is deprecated as of Spring\nBatch 6.0 and will be removed in version 7.0.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/namespace-support.adoc", "title": "namespace-support", "heading": "namespace-support", "heading_level": 1, "file_order": 27, "section_index": 0, "content_hash": "06ee1034cba4475dab126c744cd582ed49eb2dce9e703f71b63cbdbe7562f584", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/namespace-support.adoc"}}
{"id": "sha256:d6754177444de027d24daa8b7cc93de0c8ca9403c2dd322e7e964c3c3341600b", "content": "[[providing-feedback-with-informational-messages]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/providing-feedback-with-informational-messages.adoc", "title": "providing-feedback-with-informational-messages", "heading": "providing-feedback-with-informational-messages", "heading_level": 1, "file_order": 28, "section_index": 0, "content_hash": "d6754177444de027d24daa8b7cc93de0c8ca9403c2dd322e7e964c3c3341600b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/providing-feedback-with-informational-messages.adoc"}}
{"id": "sha256:67c5642fc81d8303fce2bac5572a90e3c3c795fc6c0d0fb91f6b257f223139f1", "content": "As Spring Batch jobs can run for long times, providing progress\ninformation is often critical. For example, stakeholders may want\nto be notified if some or all parts of a batch job have failed.\nSpring Batch provides support for this information being gathered\nthrough:\n\n* Active polling\n* Event-driven listeners\n\nWhen starting a Spring Batch job asynchronously (for example, by using the Job Launching\nGateway), a `JobExecution` instance is returned. Thus, you can use `JobExecution.getJobInstanceId()`\nto continuously poll for status updates by retrieving updated instances of the\n`JobExecution` from the `JobRepository` by using the `JobExplorer`. However, this is\nconsidered sub-optimal, and an event-driven approach is preferred.\n\nTherefore, Spring Batch provides listeners, including the three most commonly used\nlisteners:\n\n* `StepListener`\n* `ChunkListener`\n* `JobExecutionListener`\n\nIn the example shown in the following image, a Spring Batch job has been configured with a\n`StepExecutionListener`. Thus, Spring Integration receives and processes any step before\nor after events. For example, you can inspect the received `StepExecution` by using a\n`Router`. Based on the results of that inspection, various things can occur (such as\nrouting a message to a mail outbound channel adapter), so that an email notification can\nbe sent out based on some condition.\n\n.Handling Informational Messages\nimage::handling-informational-messages.png[Handling Informational Messages, scaledwidth=\"60%\"]\n\nThe following two-part example shows how a listener is configured to send a\nmessage to a `Gateway` for a `StepExecution` events and log its output to a\n`logging-channel-adapter`.\n\nFirst, create the notification integration beans.\n\n[tabs]\n====\nJava::\n+\nThe following example shows the how to create the notification integration beans in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\n@ServiceActivator(inputChannel = \"stepExecutionsChannel\")\npublic LoggingHandler loggingHandler() {\n LoggingHandler adapter = new LoggingHandler(LoggingHandler.Level.WARN);\n adapter.setLoggerName(\"TEST_LOGGER\");\n adapter.setLogExpressionString(\"headers.id + ': ' + payload\");\n return adapter;\n}\n\n@MessagingGateway(name = \"notificationExecutionsListener\", defaultRequestChannel = \"stepExecutionsChannel\")\npublic interface NotificationExecutionListener extends StepExecutionListener {}\n----\n+\nNOTE: You need to add the `@IntegrationComponentScan` annotation to your configuration.\n\nXML::\n+\nThe following example shows the how to create the notification integration beans in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<int:channel id=\"stepExecutionsChannel\"/>\n\n<int:gateway id=\"notificationExecutionsListener\"\n service-interface=\"org.springframework.batch.core.listener.StepExecutionListener\"\n default-request-channel=\"stepExecutionsChannel\"/>\n\n<int:logging-channel-adapter channel=\"stepExecutionsChannel\"/>\n----\n\n====\n\n[[message-gateway-entry-list]]\n\nSecond, modify your job to add a step-level listener.\n\n[tabs]\n====\nJava::\n+\nThe following example shows the how to add a step-level listener in Java:\n+\n.Java Configuration\n[source, java]\n----\npublic Job importPaymentsJob(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n return new JobBuilder(\"importPayments\", jobRepository)\n .start(new StepBuilder(\"step1\", jobRepository)\n .chunk(200, transactionManager)\n .listener(notificationExecutionsListener())\n // ...\n .build();\n )\n .build();\n}\n----\n\nXML::\n+\nThe following example shows the how to add a step-level listener in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"importPayments\">\n <step id=\"step1\">\n <tasklet ../>\n <chunk ../>\n <listeners>\n <listener ref=\"notificationExecutionsListener\"/>\n </listeners>\n </tasklet>\n ...\n </step>\n</job>\n----\n\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration/providing-feedback-with-informational-messages.adoc", "title": "providing-feedback-with-informational-messages", "heading": "Providing Feedback with Informational Messages", "heading_level": 2, "file_order": 28, "section_index": 1, "content_hash": "67c5642fc81d8303fce2bac5572a90e3c3c795fc6c0d0fb91f6b257f223139f1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration/providing-feedback-with-informational-messages.adoc"}}
{"id": "sha256:930e71fc1606d3b114531782b3d23d095458b57b15b2910ab3c1231e106733ec", "content": "[[jfr]]\n\nAs of version 6, Spring Batch provides support for Java Flight Recorder (JFR) to help you monitor and troubleshoot batch jobs. JFR is a low-overhead, event-based profiling tool built into the Java Virtual Machine (JVM) that allows developers to collect detailed information about the performance and behavior of their applications.\n\nJFR can be enabled by adding the following JVM options when starting your Spring Batch application:\n\n[source, bash]\n----\njava -XX:StartFlightRecording:filename=my-batch-job.jfr,dumponexit=true -jar my-batch-job.jar\n----\n\nOnce JFR is enabled, Spring Batch will automatically create JFR events for key batch processing activities, such as job and step executions, item reads and writes, as well as transaction boundaries. These events can be viewed and analyzed using tools such as Java Mission Control (JMC) or other JFR-compatible tools.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-observability/jfr.adoc", "title": "jfr", "heading": "jfr", "heading_level": 1, "file_order": 29, "section_index": 0, "content_hash": "930e71fc1606d3b114531782b3d23d095458b57b15b2910ab3c1231e106733ec", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-observability/jfr.adoc"}}
{"id": "sha256:f3d95085eaab428071fb3ef61e8d70e903014606e017ba3843605f719b1bca06", "content": "[[micrometer]]\n\n[[monitoring-and-metrics]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc", "title": "micrometer", "heading": "micrometer", "heading_level": 1, "file_order": 30, "section_index": 0, "content_hash": "f3d95085eaab428071fb3ef61e8d70e903014606e017ba3843605f719b1bca06", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc"}}
{"id": "sha256:03c682350706e2dc97e55896d2315dd5ff196ed62f7663739aff8d5c8d613824", "content": "Since version 4.2, Spring Batch provides support for batch monitoring and metrics\nbased on link:$$https://micrometer.io/$$[Micrometer]. This section describes\nwhich metrics are provided out-of-the-box and how to contribute custom metrics.\n\n[[built-in-metrics]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc", "title": "micrometer", "heading": "Monitoring and metrics", "heading_level": 2, "file_order": 30, "section_index": 1, "content_hash": "03c682350706e2dc97e55896d2315dd5ff196ed62f7663739aff8d5c8d613824", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc"}}
{"id": "sha256:46b0ad68ed8e7244be0366f4f58fd2010b3d5ac40ff53071a8fb49d4701bda13", "content": "Metrics collection is disabled by default. To enable it, you need to define a Micrometer\n`ObservationRegistry` bean in your application context. Typically, you would need to define\nwhich ObservationHandler to use. The following example shows how to register a `DefaultMeterObservationHandler`\nthat will store metrics in a `MeterRegistry` (for example, a Prometheus registry):\n\n[source, java]\n----\n@Bean\npublic ObservationRegistry observationRegistry(MeterRegistry meterRegistry) {\n ObservationRegistry observationRegistry = ObservationRegistry.create();\n observationRegistry.observationConfig()\n .observationHandler(new DefaultMeterObservationHandler(meterRegistry));\n return observationRegistry;\n}\n----\n\nSpring Batch specific metrics are registered under the `spring.batch` prefix. The following\ntable explains all the metrics in details:\n\n|===============\n|__Metric Name__|__Type__|__Description__|__Tags__\n|`spring.batch.job`|`TIMER`|Duration of job execution|`name`, `status`\n|`spring.batch.job.active`|`LONG_TASK_TIMER`|Currently active job|`name`\n|`spring.batch.step`|`TIMER`|Duration of step execution|`name`, `job.name`, `status`\n|`spring.batch.step.active`|`LONG_TASK_TIMER`|Currently active step|`name`\n|`spring.batch.item.read`|`TIMER`|Duration of item reading|`job.name`, `step.name`, `status`\n|`spring.batch.item.process`|`TIMER`|Duration of item processing|`job.name`, `step.name`, `status`\n|`spring.batch.chunk.write`|`TIMER`|Duration of chunk writing|`job.name`, `step.name`, `status`\n|`spring.batch.job.launch.count`|`COUNTER`|Job launch count| N/A\n|===============\n\nNOTE: The `status` tag for jobs and steps is equal to the exit status. For item reading, processing\nand writing, this `status` tag can be either `SUCCESS` or `FAILURE`.\n\n[[custom-metrics]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc", "title": "micrometer", "heading": "Built-in metrics", "heading_level": 2, "file_order": 30, "section_index": 2, "content_hash": "46b0ad68ed8e7244be0366f4f58fd2010b3d5ac40ff53071a8fb49d4701bda13", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc"}}
{"id": "sha256:78c762237574dde29cbc4a57fe1e77a961e9992b122237cb6bc70d5d6e3ec9bc", "content": "If you want to use your own metrics in your custom components, we recommend using\nMicrometer APIs directly. The following is an example of how to time a `Tasklet`:\n\n[source, java]\n----\nimport io.micrometer.observation.Observation;\nimport io.micrometer.observation.ObservationRegistry;\n\nimport org.springframework.batch.core.StepContribution;\nimport org.springframework.batch.core.scope.context.ChunkContext;\nimport org.springframework.batch.core.step.tasklet.Tasklet;\nimport org.springframework.batch.repeat.RepeatStatus;\n\npublic class MyTimedTasklet implements Tasklet {\n\n private ObservationRegistry observationRegistry;\n\n public MyTimedTasklet(ObservationRegistry observationRegistry) {\n this.observationRegistry = observationRegistry;\n }\n\n\t@Override\n\tpublic RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) {\n Observation observation = Observation.start(\"my.tasklet.step\", this.observationRegistry);\n try (Observation.Scope scope = observation.openScope()) {\n // do some work\n return RepeatStatus.FINISHED;\n } catch (Exception e) {\n // handle exception\n observation.error(exception);\n } finally {\n observation.stop();\n }\n\t}\n}\n----\n\n[[tracing]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc", "title": "micrometer", "heading": "Custom metrics", "heading_level": 2, "file_order": 30, "section_index": 3, "content_hash": "78c762237574dde29cbc4a57fe1e77a961e9992b122237cb6bc70d5d6e3ec9bc", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc"}}
{"id": "sha256:e84f6fe8b9b25fd1486a07a2fe03914cef5bcf1a51d6dbbddea7d5d25291fdb4", "content": "As of version 5, Spring Batch provides tracing through Micrometer's `Observation` API. By default, tracing is disabled.\nTo enable it, you need to define an `ObservationRegistry` bean configured with an `ObservationHandler` that supports tracing,\nsuch as `TracingAwareMeterObservationHandler`:\n\n[source, java]\n----\n@Bean\npublic ObservationRegistry observationRegistry(MeterRegistry meterRegistry, Tracer tracer) {\n DefaultMeterObservationHandler observationHandler = new DefaultMeterObservationHandler(meterRegistry);\n ObservationRegistry observationRegistry = ObservationRegistry.create();\n observationRegistry.observationConfig()\n .observationHandler(new TracingAwareMeterObservationHandler<>(observationHandler, tracer));\n return observationRegistry;\n}\n----\n\nWith that in place, Spring Batch will create a trace for each job execution and a span for each step execution.\n\nIf you do not use `EnableBatchProcessing` or `DefaultBatchConfiguration`, you need to register a\n`BatchObservabilityBeanPostProcessor` in your application context, which will automatically set Micrometer's observation\nregistry in observable batch artefacts.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc", "title": "micrometer", "heading": "Tracing", "heading_level": 2, "file_order": 30, "section_index": 4, "content_hash": "e84f6fe8b9b25fd1486a07a2fe03914cef5bcf1a51d6dbbddea7d5d25291fdb4", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-observability/micrometer.adoc"}}
{"id": "sha256:3d8d0b2083cd07f0d52fb4011e49aabb0b69ae0b44e304a5496d72352981ca0c", "content": "[[commitInterval]]\n\nAs mentioned previously, a step reads in and writes out items, periodically committing\nby using the supplied `PlatformTransactionManager`. With a `commit-interval` of 1, it\ncommits after writing each individual item. This is less than ideal in many situations,\nsince beginning and committing a transaction is expensive. Ideally, it is preferable to\nprocess as many items as possible in each transaction, which is completely dependent upon\nthe type of data being processed and the resources with which the step is interacting.\nFor this reason, you can configure the number of items that are processed within a commit.\n\n[tabs]\n====\nJava::\n+\nThe following example shows a `step` whose `tasklet` has a `commit-interval`\nvalue of 10 as it would be defined in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job sampleJob(JobRepository jobRepository, Step step1) {\n return new JobBuilder(\"sampleJob\", jobRepository)\n .start(step1)\n .build();\n}\n\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(itemReader())\n .writer(itemWriter())\n .build();\n}\n----\n\nXML::\n+\nThe following example shows a `step` whose `tasklet` has a `commit-interval`\nvalue of 10 as it would be defined in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"sampleJob\">\n <step id=\"step1\">\n <tasklet>\n <chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"10\"/>\n </tasklet>\n </step>\n</job>\n----\n\n====\n\nIn the preceding example, 10 items are processed within each transaction. At the\nbeginning of processing, a transaction is begun. Also, each time `read` is called on the\n`ItemReader`, a counter is incremented. When it reaches 10, the list of aggregated items\nis passed to the `ItemWriter`, and the transaction is committed.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/commit-interval.adoc", "title": "commit-interval", "heading": "commit-interval", "heading_level": 1, "file_order": 31, "section_index": 0, "content_hash": "3d8d0b2083cd07f0d52fb4011e49aabb0b69ae0b44e304a5496d72352981ca0c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/commit-interval.adoc"}}
{"id": "sha256:153af818298c40689862b864c5855aa9d9cec2eefeddb805c9644f70517d76fb", "content": "[[configuringSkip]]\n\nThere are many scenarios where errors encountered while processing should not result in\n`Step` failure but should be skipped instead. This is usually a decision that must be\nmade by someone who understands the data itself and what meaning it has. Financial data,\nfor example, may not be skippable because it results in money being transferred, which\nneeds to be completely accurate. Loading a list of vendors, on the other hand, might\nallow for skips. If a vendor is not loaded because it was formatted incorrectly or was\nmissing necessary information, there probably are not issues. Usually, these bad\nrecords are logged as well, which is covered later when discussing listeners.\n\n[tabs]\n====\nJava::\n+\nThe following Java example shows an example of using a skip limit:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n int skipLimit = 10;\n var skippableExceptions = Set.of(FlatFileParseException.class);\n SkipPolicy skipPolicy = new LimitCheckingExceptionHierarchySkipPolicy(skippableExceptions, skipLimit);\n\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(flatFileItemReader())\n .writer(itemWriter())\n .faultTolerant()\n .skipPolicy(skipPolicy)\n .build();\n}\n----\n+\nNote: The `skipLimit` can be explicitly set using the `skipLimit()` method.\n\nXML::\n+\nThe following XML example shows an example of using a skip limit:\n+\n.XML Configuration\n[source,xml]\n----\n<step id=\"step1\">\n <tasklet>\n <chunk reader=\"flatFileItemReader\" writer=\"itemWriter\"\n commit-interval=\"10\" skip-limit=\"10\">\n <skippable-exception-classes>\n <include class=\"org.springframework.batch.infrastructure.item.file.FlatFileParseException\"/>\n </skippable-exception-classes>\n </chunk>\n </tasklet>\n</step>\n----\n\n====\n\nIn the preceding example, a `FlatFileItemReader` is used. If, at any point, a\n`FlatFileParseException` is thrown, the item is skipped and counted against the total\nskip limit of 10. Exceptions (and their subclasses) that are declared might be thrown\nduring any phase of the chunk processing (read, process, or write). Separate counts\nare made of skips on read, process, and write inside\nthe step execution, but the limit applies across all skips. Once the skip limit is\nreached, the next exception found causes the step to fail. In other words, the eleventh\nskip triggers the exception, not the tenth.\n\n[NOTE]\n====\nThe skip limit applies to all skips (read, process and write).\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/configuring-skip.adoc", "title": "configuring-skip", "heading": "configuring-skip", "heading_level": 1, "file_order": 32, "section_index": 0, "content_hash": "153af818298c40689862b864c5855aa9d9cec2eefeddb805c9644f70517d76fb", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/configuring-skip.adoc"}}
{"id": "sha256:c1bd64b4025462324c8b79dbada6ec6621be36dd4a1d2426fa64bd5b1e3f21d8", "content": "[[configuringAStep]]\n\nDespite the relatively short list of required dependencies for a `Step`, it is an\nextremely complex class that can potentially contain many collaborators.\n\n[tabs]\n====\nJava::\n+\nWhen using Java configuration, you can use the Spring Batch builders, as the\nfollowing example shows:\n+\n.Java Configuration\n[source, java]\n----\n/**\n * Note the JobRepository is typically autowired in and not needed to be explicitly\n * configured\n */\n@Bean\npublic Job sampleJob(JobRepository jobRepository, Step sampleStep) {\n return new JobBuilder(\"sampleJob\", jobRepository)\n .start(sampleStep)\n .build();\n}\n\n/**\n * Note the TransactionManager is typically autowired in and not needed to be explicitly\n * configured\n */\n@Bean\npublic Step sampleStep(JobRepository jobRepository, // <1>\n PlatformTransactionManager transactionManager) { // <2>\n\treturn new StepBuilder(jobRepository) // <3>\n .<String, String>chunk(10).transactionManager(transactionManager) // <4>\n .reader(itemReader())\n .writer(itemWriter())\n .build();\n}\n----\n<1> `repository`: The Java-specific name of the `JobRepository` that periodically stores\nthe `StepExecution` and `ExecutionContext` during processing (just before committing).\n<2> `transactionManager`: Spring's `PlatformTransactionManager` that begins and commits\ntransactions during processing.\n<3> Step name: when the step is declared as a bean, the name can be omitted and will be derived\nfrom the method name. However, if the step is *not* defined as a bean, the name must be explicitly\nprovided to the `StepBuilder` constructor like `new StepBuilder(\"myStep\", jobRepository)`.\n<4> `chunk`: The Java-specific name of the dependency that indicates that this is an\nitem-based step and the number of items to be processed before the transaction is\ncommitted.\n+\nNOTE: Note that `repository` defaults to `jobRepository` (provided through `@EnableBatchProcessing`)\nand `transactionManager` defaults to `transactionManager` (provided from the application context). The\ntransaction manager is optional and defaults to a `ResourcelessTransactionManager`.\nAlso, the `ItemProcessor` is optional, since the item could be\ndirectly passed from the reader to the writer.\n\nXML::\n+\nTo ease configuration, you can use the Spring Batch XML namespace, as\nthe following example shows:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"sampleJob\" job-repository=\"jobRepository\"> <!--2-->\n <step id=\"step1\">\n <tasklet transaction-manager=\"transactionManager\"> <!--1-->\n <chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"10\"/> <!--3-->\n </tasklet>\n </step>\n</job>\n----\n<1> `transaction-manager`: Spring's `PlatformTransactionManager` that begins and commits\ntransactions during processing.\n<2> `job-repository`: The XML-specific name of the `JobRepository` that periodically stores\nthe `StepExecution` and `ExecutionContext` during processing (just before committing). For\nan in-line `<step/>` (one defined within a `<job/>`), it is an attribute on the `<job/>`\nelement. For a standalone `<step/>`, it is defined as an attribute of the `<tasklet/>`.\n<3> `commit-interval`: The XML-specific name of the number of items to be processed\nbefore the transaction is committed.\n+\nNOTE: Note that `job-repository` defaults to `jobRepository` and\n`transaction-manager` defaults to `transactionManager`. Also, the `ItemProcessor` is\noptional, since the item could be directly passed from the reader to the writer.\n====\n\nThe preceding configuration includes the only required dependencies to create an item-oriented\nstep:\n\n* `reader`: The `ItemReader` that provides items for processing.\n* `writer`: The `ItemWriter` that processes the items provided by the `ItemReader`.\n\nNOTE: The transaction manager used in the step could be different from the one used in the job\nrepository. The caveat though is that the job repository and the processing database\nwon't be in the same transaction, so if a failure occurs after processing but before the job\nrepository is updated, the step could be re-executed and lead to duplicate processing. This could\nbe mitigated through idempotent processing or external transaction management (e.g., JTA).", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/configuring.adoc", "title": "configuring", "heading": "configuring", "heading_level": 1, "file_order": 33, "section_index": 0, "content_hash": "c1bd64b4025462324c8b79dbada6ec6621be36dd4a1d2426fa64bd5b1e3f21d8", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/configuring.adoc"}}
{"id": "sha256:0b52a3d3bfd883eee75c0260ff1689d9cd118408fc3db7c1298138f4f2c86b47", "content": "[[inheriting-from-a-parent-step]]\n\n[role=\"xmlContent\"]\nIf a group of `Steps` share similar configurations, then it may be helpful to define a\n\"`parent`\" `Step` from which the concrete `Steps` may inherit properties. Similar to class\ninheritance in Java, the \"`child`\" `Step` combines its elements and attributes with the\nparent's. The child also overrides any of the parent's `Steps`.\n\n[role=\"xmlContent\"]\nIn the following example, the `Step`, `concreteStep1`, inherits from `parentStep`. It is\ninstantiated with `itemReader`, `itemProcessor`, `itemWriter`, `startLimit=5`, and\n`allowStartIfComplete=true`. Additionally, the `commitInterval` is `5`, since it is\noverridden by the `concreteStep1` `Step`, as the following example shows:\n\n[source, xml, role=\"xmlContent\"]\n----\n<step id=\"parentStep\">\n <tasklet allow-start-if-complete=\"true\">\n <chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"10\"/>\n </tasklet>\n</step>\n\n<step id=\"concreteStep1\" parent=\"parentStep\">\n <tasklet start-limit=\"5\">\n <chunk processor=\"itemProcessor\" commit-interval=\"5\"/>\n </tasklet>\n</step>\n----\n\n[role=\"xmlContent\"]\nThe `id` attribute is still required on the step within the job element. This is for two\nreasons:\n\n* The `id` is used as the step name when persisting the `StepExecution`. If the same\nstandalone step is referenced in more than one step in the job, an error occurs.\n\n[role=\"xmlContent\"]\n* When creating job flows, as described xref:step/controlling-flow.adoc[later in this chapter], the `next` attribute\nshould refer to the step in the flow, not the standalone step.\n\n[[abstractStep]]\n[role=\"xmlContent\"]\n[[abstract-step]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/inheriting-from-parent.adoc", "title": "inheriting-from-parent", "heading": "inheriting-from-parent", "heading_level": 1, "file_order": 34, "section_index": 0, "content_hash": "0b52a3d3bfd883eee75c0260ff1689d9cd118408fc3db7c1298138f4f2c86b47", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/inheriting-from-parent.adoc"}}
{"id": "sha256:9e97e9d9f87fd85c4372991c722253d11b8b907d5cd2c650a734b861f88919a8", "content": "[role=\"xmlContent\"]\nSometimes, it may be necessary to define a parent `Step` that is not a complete `Step`\nconfiguration. If, for instance, the `reader`, `writer`, and `tasklet` attributes are\nleft off of a `Step` configuration, then initialization fails. If a parent must be\ndefined without one or more of these properties, the `abstract` attribute should be used. An\n`abstract` `Step` is only extended, never instantiated.\n\n[role=\"xmlContent\"]\nIn the following example, the `Step` (`abstractParentStep`) would not be instantiated if it\nwere not declared to be abstract. The `Step`, (`concreteStep2`) has `itemReader`,\n`itemWriter`, and `commit-interval=10`.\n\n[source, xml, role=\"xmlContent\"]\n----\n<step id=\"abstractParentStep\" abstract=\"true\">\n <tasklet>\n <chunk commit-interval=\"10\"/>\n </tasklet>\n</step>\n\n<step id=\"concreteStep2\" parent=\"abstractParentStep\">\n <tasklet>\n <chunk reader=\"itemReader\" writer=\"itemWriter\"/>\n </tasklet>\n</step>\n----\n\n[[mergingListsOnStep]]\n[role=\"xmlContent\"]\n[[merging-lists]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/inheriting-from-parent.adoc", "title": "inheriting-from-parent", "heading": "Abstract `Step`", "heading_level": 2, "file_order": 34, "section_index": 1, "content_hash": "9e97e9d9f87fd85c4372991c722253d11b8b907d5cd2c650a734b861f88919a8", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/inheriting-from-parent.adoc"}}
{"id": "sha256:e32066397c65a7ad69eabc5fae9534d892d197049c9e00021e309f636ac7fe9c", "content": "[role=\"xmlContent\"]\nSome of the configurable elements on `Steps` are lists, such as the `<listeners/>` element.\nIf both the parent and child `Steps` declare a `<listeners/>` element, the\nchild's list overrides the parent's. To allow a child to add additional\nlisteners to the list defined by the parent, every list element has a `merge` attribute.\nIf the element specifies that `merge=\"true\"`, then the child's list is combined with the\nparent's instead of overriding it.\n\n[role=\"xmlContent\"]\nIn the following example, the `Step` \"concreteStep3\", is created with two listeners:\n`listenerOne` and `listenerTwo`:\n\n[source, xml, role=\"xmlContent\"]\n----\n<step id=\"listenersParentStep\" abstract=\"true\">\n <listeners>\n <listener ref=\"listenerOne\"/>\n </listeners>\n</step>\n\n<step id=\"concreteStep3\" parent=\"listenersParentStep\">\n <tasklet>\n <chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"5\"/>\n </tasklet>\n <listeners merge=\"true\">\n <listener ref=\"listenerTwo\"/>\n </listeners>\n</step>\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/inheriting-from-parent.adoc", "title": "inheriting-from-parent", "heading": "Merging Lists", "heading_level": 2, "file_order": 34, "section_index": 2, "content_hash": "e32066397c65a7ad69eabc5fae9534d892d197049c9e00021e309f636ac7fe9c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/inheriting-from-parent.adoc"}}
{"id": "sha256:6f5bf75846a887a2da54ffa7a41c4e2fc9649b6025ae5123a0ea058407f7b416", "content": "[[interceptingStepExecution]]\n\nJust as with the `Job`, there are many events during the execution of a `Step` where a\nuser may need to perform some functionality. For example, to write out to a flat\nfile that requires a footer, the `ItemWriter` needs to be notified when the `Step` has\nbeen completed so that the footer can be written. This can be accomplished with one of many\n`Step` scoped listeners.\n\nYou can apply any class that implements one of the extensions of `StepListener` (but not that interface\nitself, since it is empty) to a step through the `listeners` element.\nThe `listeners` element is valid inside a step, tasklet, or chunk declaration. We\nrecommend that you declare the listeners at the level at which its function applies\nor, if it is multi-featured (such as `StepExecutionListener` and `ItemReadListener`),\ndeclare it at the most granular level where it applies.\n\n[tabs]\n====\nJava::\n+\nThe following example shows a listener applied at the chunk level in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(reader())\n .writer(writer())\n .listener(chunkListener())\n .build();\n}\n----\n\nXML::\n+\nThe following example shows a listener applied at the chunk level in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<step id=\"step1\">\n <tasklet>\n <chunk reader=\"reader\" writer=\"writer\" commit-interval=\"10\"/>\n <listeners>\n <listener ref=\"chunkListener\"/>\n </listeners>\n </tasklet>\n</step>\n----\n\n====\n\nAn `ItemReader`, `ItemWriter`, or `ItemProcessor` that itself implements one of the\n`StepListener` interfaces is registered automatically with the `Step` if using the\nnamespace `<step>` element or one of the `*StepFactoryBean` factories. This only\napplies to components directly injected into the `Step`. If the listener is nested inside\nanother component, you need to explicitly register it (as described previously under\nxref:step/chunk-oriented-processing/registering-item-streams.adoc[Registering `ItemStream` with a `Step`]).\n\nIn addition to the `StepListener` interfaces, annotations are provided to address the\nsame concerns. Plain old Java objects can have methods with these annotations that are\nthen converted into the corresponding `StepListener` type. It is also common to annotate\ncustom implementations of chunk components, such as `ItemReader` or `ItemWriter` or\n`Tasklet`. The annotations are analyzed by the XML parser for the `<listener/>` elements\nas well as registered with the `listener` methods in the builders, so all you need to do\nis use the XML namespace or builders to register the listeners with a step.\n\n[[stepExecutionListener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc", "title": "intercepting-execution", "heading": "intercepting-execution", "heading_level": 1, "file_order": 35, "section_index": 0, "content_hash": "6f5bf75846a887a2da54ffa7a41c4e2fc9649b6025ae5123a0ea058407f7b416", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc"}}
{"id": "sha256:e14606264f9ea1e3d244b74393454bb32b675b9f56a9d803580e904c61ede7c7", "content": "`StepExecutionListener` represents the most generic listener for `Step` execution. It\nallows for notification before a `Step` is started and after it ends, whether it ended\nnormally or failed, as the following example shows:\n\n[source, java]\n----\npublic interface StepExecutionListener extends StepListener {\n\n void beforeStep(StepExecution stepExecution);\n\n ExitStatus afterStep(StepExecution stepExecution);\n\n}\n----\n\n`afterStep` has a return type of `ExitStatus`, to give listeners the chance to\nmodify the exit code that is returned upon completion of a `Step`.\n\nThe annotations corresponding to this interface are:\n\n* `@BeforeStep`\n* `@AfterStep`\n\n[[chunkListener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc", "title": "intercepting-execution", "heading": "`StepExecutionListener`", "heading_level": 2, "file_order": 35, "section_index": 1, "content_hash": "e14606264f9ea1e3d244b74393454bb32b675b9f56a9d803580e904c61ede7c7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc"}}
{"id": "sha256:c0be451eb92847f7015e8b451e73588d61e58232cff132519d040295b868cf7b", "content": "A \"`chunk`\" is defined as the items processed within the scope of a transaction. Committing a\ntransaction, at each commit interval, commits a chunk. You can use a `ChunkListener` to\nperform logic before a chunk begins processing or after a chunk has completed\nsuccessfully or in failure, as the following interface definition shows:\n\n[source, java]\n----\npublic interface ChunkListener<I, O> extends StepListener {\n\n void beforeChunk(Chunk<I> chunk);\n void afterChunk(Chunk<O> chunk);\n void afterChunkError(Exception exception, Chunk<O> chunk);\n\n}\n----\n\nThe `beforeChunk` method is called after the transaction is started after reading a chunk\nof items but before processing start. Conversely, `afterChunk` is called after the chunk\nis written but before the transaction is committed or rolled back.\n\nNOTE: The `ChunkListener` listener interface is not called in concurrent steps\n\nThe annotations corresponding to this interface are:\n\n* `@BeforeChunk`\n* `@AfterChunk`\n* `@AfterChunkError`\n\nA `ChunkListener` is not designed to throw checked exceptions. Errors must be handled in the\nimplementation or the step will terminate.\n\n[[itemReadListener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc", "title": "intercepting-execution", "heading": "`ChunkListener`", "heading_level": 2, "file_order": 35, "section_index": 2, "content_hash": "c0be451eb92847f7015e8b451e73588d61e58232cff132519d040295b868cf7b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc"}}
{"id": "sha256:cf2ea912508125eb211066cb4cbaf1e3076119c440f486f802e3cbb8962ef316", "content": "When discussing skip logic previously, it was mentioned that it may be beneficial to log\nthe skipped records so that they can be dealt with later. In the case of read errors,\nthis can be done with an `ItemReaderListener`, as the following interface\ndefinition shows:\n\n[source, java]\n----\npublic interface ItemReadListener<T> extends StepListener {\n\n void beforeRead();\n void afterRead(T item);\n void onReadError(Exception ex);\n\n}\n----\n\nThe `beforeRead` method is called before each call to read on the `ItemReader`. The\n`afterRead` method is called after each successful call to read and is passed the item\nthat was read. If there was an error while reading, the `onReadError` method is called.\nThe exception encountered is provided so that it can be logged.\n\nThe annotations corresponding to this interface are:\n\n* `@BeforeRead`\n* `@AfterRead`\n* `@OnReadError`\n\n[[itemProcessListener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc", "title": "intercepting-execution", "heading": "`ItemReadListener`", "heading_level": 2, "file_order": 35, "section_index": 3, "content_hash": "cf2ea912508125eb211066cb4cbaf1e3076119c440f486f802e3cbb8962ef316", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc"}}
{"id": "sha256:6c3e485febf2f548c196897f68388e2206a6faaa89134f381f4a9a8800928e64", "content": "As with the `ItemReadListener`, the processing of an item can be \"`listened`\" to, as\nthe following interface definition shows:\n\n[source, java]\n----\npublic interface ItemProcessListener<T, S> extends StepListener {\n\n void beforeProcess(T item);\n void afterProcess(T item, S result);\n void onProcessError(T item, Exception e);\n\n}\n----\n\nThe `beforeProcess` method is called before `process` on the `ItemProcessor` and is\nhanded the item that is to be processed. The `afterProcess` method is called after the\nitem has been successfully processed. If there was an error while processing, the\n`onProcessError` method is called. The exception encountered and the item that was\nattempted to be processed are provided, so that they can be logged.\n\nThe annotations corresponding to this interface are:\n\n* `@BeforeProcess`\n* `@AfterProcess`\n* `@OnProcessError`\n\n[[itemWriteListener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc", "title": "intercepting-execution", "heading": "`ItemProcessListener`", "heading_level": 2, "file_order": 35, "section_index": 4, "content_hash": "6c3e485febf2f548c196897f68388e2206a6faaa89134f381f4a9a8800928e64", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc"}}
{"id": "sha256:77eb97c363043a6c1d6e08af8dd729dcf29a6b245aa0588118294fa679043f94", "content": "You can \"`listen`\" to the writing of an item with the `ItemWriteListener`, as the\nfollowing interface definition shows:\n\n[source, java]\n----\npublic interface ItemWriteListener<S> extends StepListener {\n\n void beforeWrite(List<? extends S> items);\n void afterWrite(List<? extends S> items);\n void onWriteError(Exception exception, List<? extends S> items);\n\n}\n----\n\nThe `beforeWrite` method is called before `write` on the `ItemWriter` and is handed the\nlist of items that is written. The `afterWrite` method is called after the items have been\nsuccessfully written, but before committing the transaction associated with the chunk's processing.\nIf there was an error while writing, the `onWriteError` method is called.\nThe exception encountered and the item that was attempted to be written are\nprovided, so that they can be logged.\n\nThe annotations corresponding to this interface are:\n\n* `@BeforeWrite`\n* `@AfterWrite`\n* `@OnWriteError`\n\n[[skipListener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc", "title": "intercepting-execution", "heading": "`ItemWriteListener`", "heading_level": 2, "file_order": 35, "section_index": 5, "content_hash": "77eb97c363043a6c1d6e08af8dd729dcf29a6b245aa0588118294fa679043f94", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc"}}
{"id": "sha256:cf475e946d115dde1ef6479024e43c055320f3535ed44066d924e239611bb010", "content": "`ItemReadListener`, `ItemProcessListener`, and `ItemWriteListener` all provide mechanisms\nfor being notified of errors, but none informs you that a record has actually been\nskipped. `onWriteError`, for example, is called even if an item is retried and\nsuccessful. For this reason, there is a separate interface for tracking skipped items, as\nthe following interface definition shows:\n\n[source, java]\n----\npublic interface SkipListener<T,S> extends StepListener {\n\n void onSkipInRead(Throwable t);\n void onSkipInProcess(T item, Throwable t);\n void onSkipInWrite(S item, Throwable t);\n\n}\n----\n\n`onSkipInRead` is called whenever an item is skipped while reading. It should be noted\nthat rollbacks may cause the same item to be registered as skipped more than once.\n`onSkipInWrite` is called when an item is skipped while writing. Because the item has\nbeen read successfully (and not skipped), it is also provided the item itself as an\nargument.\n\nThe annotations corresponding to this interface are:\n\n* `@OnSkipInRead`\n* `@OnSkipInWrite`\n* `@OnSkipInProcess`\n\n[[skipListenersAndTransactions]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc", "title": "intercepting-execution", "heading": "`SkipListener`", "heading_level": 2, "file_order": 35, "section_index": 6, "content_hash": "cf475e946d115dde1ef6479024e43c055320f3535ed44066d924e239611bb010", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc"}}
{"id": "sha256:0332a575249bfb21cfa647ee6d36a1fe439f8da342d87ebb94dd7717fb11274b", "content": "One of the most common use cases for a `SkipListener` is to log out a skipped item, so\nthat another batch process or even human process can be used to evaluate and fix the\nissue that leads to the skip. Because there are many cases in which the original transaction\nmay be rolled back, Spring Batch makes two guarantees:\n\n* The appropriate skip method (depending on when the error happened) is called only once\nper item.\n* The `SkipListener` is always called just before the transaction is committed. This is\nto ensure that any transactional resources call by the listener are not rolled back by a\nfailure within the `ItemWriter`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc", "title": "intercepting-execution", "heading": "SkipListeners and Transactions", "heading_level": 3, "file_order": 35, "section_index": 7, "content_hash": "0332a575249bfb21cfa647ee6d36a1fe439f8da342d87ebb94dd7717fb11274b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/intercepting-execution.adoc"}}
{"id": "sha256:9facd4ce0d00970592fe950e1f8eebf9794c4a5654b2d7243c880cf1f196a2ae", "content": "[[registeringItemStreams]]\n\nThe step has to take care of `ItemStream` callbacks at the necessary points in its\nlifecycle. (For more information on the `ItemStream` interface, see\nxref:readers-and-writers/item-stream.adoc[ItemStream]). This is vital if a step fails and might\nneed to be restarted, because the `ItemStream` interface is where the step gets the\ninformation it needs about persistent state between executions.\n\nIf the `ItemReader`, `ItemProcessor`, or `ItemWriter` itself implements the `ItemStream`\ninterface, these are registered automatically. Any other streams need to be\nregistered separately. This is often the case where indirect dependencies, such as\ndelegates, are injected into the reader and writer. You can register a stream on the\n`step` through the `stream` element.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to register a `stream` on a `step` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(2).transactionManager(transactionManager)\n .reader(itemReader())\n .writer(compositeItemWriter())\n .stream(fileItemWriter1())\n .stream(fileItemWriter2())\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to register a `stream` on a `step` in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<step id=\"step1\">\n <tasklet>\n <chunk reader=\"itemReader\" writer=\"compositeWriter\" commit-interval=\"2\">\n <streams>\n <stream ref=\"fileItemWriter1\"/>\n <stream ref=\"fileItemWriter2\"/>\n </streams>\n </chunk>\n </tasklet>\n</step>\n----\n\n====\n\nIn the preceding example, the `CompositeItemWriter` is not an `ItemStream`, but both of its\ndelegates are. Therefore, both delegate writers must be explicitly registered as streams\nfor the framework to handle them correctly. The `ItemReader` does not need to be\nexplicitly registered as a stream because it is a direct property of the `Step`. The step\nis now restartable, and the state of the reader and writer is correctly persisted in the\nevent of a failure.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/registering-item-streams.adoc", "title": "registering-item-streams", "heading": "registering-item-streams", "heading_level": 1, "file_order": 36, "section_index": 0, "content_hash": "9facd4ce0d00970592fe950e1f8eebf9794c4a5654b2d7243c880cf1f196a2ae", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/registering-item-streams.adoc"}}
{"id": "sha256:c065ff86d7b17504fd9b8c6ea730813dd15fd9623e3deb38fce11a73e65aa9b4", "content": "[[stepRestart]]\n\nIn the \"`xref:job.adoc[Configuring and Running a Job]`\" section , restarting a\n`Job` was discussed. Restart has numerous impacts on steps, and, consequently, may\nrequire some specific configuration.\n\n[[startLimit]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/restart.adoc", "title": "restart", "heading": "restart", "heading_level": 1, "file_order": 37, "section_index": 0, "content_hash": "c065ff86d7b17504fd9b8c6ea730813dd15fd9623e3deb38fce11a73e65aa9b4", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/restart.adoc"}}
{"id": "sha256:2abb89a3b7eb94670e254e619e77cbf775d38ce13fe3049bb41dc785785140e7", "content": "There are many scenarios where you may want to control the number of times a `Step` can\nbe started. For example, you might need to configure a particular `Step` so that it\nruns only once because it invalidates some resource that must be fixed manually before it can\nbe run again. This is configurable on the step level, since different steps may have\ndifferent requirements. A `Step` that can be executed only once can exist as part of the\nsame `Job` as a `Step` that can be run infinitely.\n\n[tabs]\n====\nJava::\n+\nThe following code fragment shows an example of a start limit configuration in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(itemReader())\n .writer(itemWriter())\n .startLimit(1)\n .build();\n}\n----\n\nXML::\n+\nThe following code fragment shows an example of a start limit configuration in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<step id=\"step1\">\n <tasklet start-limit=\"1\">\n <chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"10\"/>\n </tasklet>\n</step>\n----\n\n====\n\nThe step shown in the preceding example can be run only once. Attempting to run it again\ncauses a `StartLimitExceededException` to be thrown. Note that the default value for the\nstart-limit is `Integer.MAX_VALUE`.\n\n[[allowStartIfComplete]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/restart.adoc", "title": "restart", "heading": "Setting a Start Limit", "heading_level": 2, "file_order": 37, "section_index": 1, "content_hash": "2abb89a3b7eb94670e254e619e77cbf775d38ce13fe3049bb41dc785785140e7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/restart.adoc"}}
{"id": "sha256:c474bd8249cf393e3d30eb041b72799fe870bddc915dc7cf639be10f9234c7c7", "content": "In the case of a restartable job, there may be one or more steps that should always be\nrun, regardless of whether or not they were successful the first time. An example might\nbe a validation step or a `Step` that cleans up resources before processing. During\nnormal processing of a restarted job, any step with a status of `COMPLETED` (meaning it\nhas already been completed successfully), is skipped. Setting `allow-start-if-complete` to\n`true` overrides this so that the step always runs.\n\n[tabs]\n====\nJava::\n+\nThe following code fragment shows how to define a restartable job in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(itemReader())\n .writer(itemWriter())\n .allowStartIfComplete(true)\n .build();\n}\n----\n\nXML::\n+\nThe following code fragment shows how to define a restartable job in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<step id=\"step1\">\n <tasklet allow-start-if-complete=\"true\">\n <chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"10\"/>\n </tasklet>\n</step>\n----\n\n====\n\n[[stepRestartExample]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/restart.adoc", "title": "restart", "heading": "Restarting a Completed `Step`", "heading_level": 2, "file_order": 37, "section_index": 2, "content_hash": "c474bd8249cf393e3d30eb041b72799fe870bddc915dc7cf639be10f9234c7c7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/restart.adoc"}}
{"id": "sha256:fee2babcd7072103d6cb4c9dd0b6e4d97120d253b33b40b4354f319af7fc59c0", "content": "[tabs]\n====\nJava::\n+\nThe following Java example shows how to configure a job to have steps that can be\nrestarted:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job footballJob(JobRepository jobRepository, Step playerLoad, Step gameLoad, Step playerSummarization) {\n\treturn new JobBuilder(\"footballJob\", jobRepository)\n .start(playerLoad)\n .next(gameLoad)\n .next(playerSummarization)\n .build();\n}\n\n@Bean\npublic Step playerLoad(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"playerLoad\", jobRepository)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(playerFileItemReader())\n .writer(playerWriter())\n .build();\n}\n\n@Bean\npublic Step gameLoad(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"gameLoad\", jobRepository)\n .allowStartIfComplete(true)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(gameFileItemReader())\n .writer(gameWriter())\n .build();\n}\n\n@Bean\npublic Step playerSummarization(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"playerSummarization\", jobRepository)\n .startLimit(2)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(playerSummarizationSource())\n .writer(summaryWriter())\n .build();\n}\n----\n\nXML::\n+\nThe following XML example shows how to configure a job to have steps that can be\nrestarted:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"footballJob\" restartable=\"true\">\n <step id=\"playerload\" next=\"gameLoad\">\n <tasklet>\n <chunk reader=\"playerFileItemReader\" writer=\"playerWriter\"\n commit-interval=\"10\" />\n </tasklet>\n </step>\n <step id=\"gameLoad\" next=\"playerSummarization\">\n <tasklet allow-start-if-complete=\"true\">\n <chunk reader=\"gameFileItemReader\" writer=\"gameWriter\"\n commit-interval=\"10\"/>\n </tasklet>\n </step>\n <step id=\"playerSummarization\">\n <tasklet start-limit=\"2\">\n <chunk reader=\"playerSummarizationSource\" writer=\"summaryWriter\"\n commit-interval=\"10\"/>\n </tasklet>\n </step>\n</job>\n----\n\n====\n\nThe preceding example configuration is for a job that loads in information about football\ngames and summarizes them. It contains three steps: `playerLoad`, `gameLoad`, and\n`playerSummarization`. The `playerLoad` step loads player information from a flat file,\nwhile the `gameLoad` step does the same for games. The final step,\n`playerSummarization`, then summarizes the statistics for each player, based upon the\nprovided games. It is assumed that the file loaded by `playerLoad` must be loaded only\nonce but that `gameLoad` can load any games found within a particular directory,\ndeleting them after they have been successfully loaded into the database. As a result,\nthe `playerLoad` step contains no additional configuration. It can be started any number\nof times is skipped if complete. The `gameLoad` step, however, needs to be run\nevery time in case extra files have been added since it last ran. It has\n`allow-start-if-complete` set to `true` to always be started. (It is assumed\nthat the database table that games are loaded into has a process indicator on it, to ensure\nnew games can be properly found by the summarization step). The summarization step,\nwhich is the most important in the job, is configured to have a start limit of 2. This\nis useful because, if the step continually fails, a new exit code is returned to the\noperators that control job execution, and it can not start again until manual\nintervention has taken place.\n\nNOTE: This job provides an example for this document and is not the same as the `footballJob`\nfound in the samples project.\n\nThe remainder of this section describes what happens for each of the three runs of the\n`footballJob` example.\n\nRun 1:\n\n. `playerLoad` runs and completes successfully, adding 400 players to the `PLAYERS`\ntable.\n. `gameLoad` runs and processes 11 files worth of game data, loading their contents\ninto the `GAMES` table.\n. `playerSummarization` begins processing and fails after 5 minutes.\n\nRun 2:\n\n. `playerLoad` does not run, since it has already completed successfully, and\n`allow-start-if-complete` is `false` (the default).\n. `gameLoad` runs again and processes another 2 files, loading their contents into the\n`GAMES` table as well (with a process indicator indicating they have yet to be\nprocessed).\n. `playerSummarization` begins processing of all remaining game data (filtering using the\nprocess indicator) and fails again after 30 minutes.\n\nRun 3:\n\n. `playerLoad` does not run, since it has already completed successfully, and\n`allow-start-if-complete` is `false` (the default).\n. `gameLoad` runs again and processes another 2 files, loading their contents into the\n`GAMES` table as well (with a process indicator indicating they have yet to be\nprocessed).\n. `playerSummarization` is not started and the job is immediately killed, since this is\nthe third execution of `playerSummarization`, and its limit is only 2. Either the limit\nmust be raised or the `Job` must be executed as a new `JobInstance`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/restart.adoc", "title": "restart", "heading": "`Step` Restart Configuration Example", "heading_level": 2, "file_order": 37, "section_index": 3, "content_hash": "fee2babcd7072103d6cb4c9dd0b6e4d97120d253b33b40b4354f319af7fc59c0", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/restart.adoc"}}
{"id": "sha256:f7b2be61dfb5c0445177ade1a2b0a13d67ba43f43529f722694216c46e359c4e", "content": "[[retryLogic]]\n\nIn most cases, you want an exception to cause either a skip or a `Step` failure. However,\nnot all exceptions are deterministic. If a `FlatFileParseException` is encountered while\nreading, it is always thrown for that record. Resetting the `ItemReader` does not help.\nHowever, for other exceptions (such as a `DeadlockLoserDataAccessException`, which\nindicates that the current process has attempted to update a record that another process\nholds a lock on), waiting and trying again might result in success.\n\n[tabs]\n====\nJava::\n+\nIn Java, retry should be configured as follows:\n+\n[source, java]\n----\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n // retry policy configuration\n int retryLimit = 3;\n var retrybaleExceptions = Set.of(DeadlockLoserDataAccessException.class);\n RetryPolicy retryPolicy = RetryPolicy.builder()\n .maxRetries(retryLimit)\n .includes(retrybaleExceptions)\n .build();\n\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(2).transactionManager(transactionManager)\n .reader(itemReader())\n .writer(itemWriter())\n .faultTolerant()\n .retryPolicy(retryPolicy)\n .build();\n}\n----\n\nXML::\n+\nIn XML, retry should be configured as follows:\n+\n[source, xml]\n----\n<step id=\"step1\">\n <tasklet>\n <chunk reader=\"itemReader\" writer=\"itemWriter\"\n commit-interval=\"2\" retry-limit=\"3\">\n <retryable-exception-classes>\n <include class=\"org.springframework.dao.DeadlockLoserDataAccessException\"/>\n </retryable-exception-classes>\n </chunk>\n </tasklet>\n</step>\n----\n\n====\n\nThe `Step` allows a limit for the number of times an individual item can be retried and a\nlist of exceptions that are \"`retryable`\".", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/retry-logic.adoc", "title": "retry-logic", "heading": "retry-logic", "heading_level": 1, "file_order": 38, "section_index": 0, "content_hash": "f7b2be61dfb5c0445177ade1a2b0a13d67ba43f43529f722694216c46e359c4e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/retry-logic.adoc"}}
{"id": "sha256:7a46b474814d6863da9f5878c8275463486dd8babba9374085b71ff6c18f6cf0", "content": "[[transactionAttributes]]\n\nYou can use transaction attributes to control the `isolation`, `propagation`, and\n`timeout` settings. You can find more information on setting transaction attributes in\nthe\nhttps://docs.spring.io/spring/docs/current/spring-framework-reference/data-access.html#transaction[Spring\ncore documentation].\n\n[tabs]\n====\nJava::\n+\nThe following example sets the `isolation`, `propagation`, and `timeout` transaction\nattributes in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\tDefaultTransactionAttribute attribute = new DefaultTransactionAttribute();\n\tattribute.setPropagationBehavior(Propagation.REQUIRED.value());\n\tattribute.setIsolationLevel(Isolation.DEFAULT.value());\n\tattribute.setTimeout(30);\n\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(2).transactionManager(transactionManager)\n .reader(itemReader())\n .writer(itemWriter())\n .transactionAttribute(attribute)\n .build();\n}\n----\n\nXML::\n+\nThe following example sets the `isolation`, `propagation`, and `timeout` transaction\nattributes in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<step id=\"step1\">\n <tasklet>\n <chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"2\"/>\n <transaction-attributes isolation=\"DEFAULT\"\n propagation=\"REQUIRED\"\n timeout=\"30\"/>\n </tasklet>\n</step>\n----\n\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/transaction-attributes.adoc", "title": "transaction-attributes", "heading": "transaction-attributes", "heading_level": 1, "file_order": 39, "section_index": 0, "content_hash": "7a46b474814d6863da9f5878c8275463486dd8babba9374085b71ff6c18f6cf0", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing/transaction-attributes.adoc"}}
{"id": "sha256:6fd0cd3224589aaf1b16a99853914a72b7a2179a80b6b19b3e163af4e55ae000", "content": "[[chunkOrientedProcessing]]\n\nSpring Batch uses a \"`chunk-oriented`\" processing style in its most common\nimplementation. Chunk oriented processing refers to reading the data one at a time and\ncreating 'chunks' that are written out within a transaction boundary. Once the number of\nitems read equals the commit interval, the entire chunk is written out by the\n`ItemWriter`, and then the transaction is committed. The following image shows the\nprocess:\n\n.Chunk-oriented Processing\nimage::chunk-oriented-processing.png[Chunk Oriented Processing, scaledwidth=\"60%\"]\n\nThe following pseudo code shows the same concepts in a simplified form:\n\n[source, java]\n----\nList items = new Arraylist();\nfor(int i = 0; i < commitInterval; i++){\n Object item = itemReader.read();\n if (item != null) {\n items.add(item);\n }\n}\nitemWriter.write(items);\n----\n\nYou can also configure a chunk-oriented step with an optional `ItemProcessor`\nto process items before passing them to the `ItemWriter`. The following image\nshows the process when an `ItemProcessor` is registered in the step:\n\n.Chunk-oriented Processing with Item Processor\nimage::chunk-oriented-processing-with-item-processor.png[Chunk Oriented Processing With Item Processor, scaledwidth=\"60%\"]\n\nThe following pseudo code shows how this is implemented in a simplified form:\n\n[source, java]\n----\nList items = new Arraylist();\nfor(int i = 0; i < commitInterval; i++){\n Object item = itemReader.read();\n if (item != null) {\n items.add(item);\n }\n}\n\nList processedItems = new Arraylist();\nfor(Object item: items){\n Object processedItem = itemProcessor.process(item);\n if (processedItem != null) {\n processedItems.add(processedItem);\n }\n}\n\nitemWriter.write(processedItems);\n----\n\nFor more details about item processors and their use cases, see the\nxref:processor.adoc[Item processing] section.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing.adoc", "title": "chunk-oriented-processing", "heading": "chunk-oriented-processing", "heading_level": 1, "file_order": 40, "section_index": 0, "content_hash": "6fd0cd3224589aaf1b16a99853914a72b7a2179a80b6b19b3e163af4e55ae000", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/chunk-oriented-processing.adoc"}}
{"id": "sha256:a51423f46ac38c8a4932f5777f3d1002667b3091691cf8a96e162cb4a3d34109", "content": "[[controllingStepFlow]]\n\nWith the ability to group steps together within an owning job comes the need to be able\nto control how the job \"`flows`\" from one step to another. The failure of a `Step` does not\nnecessarily mean that the `Job` should fail. Furthermore, there may be more than one type\nof \"`success`\" that determines which `Step` should be executed next. Depending upon how a\ngroup of `Steps` is configured, certain steps may not even be processed at all.\n\n[IMPORTANT]\n.Step bean method proxying in flow definitions\n====\nA step instance must be unique within a flow definition. When a step has multiple outcomes in a flow definition,\nit is important that the same instance of the step is passed to the flow definition methods (`start`, `from`, etc).\nOtherwise, the flow execution might behave unexpectedly.\n\nIn the following examples, steps are injected as parameters to the flow or job bean definition methods. This dependency injection style guarantees the uniqueness of steps in the flow definition.\nHowever, if the flow is defined by calling step definition methods annotated with `@Bean`, then steps might not be unique if bean method proxying is disabled (ie `@Configuration(proxyBeanMethods = false)`).\nIf the inter-bean injection style is preferred, then bean method proxying must be enabled.\n\nPlease refer to the https://docs.spring.io/spring-framework/reference/core/beans/java/configuration-annotation.html[Using the @Configuration annotation]\nsection for more details about bean method proxying in Spring Framework.\n====\n\n[[SequentialFlow]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "controlling-flow", "heading_level": 1, "file_order": 41, "section_index": 0, "content_hash": "a51423f46ac38c8a4932f5777f3d1002667b3091691cf8a96e162cb4a3d34109", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:8d538d0069f0d38269aaf208a81a2e10d5d383020299a9a5709a6154f2817de1", "content": "The simplest flow scenario is a job where all of the steps execute sequentially, as\nthe following image shows:\n\n.Sequential Flow\nimage::sequential-flow.png[Sequential Flow, scaledwidth=\"60%\"]\n\nThis can be achieved by using `next` in a `step`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to use the `next()` method in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository, Step stepA, Step stepB, Step stepC) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(stepA)\n .next(stepB)\n .next(stepC)\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to use the `next` attribute in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"job\">\n <step id=\"stepA\" parent=\"s1\" next=\"stepB\" />\n <step id=\"stepB\" parent=\"s2\" next=\"stepC\"/>\n <step id=\"stepC\" parent=\"s3\" />\n</job>\n----\n\n====\n\nIn the scenario above, `stepA` runs first because it is the first `Step` listed. If\n`stepA` completes normally, `stepB` runs, and so on. However, if `step A` fails,\nthe entire `Job` fails and `stepB` does not execute.\n\n[role=\"xmlContent\"]\nNOTE: With the Spring Batch XML namespace, the first step listed in the configuration is\n_always_ the first step run by the `Job`. The order of the other step elements does not\nmatter, but the first step must always appear first in the XML.\n\n[[conditionalFlow]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Sequential Flow", "heading_level": 2, "file_order": 41, "section_index": 1, "content_hash": "8d538d0069f0d38269aaf208a81a2e10d5d383020299a9a5709a6154f2817de1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:5c2ef0a73eea91b8ffc0a1aa50495d8c4dd07aa03ab5a7a4a8dad248ef9bfd20", "content": "In the preceding example, there are only two possibilities:\n\n. The `step` is successful, and the next `step` should be executed.\n. The `step` failed, and, thus, the `job` should fail.\n\nIn many cases, this may be sufficient. However, what about a scenario in which the\nfailure of a `step` should trigger a different `step`, rather than causing failure? The\nfollowing image shows such a flow:\n\n.Conditional Flow\nimage::conditional-flow.png[Conditional Flow, scaledwidth=\"60%\"]\n\n[[nextElement]]\n[tabs]\n====\nJava::\n+\nThe Java API offers a fluent set of methods that let you specify the flow and what to do\nwhen a step fails. The following example shows how to specify one step (`stepA`) and then\nproceed to either of two different steps (`stepB` or `stepC`), depending on whether\n`stepA` succeeds:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository, Step stepA, Step stepB, Step stepC) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(stepA)\n .on(\"*\").to(stepB)\n .from(stepA).on(\"FAILED\").to(stepC)\n .end()\n .build();\n}\n----\n\nXML::\n+\nTo handle more complex scenarios, the Spring Batch XML namespace lets you define transitions\nelements within the step element. One such transition is the `next`\nelement. Like the `next` attribute, the `next` element tells the `Job` which `Step` to\nexecute next. However, unlike the attribute, any number of `next` elements are allowed on\na given `Step`, and there is no default behavior in the case of failure. This means that, if\ntransition elements are used, all of the behavior for the `Step` transitions must be\ndefined explicitly. Note also that a single step cannot have both a `next` attribute and\na `transition` element.\n+\nThe `next` element specifies a pattern to match and the step to execute next, as\nthe following example shows:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"job\">\n <step id=\"stepA\" parent=\"s1\">\n <next on=\"*\" to=\"stepB\" />\n <next on=\"FAILED\" to=\"stepC\" />\n </step>\n <step id=\"stepB\" parent=\"s2\" next=\"stepC\" />\n <step id=\"stepC\" parent=\"s3\" />\n</job>\n----\n\n====\n\n[tabs]\n====\nJava::\n+\nWhen using java configuration, the `on()` method uses a simple pattern-matching scheme to\nmatch the `ExitStatus` that results from the execution of the `Step`.\n\nXML::\n+\nWhen using XML configuration, the `on` attribute of a transition element uses a simple\npattern-matching scheme to match the `ExitStatus` that results from the execution of the\n`Step`.\n\n====\n\nOnly two special characters are allowed in the pattern:\n\n* `*` matches zero or more characters\n* `?` matches exactly one character\n\nFor example, `c*t` matches `cat` and `count`, while `c?t` matches `cat` but not `count`.\n\nWhile there is no limit to the number of transition elements on a `Step`, if the `Step`\nexecution results in an `ExitStatus` that is not covered by an element, the\nframework throws an exception and the `Job` fails. The framework automatically orders\ntransitions from most specific to least specific. This means that, even if the ordering\nwere swapped for `stepA` in the preceding example, an `ExitStatus` of `FAILED` would still go\nto `stepC`.\n\n[[batchStatusVsExitStatus]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Conditional Flow", "heading_level": 2, "file_order": 41, "section_index": 2, "content_hash": "5c2ef0a73eea91b8ffc0a1aa50495d8c4dd07aa03ab5a7a4a8dad248ef9bfd20", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:b78527dfe8b966c8db642a1193f802dd608af20b1671172e5d0bde6c0bd70c2f", "content": "When configuring a `Job` for conditional flow, it is important to understand the\ndifference between `BatchStatus` and `ExitStatus`. `BatchStatus` is an enumeration that\nis a property of both `JobExecution` and `StepExecution` and is used by the framework to\nrecord the status of a `Job` or `Step`. It can be one of the following values:\n`COMPLETED`, `STARTING`, `STARTED`, `STOPPING`, `STOPPED`, `FAILED`, `ABANDONED`, or\n`UNKNOWN`. Most of them are self explanatory: `COMPLETED` is the status set when a step\nor job has completed successfully, `FAILED` is set when it fails, and so on.\n\n[tabs]\n====\nJava::\n+\nThe following example contains the `on` element when using Java Configuration:\n+\n[source, java]\n----\n...\n.from(stepA).on(\"FAILED\").to(stepB)\n...\n----\n\nXML::\n+\nThe following example contains the `next` element when using XML configuration:\n+\n[source, xml]\n----\n<next on=\"FAILED\" to=\"stepB\" />\n----\n\n====\n\nAt first glance, it would appear that `on` references the `BatchStatus` of the `Step` to\nwhich it belongs. However, it actually references the `ExitStatus` of the `Step`. As the\nname implies, `ExitStatus` represents the status of a `Step` after it finishes execution.\n\n[tabs]\n====\nJava::\n+\nWhen using Java configuration, the `on()` method shown in the preceding\nJava configuration example references the exit code of `ExitStatus`.\n\nXML::\n+\nMore specifically, when using XML configuration, the `next` element shown in the\npreceding XML configuration example references the exit code of `ExitStatus`.\n====\n\nIn English, it says: \"`go to stepB if the exit code is FAILED`\". By default, the exit\ncode is always the same as the `BatchStatus` for the `Step`, which is why the preceding entry\nworks. However, what if the exit code needs to be different? A good example comes from\nthe skip sample job within the samples project:\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to work with a different exit code in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository, Step step1, Step step2, Step errorPrint1) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(step1).on(\"FAILED\").end()\n .from(step1).on(\"COMPLETED WITH SKIPS\").to(errorPrint1)\n .from(step1).on(\"*\").to(step2)\n .end()\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to work with a different exit code in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<step id=\"step1\" parent=\"s1\">\n <end on=\"FAILED\" />\n <next on=\"COMPLETED WITH SKIPS\" to=\"errorPrint1\" />\n <next on=\"*\" to=\"step2\" />\n</step>\n----\n\n====\n\n`step1` has three possibilities:\n\n* The `Step` failed, in which case the job should fail.\n* The `Step` completed successfully.\n* The `Step` completed successfully but with an exit code of `COMPLETED WITH SKIPS`. In\nthis case, a different step should be run to handle the errors.\n\nThe preceding configuration works. However, something needs to change the exit code based on\nthe condition of the execution having skipped records, as the following example shows:\n\n[source, java]\n----\npublic class SkipCheckingListener implements StepExecutionListener {\n @Override\n public ExitStatus afterStep(StepExecution stepExecution) {\n String exitCode = stepExecution.getExitStatus().getExitCode();\n if (!exitCode.equals(ExitStatus.FAILED.getExitCode()) &&\n stepExecution.getSkipCount() > 0) {\n return new ExitStatus(\"COMPLETED WITH SKIPS\");\n } else {\n return null;\n }\n }\n}\n----\n\nThe preceding code is a `StepExecutionListener` that first checks to make sure the `Step` was\nsuccessful and then checks to see if the skip count on the `StepExecution` is higher than\n0. If both conditions are met, a new `ExitStatus` with an exit code of\n`COMPLETED WITH SKIPS` is returned.\n\n[[configuringForStop]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Batch Status Versus Exit Status", "heading_level": 3, "file_order": 41, "section_index": 3, "content_hash": "b78527dfe8b966c8db642a1193f802dd608af20b1671172e5d0bde6c0bd70c2f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:65d0075974584107da145ab931a0cd9dbb47d6fe2f6183f01f80a9f19eefcb03", "content": "After the discussion of xref:step/controlling-flow.adoc#batchStatusVsExitStatus[`BatchStatus` and `ExitStatus`],\none might wonder how the `BatchStatus` and `ExitStatus` are determined for the `Job`.\nWhile these statuses are determined for the `Step` by the code that is executed, the\nstatuses for the `Job` are determined based on the configuration.\n\nSo far, all of the job configurations discussed have had at least one final `Step` with\nno transitions.\n\n[tabs]\n====\nJava::\n+\nIn the following Java example, after the `step` executes, the `Job` ends:\n+\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository, Step step1) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(step1)\n .build();\n}\n----\n\nXML::\n+\nIn the following XML example, after the `step` executes, the `Job` ends:\n+\n[source, xml]\n----\n<step id=\"step1\" parent=\"s3\"/>\n----\n\n====\n\nIf no transitions are defined for a `Step`, the status of the `Job` is defined as\nfollows:\n\n* If the `Step` ends with `ExitStatus` of `FAILED`, the `BatchStatus` and `ExitStatus` of\nthe `Job` are both `FAILED`.\n\n* Otherwise, the `BatchStatus` and `ExitStatus` of the `Job` are both `COMPLETED`.\n\nWhile this method of terminating a batch job is sufficient for some batch jobs, such as a\nsimple sequential step job, custom defined job-stopping scenarios may be required. For\nthis purpose, Spring Batch provides three transition elements to stop a `Job` (in\naddition to the xref:step/controlling-flow.adoc#nextElement[`next` element] that we discussed previously).\nEach of these stopping elements stops a `Job` with a particular `BatchStatus`. It is\nimportant to note that the stop transition elements have no effect on either the\n`BatchStatus` or `ExitStatus` of any `Steps` in the `Job`. These elements affect only the\nfinal statuses of the `Job`. For example, it is possible for every step in a job to have\na status of `FAILED` but for the job to have a status of `COMPLETED`.\n\n[[endElement]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Configuring for Stop", "heading_level": 2, "file_order": 41, "section_index": 4, "content_hash": "65d0075974584107da145ab931a0cd9dbb47d6fe2f6183f01f80a9f19eefcb03", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:64bc5a2bf922c7acf9a44017d4f582a8d15e5cc47babb5288a4ff9ed942bdb6f", "content": "Configuring a step end instructs a `Job` to stop with a `BatchStatus` of `COMPLETED`. A\n`Job` that has finished with a status of `COMPLETED` cannot be restarted (the framework throws\na `JobInstanceAlreadyCompleteException`).\n\n[tabs]\n====\nJava::\n+\nWhen using Java configuration, the `end` method is used for this task. The `end` method\nalso allows for an optional `exitStatus` parameter that you can use to customize the\n`ExitStatus` of the `Job`. If no `exitStatus` value is provided, the `ExitStatus` is\n`COMPLETED` by default, to match the `BatchStatus`.\n\nXML::\n+\nWhen using XML configuration, you can use the `end` element for this task. The `end` element\nalso allows for an optional `exit-code` attribute that you can use to customize the\n`ExitStatus` of the `Job`. If no `exit-code` attribute is given, the `ExitStatus` is\n`COMPLETED` by default, to match the `BatchStatus`.\n====\n\nConsider the following scenario: If `step2` fails, the `Job` stops with a\n`BatchStatus` of `COMPLETED` and an `ExitStatus` of `COMPLETED`, and `step3` does not run.\nOtherwise, execution moves to `step3`. Note that if `step2` fails, the `Job` is not\nrestartable (because the status is `COMPLETED`).\n\n[tabs]\n====\nJava::\n+\nThe following example shows the scenario in Java:\n+\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository, Step step1, Step step2, Step step3) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(step1)\n .next(step2)\n .on(\"FAILED\").end()\n .from(step2).on(\"*\").to(step3)\n .end()\n .build();\n}\n----\n\nXML::\n+\nThe following example shows the scenario in XML:\n+\n[source, xml]\n----\n<step id=\"step1\" parent=\"s1\" next=\"step2\">\n\n<step id=\"step2\" parent=\"s2\">\n <end on=\"FAILED\"/>\n <next on=\"*\" to=\"step3\"/>\n</step>\n\n<step id=\"step3\" parent=\"s3\">\n----\n\n====\n\n[[failElement]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Ending at a Step", "heading_level": 3, "file_order": 41, "section_index": 5, "content_hash": "64bc5a2bf922c7acf9a44017d4f582a8d15e5cc47babb5288a4ff9ed942bdb6f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:d7e299ba365f095278265488f06bcdb45f3fb76bafcfb617e6e475a40c1645f1", "content": "Configuring a step to fail at a given point instructs a `Job` to stop with a\n`BatchStatus` of `FAILED`. Unlike end, the failure of a `Job` does not prevent the `Job`\nfrom being restarted.\n\n[role=\"xmlContent\"]\nWhen using XML configuration, the `fail` element also allows for an optional `exit-code`\nattribute that can be used to customize the `ExitStatus` of the `Job`. If no `exit-code`\nattribute is given, the `ExitStatus` is `FAILED` by default, to match the\n`BatchStatus`.\n\nConsider the following scenario: If `step2` fails, the `Job` stops with a\n`BatchStatus` of `FAILED` and an `ExitStatus` of `EARLY TERMINATION` and `step3` does not\nexecute. Otherwise, execution moves to `step3`. Additionally, if `step2` fails and the\n`Job` is restarted, execution begins again on `step2`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows the scenario in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository, Step step1, Step step2, Step step3) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(step1)\n .next(step2).on(\"FAILED\").fail()\n .from(step2).on(\"*\").to(step3)\n .end()\n .build();\n}\n----\n\nXML::\n+\nThe following example shows the scenario in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<step id=\"step1\" parent=\"s1\" next=\"step2\">\n\n<step id=\"step2\" parent=\"s2\">\n <fail on=\"FAILED\" exit-code=\"EARLY TERMINATION\"/>\n <next on=\"*\" to=\"step3\"/>\n</step>\n\n<step id=\"step3\" parent=\"s3\">\n----\n\n====\n\n[[stopElement]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Failing a Step", "heading_level": 3, "file_order": 41, "section_index": 6, "content_hash": "d7e299ba365f095278265488f06bcdb45f3fb76bafcfb617e6e475a40c1645f1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:a3efe347ae8fac82ee8cd1e9f7303295f0646168c04d2846502e504dbba8064b", "content": "Configuring a job to stop at a particular step instructs a `Job` to stop with a\n`BatchStatus` of `STOPPED`. Stopping a `Job` can provide a temporary break in processing,\nso that the operator can take some action before restarting the `Job`.\n\n[tabs]\n====\nJava::\n+\nWhen using Java configuration, the `stopAndRestart` method requires a `restart` attribute\nthat specifies the step where execution should pick up when the Job is restarted.\n\nXML::\n+\nWhen using XML configuration, a `stop` element requires a `restart` attribute that specifies\nthe step where execution should pick up when the `Job` is restarted.\n====\n\nConsider the following scenario: If `step1` finishes with `COMPLETE`, the job then\nstops. Once it is restarted, execution begins on `step2`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows the scenario in Java:\n+\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository, Step step1, Step step2) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(step1).on(\"COMPLETED\").stopAndRestart(step2)\n .end()\n .build();\n}\n----\n\nXML::\n+\nThe following listing shows the scenario in XML:\n+\n[source, xml]\n----\n<step id=\"step1\" parent=\"s1\">\n <stop on=\"COMPLETED\" restart=\"step2\"/>\n</step>\n\n<step id=\"step2\" parent=\"s2\"/>\n----\n\n====\n\n[[programmaticFlowDecisions]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Stopping a Job at a Given Step", "heading_level": 3, "file_order": 41, "section_index": 7, "content_hash": "a3efe347ae8fac82ee8cd1e9f7303295f0646168c04d2846502e504dbba8064b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:f15a0db29f07bc70dc9a292a567e927b8200d9b7b06385661ee5f79ef9e07478", "content": "In some situations, more information than the `ExitStatus` may be required to decide\nwhich step to execute next. In this case, a `JobExecutionDecider` can be used to assist\nin the decision, as the following example shows:\n\n[source, java]\n----\npublic class MyDecider implements JobExecutionDecider {\n public FlowExecutionStatus decide(JobExecution jobExecution, StepExecution stepExecution) {\n String status;\n if (someCondition()) {\n status = \"FAILED\";\n }\n else {\n status = \"COMPLETED\";\n }\n return new FlowExecutionStatus(status);\n }\n}\n----\n\n[tabs]\n====\nJava::\n+\nIn the following example, a bean implementing the `JobExecutionDecider` is passed\ndirectly to the `next` call when using Java configuration:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository, MyDecider decider, Step step1, Step step2, Step step3) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(step1)\n .next(decider).on(\"FAILED\").to(step2)\n .from(decider).on(\"COMPLETED\").to(step3)\n .end()\n .build();\n}\n----\n\nXML::\n+\nIn the following sample job configuration, a `decision` specifies the decider to use as\nwell as all of the transitions:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"job\">\n <step id=\"step1\" parent=\"s1\" next=\"decision\" />\n\n <decision id=\"decision\" decider=\"decider\">\n <next on=\"FAILED\" to=\"step2\" />\n <next on=\"COMPLETED\" to=\"step3\" />\n </decision>\n\n <step id=\"step2\" parent=\"s2\" next=\"step3\"/>\n <step id=\"step3\" parent=\"s3\" />\n</job>\n\n<beans:bean id=\"decider\" class=\"com.MyDecider\"/>\n----\n\n====\n\n[[split-flows]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Programmatic Flow Decisions", "heading_level": 2, "file_order": 41, "section_index": 8, "content_hash": "f15a0db29f07bc70dc9a292a567e927b8200d9b7b06385661ee5f79ef9e07478", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:d3de57c540652e088e12fe7b0d59e2c74561d422336ab2a8084420cc36c915a2", "content": "Every scenario described so far has involved a `Job` that executes its steps one at a\ntime in a linear fashion. In addition to this typical style, Spring Batch also allows\nfor a job to be configured with parallel flows.\n\n[tabs]\n====\nJava::\n+\nJava-based configuration lets you configure splits through the provided builders. As the\nfollowing example shows, the `split` element contains one or more `flow` elements, where\nentire separate flows can be defined. A `split` element can also contain any of the\npreviously discussed transition elements, such as the `next` attribute or the `next`,\n`end`, or `fail` elements.\n+\n[source, java]\n----\n@Bean\npublic Flow flow1(Step step1, Step step2) {\n\treturn new FlowBuilder<SimpleFlow>(\"flow1\")\n .start(step1)\n .next(step2)\n .build();\n}\n\n@Bean\npublic Flow flow2(Step step3) {\n\treturn new FlowBuilder<SimpleFlow>(\"flow2\")\n .start(step3)\n .build();\n}\n\n@Bean\npublic Job job(JobRepository jobRepository, Flow flow1, Flow flow2, Step step4) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(flow1)\n .split(new SimpleAsyncTaskExecutor())\n .add(flow2)\n .next(step4)\n .end()\n .build();\n}\n----\n\nXML::\n+\nThe XML namespace lets you use the `split` element. As the following example shows,\nthe `split` element contains one or more `flow` elements, where entire separate flows can\nbe defined. A `split` element can also contain any of the previously discussed transition\nelements, such as the `next` attribute or the `next`, `end`, or `fail` elements.\n+\n[source, xml]\n----\n<split id=\"split1\" next=\"step4\">\n <flow>\n <step id=\"step1\" parent=\"s1\" next=\"step2\"/>\n <step id=\"step2\" parent=\"s2\"/>\n </flow>\n <flow>\n <step id=\"step3\" parent=\"s3\"/>\n </flow>\n</split>\n<step id=\"step4\" parent=\"s4\"/>\n----\n\n====\n\n[[external-flows]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Split Flows", "heading_level": 2, "file_order": 41, "section_index": 9, "content_hash": "d3de57c540652e088e12fe7b0d59e2c74561d422336ab2a8084420cc36c915a2", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:e4c4ac952a835abada46c67ebddbcc0cd8517f0e1aaf0dbb7d762d5dc237ee1f", "content": "Part of the flow in a job can be externalized as a separate bean definition and then\nre-used. There are two ways to do so. The first is to declare the flow as a\nreference to one defined elsewhere.\n\n[tabs]\n====\nJava::\n+\nThe following Java example shows how to declare a flow as a reference to a flow defined\nelsewhere:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository, Flow flow1, Step step3) {\n\treturn new JobBuilder(\"job\", jobRepository)\n .start(flow1)\n .next(step3)\n .end()\n .build();\n}\n\n@Bean\npublic Flow flow1(Step step1, Step step2) {\n\treturn new FlowBuilder<SimpleFlow>(\"flow1\")\n .start(step1)\n .next(step2)\n .build();\n}\n----\n\nXML::\n+\nThe following XML example shows how to declare a flow as a reference to a flow defined\nelsewhere:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"job\">\n <flow id=\"job1.flow1\" parent=\"flow1\" next=\"step3\"/>\n <step id=\"step3\" parent=\"s3\"/>\n</job>\n\n<flow id=\"flow1\">\n <step id=\"step1\" parent=\"s1\" next=\"step2\"/>\n <step id=\"step2\" parent=\"s2\"/>\n</flow>\n----\n\n====\n\nThe effect of defining an external flow, as shown in the preceding example, is to insert\nthe steps from the external flow into the job as if they had been declared inline. In\nthis way, many jobs can refer to the same template flow and compose such templates into\ndifferent logical flows. This is also a good way to separate the integration testing of\nthe individual flows.\n\nThe other form of an externalized flow is to use a `JobStep`. A `JobStep` is similar to a\n`FlowStep` but actually creates and launches a separate job execution for the steps in\nthe flow specified.\n\n[tabs]\n====\nJava::\n+\nThe following example shows an example of a `JobStep` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job jobStepJob(JobRepository jobRepository, Step jobStepJobStep1) {\n\treturn new JobBuilder(\"jobStepJob\", jobRepository)\n .start(jobStepJobStep1)\n .build();\n}\n\n@Bean\npublic Step jobStepJobStep1(JobRepository jobRepository, JobLauncher jobLauncher, Job job, JobParametersExtractor jobParametersExtractor) {\n\treturn new StepBuilder(\"jobStepJobStep1\", jobRepository)\n .job(job)\n .launcher(jobLauncher)\n .parametersExtractor(jobParametersExtractor)\n .build();\n}\n\n@Bean\npublic Job job(JobRepository jobRepository) {\n\treturn new JobBuilder(\"job\", jobRepository)\n // ...\n .build();\n}\n\n@Bean\npublic DefaultJobParametersExtractor jobParametersExtractor() {\n\tDefaultJobParametersExtractor extractor = new DefaultJobParametersExtractor();\n\n\textractor.setKeys(new String[]{\"input.file\"});\n\n\treturn extractor;\n}\n----\n\nXML::\n+\nThe following example hows an example of a `JobStep` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"jobStepJob\" restartable=\"true\">\n <step id=\"jobStepJob.step1\">\n <job ref=\"job\" job-launcher=\"jobLauncher\"\n job-parameters-extractor=\"jobParametersExtractor\"/>\n </step>\n</job>\n\n<job id=\"job\" restartable=\"true\">...</job>\n\n<bean id=\"jobParametersExtractor\" class=\"org.spr...DefaultJobParametersExtractor\">\n <property name=\"keys\" value=\"input.file\"/>\n</bean>\n----\n\n====\n\nThe job parameters extractor is a strategy that determines how the `ExecutionContext` for\nthe `Step` is converted into `JobParameters` for the `Job` that is run. The `JobStep` is\nuseful when you want to have some more granular options for monitoring and reporting on\njobs and steps. Using `JobStep` is also often a good answer to the question: \"`How do I\ncreate dependencies between jobs?`\" It is a good way to break up a large system into\nsmaller modules and control the flow of jobs.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc", "title": "controlling-flow", "heading": "Externalizing Flow Definitions and Dependencies Between Jobs", "heading_level": 2, "file_order": 41, "section_index": 10, "content_hash": "e4c4ac952a835abada46c67ebddbcc0cd8517f0e1aaf0dbb7d762d5dc237ee1f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/controlling-flow.adoc"}}
{"id": "sha256:3faa960998946fc652972b9734450f62dfaf01c69da2d1fe9c138e657dcd4dbc", "content": "[[late-binding]]\n\nBoth the XML and flat file examples shown earlier use the Spring `Resource` abstraction\nto obtain a file. This works because `Resource` has a `getFile` method that returns a\n`java.io.File`. You can configure both XML and flat file resources by using standard Spring\nconstructs:\n\n[tabs]\n====\nJava::\n+\nThe following example shows late binding in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FlatFileItemReader flatFileItemReader() {\n\tFlatFileItemReader<Foo> reader = new FlatFileItemReaderBuilder<Foo>()\n .name(\"flatFileItemReader\")\n .resource(new FileSystemResource(\"file://outputs/file.txt\"))\n ...\n}\n----\n\nXML::\n+\nThe following example shows late binding in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"flatFileItemReader\"\n class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemReader\">\n <property name=\"resource\"\n value=\"file://outputs/file.txt\" />\n</bean>\n----\n\n====\n\nThe preceding `Resource` loads the file from the specified file system location. Note\nthat absolute locations have to start with a double slash (`//`). In most Spring\napplications, this solution is good enough, because the names of these resources are\nknown at compile time. However, in batch scenarios, the file name may need to be\ndetermined at runtime as a parameter to the job. This can be solved using `-D` parameters\nto read a system property.\n\n[tabs]\n====\nJava::\n+\nThe following shows how to read a file name from a property in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FlatFileItemReader flatFileItemReader(@Value(\"${input.file.name}\") String name) {\n\treturn new FlatFileItemReaderBuilder<Foo>()\n .name(\"flatFileItemReader\")\n .resource(new FileSystemResource(name))\n ...\n}\n----\n\nXML::\n+\nThe following example shows how to read a file name from a property in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"flatFileItemReader\"\n class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemReader\">\n <property name=\"resource\" value=\"${input.file.name}\" />\n</bean>\n----\n\n====\n\nAll that would be required for this solution to work would be a system argument (such as\n`-Dinput.file.name=\"file://outputs/file.txt\"`).\n\nNOTE: Although you can use a `PropertyPlaceholderConfigurer` here, it is not\nnecessary if the system property is always set because the `ResourceEditor` in Spring\nalready filters and does placeholder replacement on system properties.\n\nOften, in a batch setting, it is preferable to parameterize the file name in the\n`JobParameters` of the job (instead of through system properties) and access them that\nway. To accomplish this, Spring Batch allows for the late binding of various `Job` and\n`Step` attributes.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to parameterize a file name in Java:\n+\n.Java Configuration\n[source, java]\n----\n@StepScope\n@Bean\npublic FlatFileItemReader flatFileItemReader(@Value(\"#{jobParameters['input.file.name']}\") String name) {\n\treturn new FlatFileItemReaderBuilder<Foo>()\n .name(\"flatFileItemReader\")\n .resource(new FileSystemResource(name))\n ...\n}\n----\n\nXML::\n+\nThe following example shows how to parameterize a file name in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"flatFileItemReader\" scope=\"step\"\n class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemReader\">\n <property name=\"resource\" value=\"#{jobParameters['input.file.name']}\" />\n</bean>\n----\n\n====\n\nYou can access both the `JobExecution` and `StepExecution` level `ExecutionContext` in\nthe same way.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to access the `ExecutionContext` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@StepScope\n@Bean\npublic FlatFileItemReader flatFileItemReader(@Value(\"#{jobExecutionContext['input.file.name']}\") String name) {\n\treturn new FlatFileItemReaderBuilder<Foo>()\n .name(\"flatFileItemReader\")\n .resource(new FileSystemResource(name))\n ...\n}\n----\n+\n.Java Configuration\n[source, java]\n----\n@StepScope\n@Bean\npublic FlatFileItemReader flatFileItemReader(@Value(\"#{stepExecutionContext['input.file.name']}\") String name) {\n\treturn new FlatFileItemReaderBuilder<Foo>()\n .name(\"flatFileItemReader\")\n .resource(new FileSystemResource(name))\n ...\n}\n----\n\nXML::\n+\nThe following example shows how to access the `ExecutionContext` in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"flatFileItemReader\" scope=\"step\"\n class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemReader\">\n <property name=\"resource\" value=\"#{jobExecutionContext['input.file.name']}\" />\n</bean>\n----\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"flatFileItemReader\" scope=\"step\"\n class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemReader\">\n <property name=\"resource\" value=\"#{stepExecutionContext['input.file.name']}\" />\n</bean>\n----\n====\n\nNOTE: Any bean that uses late binding must be declared with `scope=\"step\"`. See\nxref:step/late-binding.adoc#step-scope[Step Scope] for more information.\nA `Step` bean should not be step-scoped or job-scoped. If late binding is needed in a step\ndefinition, then the components of that step (tasklet, item reade/writer, completion policy, and so on)\nare the ones that should be scoped instead.\n\nNOTE: If you use Spring 3.0 (or above), the expressions in step-scoped beans are in the\nSpring Expression Language, a powerful general purpose language with many interesting\nfeatures. To provide backward compatibility, if Spring Batch detects the presence of\nolder versions of Spring, it uses a native expression language that is less powerful and\nthat has slightly different parsing rules. The main difference is that the map keys in\nthe example above do not need to be quoted with Spring 2.5, but the quotes are mandatory\nin Spring 3.0.\n\n[[step-scope]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/late-binding.adoc", "title": "late-binding", "heading": "late-binding", "heading_level": 1, "file_order": 42, "section_index": 0, "content_hash": "3faa960998946fc652972b9734450f62dfaf01c69da2d1fe9c138e657dcd4dbc", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/late-binding.adoc"}}
{"id": "sha256:f031edc16b90dec5388bad769c148748e56a39896574ab35c7ee38e905d33b89", "content": "All of the late binding examples shown earlier have a scope of `step` declared on the\nbean definition.\n\n[tabs]\n====\nJava::\n+\nThe following example shows an example of binding to step scope in Java:\n+\n.Java Configuration\n[source, java]\n----\n@StepScope\n@Bean\npublic FlatFileItemReader flatFileItemReader(@Value(\"#{jobParameters[input.file.name]}\") String name) {\n\treturn new FlatFileItemReaderBuilder<Foo>()\n .name(\"flatFileItemReader\")\n .resource(new FileSystemResource(name))\n ...\n}\n----\n\nXML::\n+\nThe following example shows an example of binding to step scope in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"flatFileItemReader\" scope=\"step\"\n class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemReader\">\n <property name=\"resource\" value=\"#{jobParameters[input.file.name]}\" />\n</bean>\n----\n\n====\n\nUsing a scope of `Step` is required to use late binding, because the bean cannot\nactually be instantiated until the `Step` starts, to let the attributes be found.\nBecause it is not part of the Spring container by default, the scope must be added\nexplicitly, by using the `batch` namespace, by including a bean definition explicitly\nfor the `StepScope`, or by using the `@EnableBatchProcessing` annotation. Use only one of\nthose methods. The following example uses the `batch` namespace:\n\n[source, xml]\n----\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n xmlns:batch=\"http://www.springframework.org/schema/batch\"\n xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n xsi:schemaLocation=\"...\">\n<batch:job .../>\n...\n</beans>\n----\n\nThe following example includes the bean definition explicitly:\n\n[source, xml]\n----\n<bean class=\"org.springframework.batch.core.scope.StepScope\" />\n----\n\n[[job-scope]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/late-binding.adoc", "title": "late-binding", "heading": "Step Scope", "heading_level": 2, "file_order": 42, "section_index": 1, "content_hash": "f031edc16b90dec5388bad769c148748e56a39896574ab35c7ee38e905d33b89", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/late-binding.adoc"}}
{"id": "sha256:645d40aeb7ebb13f413ef8d3fb85c37f84ca9082428fbd9157c0fc6192d46943", "content": "`Job` scope, introduced in Spring Batch 3.0, is similar to `Step` scope in configuration\nbut is a scope for the `Job` context, so that there is only one instance of such a bean\nper running job. Additionally, support is provided for late binding of references\naccessible from the `JobContext` by using `#{..}` placeholders. Using this feature, you can pull bean\nproperties from the job or job execution context and the job parameters.\n\n[tabs]\n====\nJava::\n+\nThe following example shows an example of binding to job scope in Java:\n+\n.Java Configuration\n[source, java]\n----\n@JobScope\n@Bean\npublic FlatFileItemReader flatFileItemReader(@Value(\"#{jobParameters[input]}\") String name) {\n\treturn new FlatFileItemReaderBuilder<Foo>()\n .name(\"flatFileItemReader\")\n .resource(new FileSystemResource(name))\n ...\n}\n----\n+\n.Java Configuration\n[source, java]\n----\n@JobScope\n@Bean\npublic FlatFileItemReader flatFileItemReader(@Value(\"#{jobExecutionContext['input.name']}\") String name) {\n\treturn new FlatFileItemReaderBuilder<Foo>()\n .name(\"flatFileItemReader\")\n .resource(new FileSystemResource(name))\n ...\n}\n----\n\nXML::\n+\nThe following example shows an example of binding to job scope in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"...\" class=\"...\" scope=\"job\">\n <property name=\"name\" value=\"#{jobParameters[input]}\" />\n</bean>\n----\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"...\" class=\"...\" scope=\"job\">\n <property name=\"name\" value=\"#{jobExecutionContext['input.name']}.txt\" />\n</bean>\n----\n\n====\n\nBecause it is not part of the Spring container by default, the scope must be added\nexplicitly, by using the `batch` namespace, by including a bean definition explicitly for\nthe JobScope, or by using the `@EnableBatchProcessing` annotation (choose only one approach).\nThe following example uses the `batch` namespace:\n\n[source, xml]\n----\n<beans xmlns=\"http://www.springframework.org/schema/beans\"\n xmlns:batch=\"http://www.springframework.org/schema/batch\"\n xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n xsi:schemaLocation=\"...\">\n\n<batch:job .../>\n...\n</beans>\n----\n\nThe following example includes a bean that explicitly defines the `JobScope`:\n\n[source, xml]\n----\n<bean class=\"org.springframework.batch.core.scope.JobScope\" />\n----\n\nNOTE: There are some practical limitations of using job-scoped beans in multi-threaded\nor partitioned steps. Spring Batch does not control the threads spawned in these\nuse cases, so it is not possible to set them up correctly to use such beans. Hence,\nwe do not recommend using job-scoped beans in multi-threaded or partitioned steps.\n\n[[scoping-item-streams]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/late-binding.adoc", "title": "late-binding", "heading": "Job Scope", "heading_level": 2, "file_order": 42, "section_index": 2, "content_hash": "645d40aeb7ebb13f413ef8d3fb85c37f84ca9082428fbd9157c0fc6192d46943", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/late-binding.adoc"}}
{"id": "sha256:bf7d34936df9708dd574ea1dd45e5a9d7903d8e35ac545e38369eaba16173442", "content": "When using the Java configuration style to define job or step scoped `ItemStream` beans,\nthe return type of the bean definition method should be at least `ItemStream`. This is required\nso that Spring Batch correctly creates a proxy that implements this interface, and therefore\nhonors its contract by calling `open`, `update` and `close` methods as expected.\n\nIt is recommended to make the bean definition method of such beans return the most specific\nknown implementation, as shown in the following example:\n\n.Define a step-scoped bean with the most specific return type\n[source, java]\n----\n@Bean\n@StepScope\npublic FlatFileItemReader flatFileItemReader(@Value(\"#{jobParameters['input.file.name']}\") String name) {\n\treturn new FlatFileItemReaderBuilder<Foo>()\n .resource(new FileSystemResource(name))\n // set other properties of the item reader\n .build();\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/late-binding.adoc", "title": "late-binding", "heading": "Scoping `ItemStream` components", "heading_level": 2, "file_order": 42, "section_index": 3, "content_hash": "bf7d34936df9708dd574ea1dd45e5a9d7903d8e35ac545e38369eaba16173442", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/late-binding.adoc"}}
{"id": "sha256:61703bb65d826c7b70480e8c2e159baea43ce7eee70445b595a9b43ffd7ec367", "content": "[[taskletStep]]\n\nxref:step/chunk-oriented-processing.adoc[Chunk-oriented processing] is not the only way to process in a\n`Step`. What if a `Step` must consist of a stored procedure call? You could\nimplement the call as an `ItemReader` and return null after the procedure finishes.\nHowever, doing so is a bit unnatural, since there would need to be a no-op `ItemWriter`.\nSpring Batch provides the `TaskletStep` for this scenario.\n\nThe `Tasklet` interface has one method, `execute`, which is called\nrepeatedly by the `TaskletStep` until it either returns `RepeatStatus.FINISHED` or throws\nan exception to signal a failure. Each call to a `Tasklet` is wrapped in a transaction.\n`Tasklet` implementors might call a stored procedure, a script, or a SQL update\nstatement.\n\n[tabs]\n====\nJava::\n+\nTo create a `TaskletStep` in Java, the bean passed to the `tasklet` method of the builder\nshould implement the `Tasklet` interface. No call to `chunk` should be called when\nbuilding a `TaskletStep`. The following example shows a simple tasklet:\n+\n[source, java]\n----\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n return new StepBuilder(\"step1\", jobRepository)\n .tasklet(myTasklet(), transactionManager)\n .build();\n}\n----\n\nXML::\n+\nTo create a `TaskletStep` in XML, the `ref` attribute of the `<tasklet/>` element should\nreference a bean that defines a `Tasklet` object. No `<chunk/>` element should be used\nwithin the `<tasklet/>`. The following example shows a simple tasklet:\n+\n[source, xml]\n----\n<step id=\"step1\">\n <tasklet ref=\"myTasklet\"/>\n</step>\n----\n\n====\n\nNOTE: If it implements the `StepListener` interface, `TaskletStep` automatically registers the tasklet as a `StepListener`.\n\n[[taskletAdapter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/tasklet.adoc", "title": "tasklet", "heading": "tasklet", "heading_level": 1, "file_order": 43, "section_index": 0, "content_hash": "61703bb65d826c7b70480e8c2e159baea43ce7eee70445b595a9b43ffd7ec367", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/tasklet.adoc"}}
{"id": "sha256:4ac2d26ce19bc760fdac7e14854e045fb0656bea42958280742a3699d11a8a24", "content": "As with other adapters for the `ItemReader` and `ItemWriter` interfaces, the `Tasklet`\ninterface contains an implementation that allows for adapting itself to any pre-existing\nclass: `TaskletAdapter`. An example where this may be useful is an existing DAO that is\nused to update a flag on a set of records. You can use the `TaskletAdapter` to call this\nclass without having to write an adapter for the `Tasklet` interface.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to define a `TaskletAdapter` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic MethodInvokingTaskletAdapter myTasklet() {\n\tMethodInvokingTaskletAdapter adapter = new MethodInvokingTaskletAdapter();\n\n\tadapter.setTargetObject(fooDao());\n\tadapter.setTargetMethod(\"updateFoo\");\n\n\treturn adapter;\n}\n----\n\nXML::\n+\nThe following example shows how to define a `TaskletAdapter` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"myTasklet\" class=\"o.s.b.core.step.tasklet.MethodInvokingTaskletAdapter\">\n <property name=\"targetObject\">\n <bean class=\"org.mycompany.FooDao\"/>\n </property>\n <property name=\"targetMethod\" value=\"updateFoo\" />\n</bean>\n----\n\n====\n\n[[exampleTaskletImplementation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/tasklet.adoc", "title": "tasklet", "heading": "`TaskletAdapter`", "heading_level": 2, "file_order": 43, "section_index": 1, "content_hash": "4ac2d26ce19bc760fdac7e14854e045fb0656bea42958280742a3699d11a8a24", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/tasklet.adoc"}}
{"id": "sha256:b50e35ff9147914c5748271af4bbf932896b95c2050a0cd1b1e4baf963c09e5a", "content": "Many batch jobs contain steps that must be done before the main processing begins,\nto set up various resources or after processing has completed to cleanup those\nresources. In the case of a job that works heavily with files, it is often necessary to\ndelete certain files locally after they have been uploaded successfully to another\nlocation. The following example (taken from the\nhttps://github.com/spring-projects/spring-batch/tree/main/spring-batch-samples[Spring\nBatch samples project]) is a `Tasklet` implementation with just such a responsibility:\n\n[source, java]\n----\npublic class FileDeletingTasklet implements Tasklet, InitializingBean {\n\n private Resource directory;\n\n public RepeatStatus execute(StepContribution contribution,\n ChunkContext chunkContext) throws Exception {\n File dir = directory.getFile();\n Assert.state(dir.isDirectory(), \"The resource must be a directory\");\n\n File[] files = dir.listFiles();\n for (int i = 0; i < files.length; i++) {\n boolean deleted = files[i].delete();\n if (!deleted) {\n throw new UnexpectedJobExecutionException(\"Could not delete file \" +\n files[i].getPath());\n }\n }\n return RepeatStatus.FINISHED;\n }\n\n public void setDirectoryResource(Resource directory) {\n this.directory = directory;\n }\n\n public void afterPropertiesSet() throws Exception {\n Assert.state(directory != null, \"Directory must be set\");\n }\n}\n----\n\nThe preceding `tasklet` implementation deletes all files within a given directory. It\nshould be noted that the `execute` method is called only once. All that is left is to\nreference the `tasklet` from the `step`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to reference the `tasklet` from the `step` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job taskletJob(JobRepository jobRepository, Step deleteFilesInDir) {\n\treturn new JobBuilder(\"taskletJob\", jobRepository)\n .start(deleteFilesInDir)\n .build();\n}\n\n@Bean\npublic Step deleteFilesInDir(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"deleteFilesInDir\", jobRepository)\n .tasklet(fileDeletingTasklet(), transactionManager)\n .build();\n}\n\n@Bean\npublic FileDeletingTasklet fileDeletingTasklet() {\n\tFileDeletingTasklet tasklet = new FileDeletingTasklet();\n\n\ttasklet.setDirectoryResource(new FileSystemResource(\"target/test-outputs/test-dir\"));\n\n\treturn tasklet;\n}\n----\n\nXML::\n+\nThe following example shows how to reference the `tasklet` from the `step` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"taskletJob\">\n <step id=\"deleteFilesInDir\">\n <tasklet ref=\"fileDeletingTasklet\"/>\n </step>\n</job>\n\n<beans:bean id=\"fileDeletingTasklet\"\n class=\"org.springframework.batch.samples.tasklet.FileDeletingTasklet\">\n <beans:property name=\"directoryResource\">\n <beans:bean id=\"directory\"\n class=\"org.springframework.core.io.FileSystemResource\">\n <beans:constructor-arg value=\"target/test-outputs/test-dir\" />\n </beans:bean>\n </beans:property>\n</beans:bean>\n----\n\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step/tasklet.adoc", "title": "tasklet", "heading": "Example `Tasklet` Implementation", "heading_level": 2, "file_order": 43, "section_index": 2, "content_hash": "b50e35ff9147914c5748271af4bbf932896b95c2050a0cd1b1e4baf963c09e5a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step/tasklet.adoc"}}
{"id": "sha256:b22adacac35edda5565893ea355af6ab259ed18d4e1d5be983f6afde168c9870", "content": "[[listOfReadersAndWriters]]\n\n[appendix]\n[[list-of-itemreaders-and-itemwriters]]\n\n[[itemReadersAppendix]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/appendix.adoc", "title": "appendix", "heading": "appendix", "heading_level": 1, "file_order": 44, "section_index": 0, "content_hash": "b22adacac35edda5565893ea355af6ab259ed18d4e1d5be983f6afde168c9870", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/appendix.adoc"}}
{"id": "sha256:6090d66a25d44571575d5f69f448ebc15b52043781f06924b6e0eafdaa99b7a8", "content": ".Available Item Readers\n[options=\"header\"]\n|===============\n|Item Reader|Description|Thread-safe\n|`AbstractItemStreamItemReader`|Abstract base class that combines the `ItemStream` and `ItemReader` interfaces.|Yes\n|`AbstractItemCountingItemStreamItemReader`|Abstract base class that provides basic\n restart capabilities by counting the number of items returned from\n an `ItemReader`.|No\n|`AbstractPagingItemReader`|Abstract base class that provides basic paging features|No\n|`AbstractPaginatedDataItemReader`|Abstract base class that provides basic paging features based on Spring Data's\n paginated facilities|No\n|`AggregateItemReader`|An `ItemReader` that delivers a list as its\n item, storing up objects from the injected `ItemReader` until they\n are ready to be packed out as a collection. This class must be used\n as a wrapper for a custom `ItemReader` that can identify the record\n boundaries. The custom reader should mark the beginning and end of\n records by returning an `AggregateItem` which responds `true` to its\n query methods (`isHeader()` and `isFooter()`). Note that this reader\n is not part of the library of readers provided by Spring Batch\n but given as a sample in `spring-batch-samples`.|Yes\n|`AmqpItemReader`|Given a Spring `AmqpTemplate`, it provides\n synchronous receive methods. The `receiveAndConvert()` method\n lets you receive POJO objects.|Yes\n|`KafkaItemReader`|An `ItemReader` that reads messages from an Apache Kafka topic.\nIt can be configured to read messages from multiple partitions of the same topic.\nThis reader stores message offsets in the execution context to support restart capabilities.|No\n|`FlatFileItemReader`|Reads from a flat file. Includes `ItemStream`\n and `Skippable` functionality. See link:readersAndWriters.html#flatFileItemReader[\"`FlatFileItemReader`\"].|No\n|`ItemReaderAdapter`|Adapts any class to the\n `ItemReader` interface.|Yes\n|`JdbcCursorItemReader`|Reads from a database cursor over JDBC. See\n link:readers-and-writers/database.html#cursorBasedItemReaders[\"`Cursor-based ItemReaders`\"].|No\n|`JdbcPagingItemReader`|Given an SQL statement, pages through the rows,\n such that large datasets can be read without running out of\n memory.|Yes\n|`JmsItemReader`|Given a Spring `JmsOperations` object and a JMS\n destination or destination name to which to send errors, provides items\n received through the injected `JmsOperations#receive()`\n method.|Yes\n|`JpaCursorItemReader`|Executes a JPQL query and iterates over the returned result set|No\n|`JpaPagingItemReader`|Given a JPQL query, pages through the\n rows, such that large datasets can be read without running out of\n memory.|Yes\n|`ListItemReader`|Provides the items from a list, one at a time.|No\n|`MongoPagingItemReader`|Given a `MongoOperations` object and a JSON-based MongoDB\n query, provides items received from the `MongoOperations#find()` method.|Yes\n|`MongoCursorItemReader`|Given a `MongoOperations` object and a JSON-based MongoDB\n query, provides items received from the `MongoOperations#stream()` method.|Yes\n|`RepositoryItemReader`|Given a Spring Data `PagingAndSortingRepository` object,\n a `Sort`, and the name of method to execute, returns items provided by the\n Spring Data repository implementation.|Yes\n|`StoredProcedureItemReader`|Reads from a database cursor resulting from the\n execution of a database stored procedure. See link:readersAndWriters.html#StoredProcedureItemReader[`StoredProcedureItemReader`]|No\n|`StaxEventItemReader`|Reads over StAX. see link:readersAndWriters.html#StaxEventItemReader[`StaxEventItemReader`].|No\n|`JsonItemReader`|Reads items from a Json document. see link:readersAndWriters.html#JsonItemReader[`JsonItemReader`].|No\n|`AvroItemReader`|Reads items from a resource containing serialized Avro objects.|No\n|`LdifReader`|Reads items from a LDIF resource and returns them as `LdapAttributes`|No\n|`MappingLdifReader`|Reads items from a LDIF resource and uses a `RecordMapper` to map them to domain objects|No\n\n|===============\n\n[[itemWritersAppendix]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/appendix.adoc", "title": "appendix", "heading": "Item Readers", "heading_level": 2, "file_order": 44, "section_index": 1, "content_hash": "6090d66a25d44571575d5f69f448ebc15b52043781f06924b6e0eafdaa99b7a8", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/appendix.adoc"}}
{"id": "sha256:d0233218ff95162740b6e326db6f9a5de09f27029200292b683e51f869720a54", "content": ".Available Item Writers\n[options=\"header\"]\n|===============\n|Item Writer|Description|Thread-safe\n|`AbstractItemStreamItemWriter`|Abstract base class that combines the`ItemStream` and`ItemWriter` interfaces.|Yes\n|`AmqpItemWriter`|Given a Spring `AmqpTemplate`, provides\n for a synchronous `send` method. The `convertAndSend(Object)`\n method lets you send POJO objects.|Yes\n|`CompositeItemWriter`|Passes an item to the `write` method of each item\n in an injected `List` of `ItemWriter` objects.|Yes\n|`FlatFileItemWriter`|Writes to a flat file. Includes `ItemStream` and\n Skippable functionality. See link:readersAndWriters.html#flatFileItemWriter[\"`FlatFileItemWriter`\"].|No\n|`ItemWriterAdapter`|Adapts any class to the\n `ItemWriter` interface.|Yes\n|`JdbcBatchItemWriter`|Uses batching features from a\n `PreparedStatement`, if available, and can\n take rudimentary steps to locate a failure during a\n `flush`.|Yes\n|`JmsItemWriter`|Using a `JmsOperations` object, items are written\n to the default queue through the `JmsOperations#convertAndSend()` method.|Yes\n|`JpaItemWriter`|This item writer is JPA `EntityManager`-aware\n and handles some transaction-related work that a non-\"`JPA-aware`\"\n `ItemWriter` would not need to know about and\n then delegates to another writer to do the actual writing.|Yes\n|`KafkaItemWriter`|Using a `KafkaTemplate` object, items are written to the default topic through the\n `KafkaTemplate#sendDefault(Object, Object)` method by using a `Converter` to map the key from the item.\n A delete flag can also be configured to send delete events to the topic.|No\n|`MimeMessageItemWriter`|Using Spring's `JavaMailSender`, items of type `MimeMessage`\n are sent as mail messages.|Yes\n|`MongoItemWriter`|Given a `MongoOperations` object, items are written\n through the `MongoOperations.save(Object)` method. The actual write is delayed\n until the last possible moment before the transaction commits.|Yes\n|`PropertyExtractingDelegatingItemWriter`|Extends `AbstractMethodInvokingDelegator`\n creating arguments on the fly. Arguments are created by retrieving\n the values from the fields in the item to be processed (through a\n `SpringBeanWrapper`), based on an injected array of field\n names.|Yes\n|`RepositoryItemWriter`|Given a Spring Data `CrudRepository` implementation,\n items are saved through the method specified in the configuration.|Yes\n|`StaxEventItemWriter`|Uses a `Marshaller` implementation to\n convert each item to XML and then writes it to an XML file by using\n StAX.|No\n|`JsonFileItemWriter`|Uses a `JsonObjectMarshaller` implementation to\n convert each item to Json and then writes it to a Json file.|No\n|`AvroItemWriter`|Serializes data to an `WritableResource` using Avro|No\n|`ListItemWriter`|Item writer that writes items to a `List`.|No\n\n|===============", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/appendix.adoc", "title": "appendix", "heading": "Item Writers", "heading_level": 2, "file_order": 44, "section_index": 2, "content_hash": "d0233218ff95162740b6e326db6f9a5de09f27029200292b683e51f869720a54", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/appendix.adoc"}}
{"id": "sha256:42bf134b91d646af1b59fcebe51506e063cf8659672f21fca42e95e3a5beb485", "content": "[[commonPatterns]]\n\n[[common-batch-patterns]]\n\nSome batch jobs can be assembled purely from off-the-shelf components in Spring Batch.\nFor instance, the `ItemReader` and `ItemWriter` implementations can be configured to\ncover a wide range of scenarios. However, for the majority of cases, custom code must be\nwritten. The main API entry points for application developers are the `Tasklet`, the\n`ItemReader`, the `ItemWriter`, and the various listener interfaces. Most simple batch\njobs can use off-the-shelf input from a Spring Batch `ItemReader`, but it is often the\ncase that there are custom concerns in the processing and writing that require developers\nto implement an `ItemWriter` or `ItemProcessor`.\n\nIn this chapter, we provide a few examples of common patterns in custom business logic.\nThese examples primarily feature the listener interfaces. It should be noted that an\n`ItemReader` or `ItemWriter` can implement a listener interface as well, if appropriate.\n\n[[loggingItemProcessingAndFailures]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "common-patterns", "heading_level": 1, "file_order": 46, "section_index": 0, "content_hash": "42bf134b91d646af1b59fcebe51506e063cf8659672f21fca42e95e3a5beb485", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:828ac2888025633e3d6cc316983163702d5b4cb4f18259f206cd158403d8f425", "content": "A common use case is the need for special handling of errors in a step, item by item,\nperhaps logging to a special channel or inserting a record into a database. A\nchunk-oriented `Step` (created from the step factory beans) lets users implement this use\ncase with a simple `ItemReadListener` for errors on `read` and an `ItemWriteListener` for\nerrors on `write`. The following code snippet illustrates a listener that logs both read\nand write failures:\n\n[source, java]\n----\npublic class ItemFailureLoggerListener extends ItemListenerSupport {\n\n private static Log logger = LogFactory.getLog(\"item.error\");\n\n public void onReadError(Exception ex) {\n logger.error(\"Encountered error on read\", e);\n }\n\n public void onWriteError(Exception ex, List<? extends Object> items) {\n logger.error(\"Encountered error on write\", ex);\n }\n}\n----\n\nHaving implemented this listener, it must be registered with a step.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to register a listener with a step Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step simpleStep(JobRepository jobRepository) {\n\treturn new StepBuilder(\"simpleStep\", jobRepository)\n ...\n .listener(new ItemFailureLoggerListener())\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to register a listener with a step in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<step id=\"simpleStep\">\n...\n<listeners>\n <listener>\n <bean class=\"org.example...ItemFailureLoggerListener\"/>\n </listener>\n</listeners>\n</step>\n----\n\n====\n\nIMPORTANT: if your listener does anything in an `onError()` method, it must be inside\na transaction that is going to be rolled back. If you need to use a transactional\nresource, such as a database, inside an `onError()` method, consider adding a declarative\ntransaction to that method (see Spring Core Reference Guide for details), and giving its\npropagation attribute a value of `REQUIRES_NEW`.\n\n[[stoppingAJobManuallyForBusinessReasons]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "Logging Item Processing and Failures", "heading_level": 2, "file_order": 46, "section_index": 1, "content_hash": "828ac2888025633e3d6cc316983163702d5b4cb4f18259f206cd158403d8f425", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:3991d4d4811a43b3ea13c8e0a8badd2de3d790c75d9723f1ada278d5847fddfa", "content": "Spring Batch provides a `stop()` method through the `JobOperator` interface, but this is\nreally for use by the operator rather than the application programmer. Sometimes, it is\nmore convenient or makes more sense to stop a job execution from within the business\nlogic.\n\nThe simplest thing to do is to throw a `RuntimeException` (one that is neither retried\nindefinitely nor skipped). For example, a custom exception type could be used, as shown\nin the following example:\n\n[source, java]\n----\npublic class PoisonPillItemProcessor<T> implements ItemProcessor<T, T> {\n\n @Override\n public T process(T item) throws Exception {\n if (isPoisonPill(item)) {\n throw new PoisonPillException(\"Poison pill detected: \" + item);\n }\n return item;\n }\n}\n----\n\nAnother simple way to stop a step from executing is to return `null` from the\n`ItemReader`, as shown in the following example:\n\n[source, java]\n----\npublic class EarlyCompletionItemReader implements ItemReader<T> {\n\n private ItemReader<T> delegate;\n\n public void setDelegate(ItemReader<T> delegate) { ... }\n\n public T read() throws Exception {\n T item = delegate.read();\n if (isEndItem(item)) {\n return null; // end the step here\n }\n return item;\n }\n\n}\n----\nThe previous example actually relies on the fact that there is a default implementation\nof the `CompletionPolicy` strategy that signals a complete batch when the item to be\nprocessed is `null`. A more sophisticated completion policy could be implemented and\ninjected into the `Step` through the `SimpleStepFactoryBean`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to inject a completion policy into a step in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step simpleStep(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"simpleStep\", jobRepository)\n .<String, String>chunk(new SpecialCompletionPolicy(), transactionManager)\n .reader(reader())\n .writer(writer())\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to inject a completion policy into a step in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<step id=\"simpleStep\">\n <tasklet>\n <chunk reader=\"reader\" writer=\"writer\" commit-interval=\"10\"\n chunk-completion-policy=\"completionPolicy\"/>\n </tasklet>\n</step>\n\n<bean id=\"completionPolicy\" class=\"org.example...SpecialCompletionPolicy\"/>\n----\n\n====\n\nAn alternative is to set a flag in the `StepExecution`, which is checked by the `Step`\nimplementations in the framework in between item processing. To implement this\nalternative, we need access to the current `StepExecution`, and this can be achieved by\nimplementing a `StepListener` and registering it with the `Step`. The following example\nshows a listener that sets the flag:\n\n[source, java]\n----\npublic class CustomItemWriter extends ItemListenerSupport implements StepListener {\n\n private StepExecution stepExecution;\n\n public void beforeStep(StepExecution stepExecution) {\n this.stepExecution = stepExecution;\n }\n\n public void afterRead(Object item) {\n if (isPoisonPill(item)) {\n stepExecution.setTerminateOnly();\n }\n }\n\n}\n----\n\nWhen the flag is set, the default behavior is for the step to throw a\n`JobInterruptedException`. This behavior can be controlled through the\n`StepInterruptionPolicy`. However, the only choice is to throw or not throw an exception,\nso this is always an abnormal ending to a job.\n\n[[addingAFooterRecord]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "Stopping a Job Manually for Business Reasons", "heading_level": 2, "file_order": 46, "section_index": 2, "content_hash": "3991d4d4811a43b3ea13c8e0a8badd2de3d790c75d9723f1ada278d5847fddfa", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:6cc10a55084a64a8771077af8ac6b8e7b7e07d2c3fe86efc88e6526d2009a0b4", "content": "Often, when writing to flat files, a \"`footer`\" record must be appended to the end of the\nfile, after all processing has be completed. This can be achieved using the\n`FlatFileFooterCallback` interface provided by Spring Batch. The `FlatFileFooterCallback`\n(and its counterpart, the `FlatFileHeaderCallback`) are optional properties of the\n`FlatFileItemWriter` and can be added to an item writer.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to use the `FlatFileHeaderCallback` and the\n`FlatFileFooterCallback` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic FlatFileItemWriter<String> itemWriter(Resource outputResource) {\n\treturn new FlatFileItemWriterBuilder<String>()\n .name(\"itemWriter\")\n .resource(outputResource)\n .lineAggregator(lineAggregator())\n .headerCallback(headerCallback())\n .footerCallback(footerCallback())\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to use the `FlatFileHeaderCallback` and the\n`FlatFileFooterCallback` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"itemWriter\" class=\"org.spr...FlatFileItemWriter\">\n <property name=\"resource\" ref=\"outputResource\" />\n <property name=\"lineAggregator\" ref=\"lineAggregator\"/>\n <property name=\"headerCallback\" ref=\"headerCallback\" />\n <property name=\"footerCallback\" ref=\"footerCallback\" />\n</bean>\n----\n\n====\n\nThe footer callback interface has just one method that is called when the footer must be\nwritten, as shown in the following interface definition:\n\n[source, java]\n----\npublic interface FlatFileFooterCallback {\n\n void writeFooter(Writer writer) throws IOException;\n\n}\n----\n\n[[writingASummaryFooter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "Adding a Footer Record", "heading_level": 2, "file_order": 46, "section_index": 3, "content_hash": "6cc10a55084a64a8771077af8ac6b8e7b7e07d2c3fe86efc88e6526d2009a0b4", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:16a9b9a7d1a901741cbe4fd5848db55be043948b09203b60209e1afd309ef5c8", "content": "A common requirement involving footer records is to aggregate information during the\noutput process and to append this information to the end of the file. This footer often\nserves as a summarization of the file or provides a checksum.\n\nFor example, if a batch job is writing `Trade` records to a flat file, and there is a\nrequirement that the total amount from all the `Trades` is placed in a footer, then the\nfollowing `ItemWriter` implementation can be used:\n\n[source, java]\n----\npublic class TradeItemWriter implements ItemWriter<Trade>,\n FlatFileFooterCallback {\n\n private ItemWriter<Trade> delegate;\n\n private BigDecimal totalAmount = BigDecimal.ZERO;\n\n public void write(Chunk<? extends Trade> items) throws Exception {\n BigDecimal chunkTotal = BigDecimal.ZERO;\n for (Trade trade : items) {\n chunkTotal = chunkTotal.add(trade.getAmount());\n }\n\n delegate.write(items);\n\n // After successfully writing all items\n totalAmount = totalAmount.add(chunkTotal);\n }\n\n public void writeFooter(Writer writer) throws IOException {\n writer.write(\"Total Amount Processed: \" + totalAmount);\n }\n\n public void setDelegate(ItemWriter delegate) {...}\n}\n----\n\nThis `TradeItemWriter` stores a `totalAmount` value that is increased with the `amount`\nfrom each `Trade` item written. After the last `Trade` is processed, the framework calls\n`writeFooter`, which puts the `totalAmount` into the file. Note that the `write` method\nmakes use of a temporary variable, `chunkTotal`, that stores the total of the\n`Trade` amounts in the chunk. This is done to ensure that, if a skip occurs in the\n`write` method, the `totalAmount` is left unchanged. It is only at the end of the `write`\nmethod, once we are guaranteed that no exceptions are thrown, that we update the\n`totalAmount`.\n\nIn order for the `writeFooter` method to be called, the `TradeItemWriter` (which\nimplements `FlatFileFooterCallback`) must be wired into the `FlatFileItemWriter` as the\n`footerCallback`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to wire the `TradeItemWriter` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic TradeItemWriter tradeItemWriter() {\n\tTradeItemWriter itemWriter = new TradeItemWriter();\n\n\titemWriter.setDelegate(flatFileItemWriter(null));\n\n\treturn itemWriter;\n}\n\n@Bean\npublic FlatFileItemWriter<String> flatFileItemWriter(Resource outputResource) {\n\treturn new FlatFileItemWriterBuilder<String>()\n .name(\"itemWriter\")\n .resource(outputResource)\n .lineAggregator(lineAggregator())\n .footerCallback(tradeItemWriter())\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to wire the `TradeItemWriter` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"tradeItemWriter\" class=\"..TradeItemWriter\">\n <property name=\"delegate\" ref=\"flatFileItemWriter\" />\n</bean>\n\n<bean id=\"flatFileItemWriter\" class=\"org.spr...FlatFileItemWriter\">\n <property name=\"resource\" ref=\"outputResource\" />\n <property name=\"lineAggregator\" ref=\"lineAggregator\"/>\n <property name=\"footerCallback\" ref=\"tradeItemWriter\" />\n</bean>\n----\n\n====\n\nThe way that the `TradeItemWriter` has been written so far functions correctly only if\nthe `Step` is not restartable. This is because the class is stateful (since it stores the\n`totalAmount`), but the `totalAmount` is not persisted to the database. Therefore, it\ncannot be retrieved in the event of a restart. In order to make this class restartable,\nthe `ItemStream` interface should be implemented along with the methods `open` and\n`update`, as shown in the following example:\n\n[source, java]\n----\npublic void open(ExecutionContext executionContext) {\n if (executionContext.containsKey(\"total.amount\") {\n totalAmount = (BigDecimal) executionContext.get(\"total.amount\");\n }\n}\n\npublic void update(ExecutionContext executionContext) {\n executionContext.put(\"total.amount\", totalAmount);\n}\n----\n\nThe update method stores the most current version of `totalAmount` to the\n`ExecutionContext` just before that object is persisted to the database. The open method\nretrieves any existing `totalAmount` from the `ExecutionContext` and uses it as the\nstarting point for processing, allowing the `TradeItemWriter` to pick up on restart where\nit left off the previous time the `Step` was run.\n\n[[drivingQueryBasedItemReaders]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "Writing a Summary Footer", "heading_level": 3, "file_order": 46, "section_index": 4, "content_hash": "16a9b9a7d1a901741cbe4fd5848db55be043948b09203b60209e1afd309ef5c8", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:f438f546f323a98717369514083d9fce4ecc9008e49650abcfb1e00c0e031d42", "content": "In the link:readersAndWriters.html[chapter on readers and writers], database input using\npaging was discussed. Many database vendors, such as DB2, have extremely pessimistic\nlocking strategies that can cause issues if the table being read also needs to be used by\nother portions of the online application. Furthermore, opening cursors over extremely\nlarge datasets can cause issues on databases from certain vendors. Therefore, many\nprojects prefer to use a 'Driving Query' approach to reading in data. This approach works\nby iterating over keys, rather than the entire object that needs to be returned, as the\nfollowing image illustrates:\n\n.Driving Query Job\nimage::drivingQueryExample.png[Driving Query Job, scaledwidth=\"60%\"]\n\nAs you can see, the example shown in the preceding image uses the same 'FOO' table as was\nused in the cursor-based example. However, rather than selecting the entire row, only the\nIDs were selected in the SQL statement. So, rather than a `FOO` object being returned\nfrom `read`, an `Integer` is returned. This number can then be used to query for the\n'details', which is a complete `Foo` object, as shown in the following image:\n\n.Driving Query Example\nimage::drivingQueryJob.png[Driving Query Example, scaledwidth=\"60%\"]\n\nAn `ItemProcessor` should be used to transform the key obtained from the driving query\ninto a full `Foo` object. An existing DAO can be used to query for the full object based\non the key.\n\n[[multiLineRecords]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "Driving Query Based ItemReaders", "heading_level": 2, "file_order": 46, "section_index": 5, "content_hash": "f438f546f323a98717369514083d9fce4ecc9008e49650abcfb1e00c0e031d42", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:d7a516a2f58e33d410e320a4e6e0a57ddf14d2cb1f63f4c82ca4ad670c8905e4", "content": "While it is usually the case with flat files that each record is confined to a single\nline, it is common that a file might have records spanning multiple lines with multiple\nformats. The following excerpt from a file shows an example of such an arrangement:\n\n----\nHEA;0013100345;2007-02-15\nNCU;Smith;Peter;;T;20014539;F\nBAD;;Oak Street 31/A;;Small Town;00235;IL;US\nFOT;2;2;267.34\n----\nEverything between the line starting with 'HEA' and the line starting with 'FOT' is\nconsidered one record. There are a few considerations that must be made in order to\nhandle this situation correctly:\n\n* Instead of reading one record at a time, the `ItemReader` must read every line of the\nmulti-line record as a group, so that it can be passed to the `ItemWriter` intact.\n* Each line type may need to be tokenized differently.\n\nBecause a single record spans multiple lines and because we may not know how many lines\nthere are, the `ItemReader` must be careful to always read an entire record. In order to\ndo this, a custom `ItemReader` should be implemented as a wrapper for the\n`FlatFileItemReader`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to implement a custom `ItemReader` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic MultiLineTradeItemReader itemReader() {\n\tMultiLineTradeItemReader itemReader = new MultiLineTradeItemReader();\n\n\titemReader.setDelegate(flatFileItemReader());\n\n\treturn itemReader;\n}\n\n@Bean\npublic FlatFileItemReader flatFileItemReader() {\n\tFlatFileItemReader<Trade> reader = new FlatFileItemReaderBuilder<>()\n .name(\"flatFileItemReader\")\n .resource(new ClassPathResource(\"data/iosample/input/multiLine.txt\"))\n .lineTokenizer(orderFileTokenizer())\n .fieldSetMapper(orderFieldSetMapper())\n .build();\n\treturn reader;\n}\n----\n\nXML::\n+\nThe following example shows how to implement a custom `ItemReader` in XML:\n+\n.XML Configuration\n[source,xml]\n----\n<bean id=\"itemReader\" class=\"org.spr...MultiLineTradeItemReader\">\n <property name=\"delegate\">\n <bean class=\"org.springframework.batch.infrastructure.item.file.FlatFileItemReader\">\n <property name=\"resource\" value=\"data/iosample/input/multiLine.txt\" />\n <property name=\"lineMapper\">\n <bean class=\"org.spr...DefaultLineMapper\">\n <property name=\"lineTokenizer\" ref=\"orderFileTokenizer\"/>\n <property name=\"fieldSetMapper\" ref=\"orderFieldSetMapper\"/>\n </bean>\n </property>\n </bean>\n </property>\n</bean>\n----\n\n====\n\nTo ensure that each line is tokenized properly, which is especially important for\nfixed-length input, the `PatternMatchingCompositeLineTokenizer` can be used on the\ndelegate `FlatFileItemReader`. See\nlink:readersAndWriters.html#flatFileItemReader[`FlatFileItemReader` in the Readers and\nWriters chapter] for more details. The delegate reader then uses a\n`PassThroughFieldSetMapper` to deliver a `FieldSet` for each line back to the wrapping\n`ItemReader`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to ensure that each line is properly tokenized in Java:\n+\n.Java Content\n[source, java]\n----\n@Bean\npublic PatternMatchingCompositeLineTokenizer orderFileTokenizer() {\n\tPatternMatchingCompositeLineTokenizer tokenizer =\n new PatternMatchingCompositeLineTokenizer();\n\n\tMap<String, LineTokenizer> tokenizers = new HashMap<>(4);\n\n\ttokenizers.put(\"HEA*\", headerRecordTokenizer());\n\ttokenizers.put(\"FOT*\", footerRecordTokenizer());\n\ttokenizers.put(\"NCU*\", customerLineTokenizer());\n\ttokenizers.put(\"BAD*\", billingAddressLineTokenizer());\n\n\ttokenizer.setTokenizers(tokenizers);\n\n\treturn tokenizer;\n}\n----\n\nXML::\n+\nThe following example shows how to ensure that each line is properly tokenized in XML:\n+\n.XML Content\n[source, xml]\n----\n<bean id=\"orderFileTokenizer\" class=\"org.spr...PatternMatchingCompositeLineTokenizer\">\n <property name=\"tokenizers\">\n <map>\n <entry key=\"HEA*\" value-ref=\"headerRecordTokenizer\" />\n <entry key=\"FOT*\" value-ref=\"footerRecordTokenizer\" />\n <entry key=\"NCU*\" value-ref=\"customerLineTokenizer\" />\n <entry key=\"BAD*\" value-ref=\"billingAddressLineTokenizer\" />\n </map>\n </property>\n</bean>\n----\n====\n\nThis wrapper has to be able to recognize the end of a record so that it can continually\ncall `read()` on its delegate until the end is reached. For each line that is read, the\nwrapper should build up the item to be returned. Once the footer is reached, the item can\nbe returned for delivery to the `ItemProcessor` and `ItemWriter`, as shown in the\nfollowing example:\n\n[source, java]\n----\nprivate FlatFileItemReader<FieldSet> delegate;\n\npublic Trade read() throws Exception {\n Trade t = null;\n\n for (FieldSet line = null; (line = this.delegate.read()) != null;) {\n String prefix = line.readString(0);\n if (prefix.equals(\"HEA\")) {\n t = new Trade(); // Record must start with header\n }\n else if (prefix.equals(\"NCU\")) {\n Assert.notNull(t, \"No header was found.\");\n t.setLast(line.readString(1));\n t.setFirst(line.readString(2));\n ...\n }\n else if (prefix.equals(\"BAD\")) {\n Assert.notNull(t, \"No header was found.\");\n t.setCity(line.readString(4));\n t.setState(line.readString(6));\n ...\n }\n else if (prefix.equals(\"FOT\")) {\n return t; // Record must end with footer\n }\n }\n Assert.isNull(t, \"No 'END' was found.\");\n return null;\n}\n----\n\n[[executingSystemCommands]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "Multi-Line Records", "heading_level": 2, "file_order": 46, "section_index": 6, "content_hash": "d7a516a2f58e33d410e320a4e6e0a57ddf14d2cb1f63f4c82ca4ad670c8905e4", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:c3adcfd9ae4f6ec1d488dc3f499037b9eeb63de0a6cf8e52bae24efc1c0de125", "content": "Many batch jobs require that an external command be called from within the batch job.\nSuch a process could be kicked off separately by the scheduler, but the advantage of\ncommon metadata about the run would be lost. Furthermore, a multi-step job would also\nneed to be split up into multiple jobs as well.\n\nBecause the need is so common, Spring Batch provides a `Tasklet` implementation for\ncalling system commands.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to call an external command in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic SystemCommandTasklet tasklet() {\n\tSystemCommandTasklet tasklet = new SystemCommandTasklet();\n\n\ttasklet.setCommand(\"echo hello\");\n\ttasklet.setTimeout(5000);\n\n\treturn tasklet;\n}\n----\n\nXML::\n+\nThe following example shows how to call an external command in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean class=\"org.springframework.batch.core.step.tasklet.SystemCommandTasklet\">\n <property name=\"command\" value=\"echo hello\" />\n <!-- 5 second timeout for the command to complete -->\n <property name=\"timeout\" value=\"5000\" />\n</bean>\n----\n====\n\n[[handlingStepCompletionWhenNoInputIsFound]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "Executing System Commands", "heading_level": 2, "file_order": 46, "section_index": 7, "content_hash": "c3adcfd9ae4f6ec1d488dc3f499037b9eeb63de0a6cf8e52bae24efc1c0de125", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:fc2c696b534ac9059d0f2c034e15922853224e878bbd9fefa972ccacf6d9a04c", "content": "In many batch scenarios, finding no rows in a database or file to process is not\nexceptional. The `Step` is simply considered to have found no work and completes with 0\nitems read. All of the `ItemReader` implementations provided out of the box in Spring\nBatch default to this approach. This can lead to some confusion if nothing is written out\neven when input is present (which usually happens if a file was misnamed or some similar\nissue arises). For this reason, the metadata itself should be inspected to determine how\nmuch work the framework found to be processed. However, what if finding no input is\nconsidered exceptional? In this case, programmatically checking the metadata for no items\nprocessed and causing failure is the best solution. Because this is a common use case,\nSpring Batch provides a listener with exactly this functionality, as shown in\nthe class definition for `NoWorkFoundStepExecutionListener`:\n\n[source, java]\n----\npublic class NoWorkFoundStepExecutionListener implements StepExecutionListener {\n\n public ExitStatus afterStep(StepExecution stepExecution) {\n if (stepExecution.getReadCount() == 0) {\n return ExitStatus.FAILED;\n }\n return null;\n }\n\n}\n----\n\nThe preceding `StepExecutionListener` inspects the `readCount` property of the\n`StepExecution` during the 'afterStep' phase to determine if no items were read. If that\nis the case, an exit code `FAILED` is returned, indicating that the `Step` should fail.\nOtherwise, `null` is returned, which does not affect the status of the `Step`.\n\n[[passingDataToFutureSteps]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "Handling Step Completion When No Input is Found", "heading_level": 2, "file_order": 46, "section_index": 8, "content_hash": "fc2c696b534ac9059d0f2c034e15922853224e878bbd9fefa972ccacf6d9a04c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:ad1db0b21a80520c965147995e81cf7083dc8be01d885539f9881629eec8ddb2", "content": "It is often useful to pass information from one step to another. This can be done through\nthe `ExecutionContext`. The catch is that there are two `ExecutionContexts`: one at the\n`Step` level and one at the `Job` level. The `Step` `ExecutionContext` remains only as\nlong as the step, while the `Job` `ExecutionContext` remains through the whole `Job`. On\nthe other hand, the `Step` `ExecutionContext` is updated every time the `Step` commits a\nchunk, while the `Job` `ExecutionContext` is updated only at the end of each `Step`.\n\nThe consequence of this separation is that all data must be placed in the `Step`\n`ExecutionContext` while the `Step` is executing. Doing so ensures that the data is\nstored properly while the `Step` runs. If data is stored to the `Job` `ExecutionContext`,\nthen it is not persisted during `Step` execution. If the `Step` fails, that data is lost.\n\n[source, java]\n----\npublic class SavingItemWriter implements ItemWriter<Object> {\n private StepExecution stepExecution;\n\n public void write(Chunk<? extends Object> items) throws Exception {\n // ...\n\n ExecutionContext stepContext = this.stepExecution.getExecutionContext();\n stepContext.put(\"someKey\", someObject);\n }\n\n @BeforeStep\n public void saveStepExecution(StepExecution stepExecution) {\n this.stepExecution = stepExecution;\n }\n}\n----\n\nTo make the data available to future `Steps`, it must be \"`promoted`\" to the `Job`\n`ExecutionContext` after the step has finished. Spring Batch provides the\n`ExecutionContextPromotionListener` for this purpose. The listener must be configured\nwith the keys related to the data in the `ExecutionContext` that must be promoted. It can\nalso, optionally, be configured with a list of exit code patterns for which the promotion\nshould occur (`COMPLETED` is the default). As with all listeners, it must be registered\non the `Step`.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to promote a step to the `Job` `ExecutionContext` in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job job1(JobRepository jobRepository, Step step1, Step step2) {\n\treturn new JobBuilder(\"job1\", jobRepository)\n .start(step1)\n .next(step2)\n .build();\n}\n\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(reader())\n .writer(savingWriter())\n .listener(promotionListener())\n .build();\n}\n\n@Bean\npublic ExecutionContextPromotionListener promotionListener() {\n\tExecutionContextPromotionListener listener = new ExecutionContextPromotionListener();\n\n\tlistener.setKeys(new String[] {\"someKey\"});\n\n\treturn listener;\n}\n----\n\nXML::\n+\nThe following example shows how to promote a step to the `Job` `ExecutionContext` in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"job1\">\n <step id=\"step1\">\n <tasklet>\n <chunk reader=\"reader\" writer=\"savingWriter\" commit-interval=\"10\"/>\n </tasklet>\n <listeners>\n <listener ref=\"promotionListener\"/>\n </listeners>\n </step>\n\n <step id=\"step2\">\n ...\n </step>\n</job>\n\n<beans:bean id=\"promotionListener\" class=\"org.spr....ExecutionContextPromotionListener\">\n <beans:property name=\"keys\">\n <list>\n <value>someKey</value>\n </list>\n </beans:property>\n</beans:bean>\n----\n\n====\n\nFinally, the saved values must be retrieved from the `Job` `ExecutionContext`, as shown\nin the following example:\n\n[source, java]\n----\npublic class RetrievingItemWriter implements ItemWriter<Object> {\n private Object someObject;\n\n public void write(Chunk<? extends Object> items) throws Exception {\n // ...\n }\n\n @BeforeStep\n public void retrieveInterstepData(StepExecution stepExecution) {\n JobExecution jobExecution = stepExecution.getJobExecution();\n ExecutionContext jobContext = jobExecution.getExecutionContext();\n this.someObject = jobContext.get(\"someKey\");\n }\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/common-patterns.adoc", "title": "common-patterns", "heading": "Passing Data to Future Steps", "heading_level": 2, "file_order": 46, "section_index": 9, "content_hash": "ad1db0b21a80520c965147995e81cf7083dc8be01d885539f9881629eec8ddb2", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/common-patterns.adoc"}}
{"id": "sha256:156a1c530a47e031e8e5bd502417acc422796210ef1888e2d4c8710156a04ca1", "content": "[[domainLanguageOfBatch]]\n\nTo any experienced batch architect, the overall concepts of batch processing used in\nSpring Batch should be familiar and comfortable. There are \"`Jobs`\" and \"`Steps`\" and\ndeveloper-supplied processing units called `ItemReader` and `ItemWriter`. However,\nbecause of the Spring patterns, operations, templates, callbacks, and idioms, there are\nopportunities for the following:\n\n* Significant improvement in adherence to a clear separation of concerns.\n* Clearly delineated architectural layers and services provided as interfaces.\n* Simple and default implementations that allow for quick adoption and ease of use\nout of the box.\n* Significantly enhanced extensibility.\n\nThe following diagram is a simplified version of the batch reference architecture that\nhas been used for decades. It provides an overview of the components that make up the\ndomain language of batch processing. This architecture framework is a blueprint that has\nbeen proven through decades of implementations on the last several generations of\nplatforms (COBOL on mainframes, C++ on Unix, and now Java anywhere). JCL and COBOL developers\nare likely to be as comfortable with the concepts as C++, C#, and Java developers. Spring\nBatch provides a physical implementation of the layers, components, and technical\nservices commonly found in the robust, maintainable systems that are used to address the\ncreation of simple to complex batch applications, with the infrastructure and extensions\nto address very complex processing needs.\n\n.Batch Stereotypes\nimage::spring-batch-reference-model.png[Figure 2.1: Batch Stereotypes, scaledwidth=\"60%\"]\n\nThe preceding diagram highlights the key concepts that make up the domain language of\nSpring Batch. A `Job` has one or more steps, each of which has exactly one `ItemReader`,\nan optional `ItemProcessor`, and one `ItemWriter`. A job is operated (started, stopped, etc)\nwith a `JobOperator`, and metadata about the currently running process is stored in and\nrestored from a `JobRepository`.\n\n[[job]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "domain", "heading_level": 1, "file_order": 47, "section_index": 0, "content_hash": "156a1c530a47e031e8e5bd502417acc422796210ef1888e2d4c8710156a04ca1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:866afba4a6d755e385fe5cfc3454ee798f12da1bc3fb0e260e51a1c35fadd48f", "content": "This section describes stereotypes relating to the concept of a batch job. A `Job` is an\nentity that encapsulates an entire batch process. As is common with other Spring\nprojects, a `Job` is wired together with either an XML configuration file or Java-based\nconfiguration. This configuration may be referred to as the \"`job configuration`\". However,\n`Job` is only the top of an overall hierarchy, as shown in the following diagram:\n\n.Job Hierarchy\nimage::job-heirarchy.png[Job Hierarchy, scaledwidth=\"60%\"]\n\nIn Spring Batch, a `Job` is simply a container for `Step` instances. It combines multiple\nsteps that logically belong together in a flow and allows for configuration of properties\nglobal to all steps, such as restartability. The job configuration contains:\n\n* The name of the job.\n* Definition and ordering of `Step` instances.\n* Whether or not the job is restartable.\n\n[tabs]\n====\nJava::\n+\nFor those who use Java configuration, Spring Batch provides a default implementation of\nthe `Job` interface in the form of the `SimpleJob` class, which creates some standard\nfunctionality on top of `Job`. When using Java-based configuration, a collection of\nbuilders is made available for the instantiation of a `Job`, as the following\nexample shows:\n+\n[source, java]\n----\n@Bean\npublic Job footballJob(JobRepository jobRepository) {\n return new JobBuilder(\"footballJob\", jobRepository)\n .start(playerLoad())\n .next(gameLoad())\n .next(playerSummarization())\n .build();\n}\n----\n\nXML::\n+\nFor those who use XML configuration, Spring Batch provides a default implementation of the\n`Job` interface in the form of the `SimpleJob` class, which creates some standard\nfunctionality on top of `Job`. However, the batch namespace abstracts away the need to\ninstantiate it directly. Instead, you can use the `<job>` element, as the\nfollowing example shows:\n+\n[source, xml]\n----\n<job id=\"footballJob\">\n <step id=\"playerload\" next=\"gameLoad\"/>\n <step id=\"gameLoad\" next=\"playerSummarization\"/>\n <step id=\"playerSummarization\"/>\n</job>\n----\n\n====\n\n[[jobinstance]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "Job", "heading_level": 2, "file_order": 47, "section_index": 1, "content_hash": "866afba4a6d755e385fe5cfc3454ee798f12da1bc3fb0e260e51a1c35fadd48f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:f2edc0b49870596397bb5f299c177d19d47c3205ba760665856f4efe93016823", "content": "A `JobInstance` refers to the concept of a logical job run. Consider a batch job that\nshould be run once at the end of the day, such as the `EndOfDay` `Job` from the preceding\ndiagram. There is one `EndOfDay` job, but each individual run of the `Job` must be\ntracked separately. In the case of this job, there is one logical `JobInstance` per day.\nFor example, there is a January 1st run, a January 2nd run, and so on. If the January 1st\nrun fails the first time and is run again the next day, it is still the January 1st run.\n(Usually, this corresponds with the data it is processing as well, meaning the January\n1st run processes data for January 1st). Therefore, each `JobInstance` can have multiple\nexecutions (`JobExecution` is discussed in more detail later in this chapter), and only\none `JobInstance` (which corresponds to a particular `Job` and identifying `JobParameters`) can\nrun at a given time.\n\nThe definition of a `JobInstance` has absolutely no bearing on the data to be loaded.\nIt is entirely up to the `ItemReader` implementation to determine how data is loaded. For\nexample, in the `EndOfDay` scenario, there may be a column on the data that indicates the\n`effective date` or `schedule date` to which the data belongs. So, the January 1st run\nwould load only data from the 1st, and the January 2nd run would use only data from the\n2nd. Because this determination is likely to be a business decision, it is left up to the\n`ItemReader` to decide. However, using the same `JobInstance` determines whether or not\nthe \"`state`\" (that is, the `ExecutionContext`, which is discussed later in this chapter)\nfrom previous executions is used. Using a new `JobInstance` means \"`start from the\nbeginning,`\" and using an existing instance generally means \"`start from where you left\noff`\".\n\n[[jobParameters]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "JobInstance", "heading_level": 3, "file_order": 47, "section_index": 2, "content_hash": "f2edc0b49870596397bb5f299c177d19d47c3205ba760665856f4efe93016823", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:18963c6b2daf45dda867fcb86e6a79e764eea259ac8fab798115a6c62b735144", "content": "Having discussed `JobInstance` and how it differs from `Job`, the natural question to ask\nis: \"`How is one `JobInstance` distinguished from another?`\" The answer is:\n`JobParameters`. A `JobParameters` object holds a set of parameters used to start a batch\njob. They can be used for identification or even as reference data during the run, as the\nfollowing image shows:\n\n.Job Parameters\nimage::job-stereotypes-parameters.png[Job Parameters, scaledwidth=\"60%\"]\n\nIn the example from the xref:domain.adoc#jobinstance[Job Instance] section, where there are\ntwo instances, one for January 1st and another for January 2nd, there is really only one `Job`,\nbut it has two `JobParameter` objects: one that was started with a job parameter of 01-01-2017\nand another that was started with a parameter of 01-02-2017. Thus, the contract can be defined\nas: `JobInstance` = `Job` + identifying `JobParameters`. This allows a developer to effectively\ncontrol how a `JobInstance` is defined, since they control what parameters are passed in.\n\nNOTE: Not all job parameters are required to contribute to the identification of a\n`JobInstance`. By default, they do so. However, the framework also allows the submission\nof a `Job` with parameters that do not contribute to the identity of a `JobInstance`.\n\n[[jobexecution]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "JobParameters", "heading_level": 3, "file_order": 47, "section_index": 3, "content_hash": "18963c6b2daf45dda867fcb86e6a79e764eea259ac8fab798115a6c62b735144", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:13c9dab551470785bbef4692b3b3ef399f608de9ec00e5abc101fd12b75842d6", "content": "A `JobExecution` refers to the technical concept of a single attempt to run a Job. An\nexecution may end in failure or success, but the `JobInstance` corresponding to a given\nexecution is not considered to be complete unless the execution completes successfully.\nUsing the `EndOfDay` `Job` described previously as an example, consider a `JobInstance` for\n01-01-2017 that failed the first time it was run. If it is run again with the same\nidentifying job parameters as the first run (01-01-2017), a new `JobExecution` is\ncreated. However, there is still only one `JobInstance`.\n\nA `Job` defines what a job is and how it is to be executed, and a `JobInstance` is a\npurely organizational object to group executions together, primarily to enable correct\nrestart semantics. A `JobExecution`, however, is the primary storage mechanism for what\nactually happened during a run and contains many more properties that must be controlled\nand persisted, as the following table shows:\n\n.JobExecution Properties\n\n|===\n|Property |Definition\n|`Status`\n|A `BatchStatus` object that indicates the status of the execution. While running, it is\n`BatchStatus#STARTED`. If it fails, it is `BatchStatus#FAILED`. If it finishes\nsuccessfully, it is `BatchStatus#COMPLETED`\n\n|`startTime`\n|A `java.time.LocalDateTime` representing the current system time when the execution was started.\nThis field is empty if the job has yet to start.\n\n|`endTime`\n|A `java.time.LocalDateTime` representing the current system time when the execution finished,\nregardless of whether it was successful or not. The field is empty if the job has yet to\nfinish.\n\n|`exitStatus`\n|The `ExitStatus`, indicating the result of the run. It is most important, because it\ncontains an exit code that is returned to the caller. See chapter 5 for more details. The\nfield is empty if the job has yet to finish.\n\n|`createTime`\n|A `java.time.LocalDateTime` representing the current system time when the `JobExecution` was\nfirst persisted. The job may not have been started yet (and thus has no start time), but\nit always has a `createTime`, which is required by the framework for managing job-level\n`ExecutionContexts`.\n\n|`lastUpdated`\n|A `java.time.LocalDateTime` representing the last time a `JobExecution` was persisted. This field\nis empty if the job has yet to start.\n\n|`executionContext`\n|The \"`property bag`\" containing any user data that needs to be persisted between\nexecutions.\n\n|`failureExceptions`\n|The list of exceptions encountered during the execution of a `Job`. These can be useful\nif more than one exception is encountered during the failure of a `Job`.\n|===\n\nThese properties are important because they are persisted and can be used to completely\ndetermine the status of an execution. For example, if the `EndOfDay` job for 01-01 is\nexecuted at 9:00 PM and fails at 9:30, the following entries are made in the batch\nmetadata tables:\n\n.BATCH_JOB_INSTANCE\n\n|===\n|JOB_INST_ID |JOB_NAME\n|1\n|EndOfDayJob\n|===\n\n.BATCH_JOB_EXECUTION_PARAMS\n|===\n|JOB_EXECUTION_ID|TYPE_CD|KEY_NAME|DATE_VAL|IDENTIFYING\n|1\n|DATE\n|schedule.Date\n|2017-01-01\n|TRUE\n|===\n\n.BATCH_JOB_EXECUTION\n|===\n|JOB_EXEC_ID|JOB_INST_ID|START_TIME|END_TIME|STATUS\n|1\n|1\n|2017-01-01 21:00\n|2017-01-01 21:30\n|FAILED\n|===\n\nNOTE: Column names may have been abbreviated or removed for the sake of clarity and\nformatting.\n\nNow that the job has failed, assume that it took the entire night for the problem to be\ndetermined, so that the \"`batch window`\" is now closed. Further assuming that the window\nstarts at 9:00 PM, the job is kicked off again for 01-01, starting where it left off and\ncompleting successfully at 9:30. Because it is now the next day, the 01-02 job must be\nrun as well, and it is kicked off just afterwards at 9:31 and completes in its normal one\nhour time at 10:30. There is no requirement that one `JobInstance` be kicked off after\nanother, unless there is potential for the two jobs to attempt to access the same data,\ncausing issues with locking at the database level. It is entirely up to the scheduler to\ndetermine when a `Job` should be run. Since they are separate `JobInstances`, Spring\nBatch makes no attempt to stop them from being run concurrently. (Attempting to run the\nsame `JobInstance` while another is already running results in a\n`JobExecutionAlreadyRunningException` being thrown). There should now be an extra entry\nin both the `JobInstance` and `JobParameters` tables and two extra entries in the\n`JobExecution` table, as shown in the following tables:\n\n.BATCH_JOB_INSTANCE\n|===\n|JOB_INST_ID |JOB_NAME\n|1\n|EndOfDayJob\n\n|2\n|EndOfDayJob\n|===\n\n.BATCH_JOB_EXECUTION_PARAMS\n|===\n|JOB_EXECUTION_ID|TYPE_CD|KEY_NAME|DATE_VAL|IDENTIFYING\n|1\n|DATE\n|schedule.Date\n|2017-01-01 00:00:00\n|TRUE\n\n|2\n|DATE\n|schedule.Date\n|2017-01-01 00:00:00\n|TRUE\n\n|3\n|DATE\n|schedule.Date\n|2017-01-02 00:00:00\n|TRUE\n|===\n\n.BATCH_JOB_EXECUTION\n|===\n|JOB_EXEC_ID|JOB_INST_ID|START_TIME|END_TIME|STATUS\n|1\n|1\n|2017-01-01 21:00\n|2017-01-01 21:30\n|FAILED\n\n|2\n|1\n|2017-01-02 21:00\n|2017-01-02 21:30\n|COMPLETED\n\n|3\n|2\n|2017-01-02 21:31\n|2017-01-02 22:29\n|COMPLETED\n|===\n\nNOTE: Column names may have been abbreviated or removed for the sake of clarity and\nformatting.\n\n[[step]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "JobExecution", "heading_level": 3, "file_order": 47, "section_index": 4, "content_hash": "13c9dab551470785bbef4692b3b3ef399f608de9ec00e5abc101fd12b75842d6", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:3e71840f1038ab862d704e4faf4056bf21184392ab8659b071c95342ed908ee5", "content": "A `Step` is a domain object that encapsulates an independent, sequential phase of a batch\njob. Therefore, every `Job` is composed entirely of one or more steps. A `Step` contains\nall the information necessary to define and control the actual batch processing. This\nis a necessarily vague description because the contents of any given `Step` are at the\ndiscretion of the developer writing a `Job`. A `Step` can be as simple or complex as the\ndeveloper desires. A simple `Step` might load data from a file into the database,\nrequiring little or no code (depending upon the implementations used). A more complex\n`Step` may have complicated business rules that are applied as part of the processing. As\nwith a `Job`, a `Step` has an individual `StepExecution` that correlates with a unique\n`JobExecution`, as the following image shows:\n\n.Job Hierarchy With Steps\nimage::jobHeirarchyWithSteps.png[Figure 2.1: Job Hierarchy With Steps, scaledwidth=\"60%\"]\n\n[[stepexecution]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "Step", "heading_level": 2, "file_order": 47, "section_index": 5, "content_hash": "3e71840f1038ab862d704e4faf4056bf21184392ab8659b071c95342ed908ee5", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:ba873068ba4f39754fe71c24888c6b9c7dda8e155375489ec85ee7908b66649b", "content": "A `StepExecution` represents a single attempt to execute a `Step`. A new `StepExecution`\nis created each time a `Step` is run, similar to `JobExecution`. However, if a step fails\nto execute because the step before it fails, no execution is persisted for it. A\n`StepExecution` is created only when its `Step` is actually started.\n\n`Step` executions are represented by objects of the `StepExecution` class. Each execution\ncontains a reference to its corresponding step and `JobExecution` and transaction-related\ndata, such as commit and rollback counts and start and end times. Additionally, each step\nexecution contains an `ExecutionContext`, which contains any data a developer needs to\nhave persisted across batch runs, such as statistics or state information needed to\nrestart. The following table lists the properties for `StepExecution`:\n\n.StepExecution Properties\n|===\n|Property|Definition\n|`Status`\n|A `BatchStatus` object that indicates the status of the execution. While running, the\nstatus is `BatchStatus.STARTED`. If it fails, the status is `BatchStatus.FAILED`. If it\nfinishes successfully, the status is `BatchStatus.COMPLETED`.\n\n|`startTime`\n|A `java.time.LocalDateTime` representing the current system time when the execution was started.\nThis field is empty if the step has yet to start.\n\n|`endTime`\n\n|A `java.time.LocalDateTime` representing the current system time when the execution finished,\nregardless of whether it was successful or not. This field is empty if the step has yet to\nexit.\n\n|`exitStatus`\n|The `ExitStatus` indicating the result of the execution. It is most important, because\nit contains an exit code that is returned to the caller. See chapter 5 for more details.\nThis field is empty if the job has yet to exit.\n\n|`executionContext`\n|The \"`property bag`\" containing any user data that needs to be persisted between\nexecutions.\n\n|`readCount`\n|The number of items that have been successfully read.\n\n|`writeCount`\n|The number of items that have been successfully written.\n\n|`commitCount`\n|The number of transactions that have been committed for this execution.\n\n|`rollbackCount`\n|The number of times the business transaction controlled by the `Step` has been rolled\nback.\n\n|`readSkipCount`\n|The number of times `read` has failed, resulting in a skipped item.\n\n|`processSkipCount`\n|The number of times `process` has failed, resulting in a skipped item.\n\n|`filterCount`\n|The number of items that have been \"`filtered`\" by the `ItemProcessor`.\n\n|`writeSkipCount`\n|The number of times `write` has failed, resulting in a skipped item.\n|===\n\n[[executioncontext]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "StepExecution", "heading_level": 3, "file_order": 47, "section_index": 6, "content_hash": "ba873068ba4f39754fe71c24888c6b9c7dda8e155375489ec85ee7908b66649b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:7bf1d92d7e877edea160eff3a8c812a325a699e40479803c909608cd79b28314", "content": "An `ExecutionContext` represents a collection of key/value pairs that are persisted and\ncontrolled by the framework to give developers a place to store persistent\nstate that is scoped to a `StepExecution` object or a `JobExecution` object. (For those\nfamiliar with Quartz, it is very similar to `JobDataMap`.) The best usage example is to\nfacilitate restart. Using flat file input as an example, while processing individual\nlines, the framework periodically persists the `ExecutionContext` at commit points. Doing\nso lets the `ItemReader` store its state in case a fatal error occurs during the run\nor even if the power goes out. All that is needed is to put the current number of lines\nread into the context, as the following example shows, and the framework does the\nrest:\n\n[source, java]\n----\nexecutionContext.putLong(getKey(LINES_READ_COUNT), reader.getPosition());\n----\n\nUsing the `EndOfDay` example from the `Job` stereotypes section as an example, assume there\nis one step, `loadData`, that loads a file into the database. After the first failed run,\nthe metadata tables would look like the following example:\n\n.BATCH_JOB_INSTANCE\n|===\n|JOB_INST_ID|JOB_NAME\n|1\n|EndOfDayJob\n|===\n\n.BATCH_JOB_EXECUTION_PARAMS\n|===\n|JOB_INST_ID|TYPE_CD|KEY_NAME|DATE_VAL\n|1\n|DATE\n|schedule.Date\n|2017-01-01\n|===\n\n.BATCH_JOB_EXECUTION\n|===\n|JOB_EXEC_ID|JOB_INST_ID|START_TIME|END_TIME|STATUS\n|1\n|1\n|2017-01-01 21:00\n|2017-01-01 21:30\n|FAILED\n|===\n\n.BATCH_STEP_EXECUTION\n|===\n|STEP_EXEC_ID|JOB_EXEC_ID|STEP_NAME|START_TIME|END_TIME|STATUS\n|1\n|1\n|loadData\n|2017-01-01 21:00\n|2017-01-01 21:30\n|FAILED\n|===\n\n.BATCH_STEP_EXECUTION_CONTEXT\n|===\n|STEP_EXEC_ID|SHORT_CONTEXT\n|1\n|{piece.count=40321}\n|===\n\nIn the preceding case, the `Step` ran for 30 minutes and processed 40,321 \"`pieces`\", which\nwould represent lines in a file in this scenario. This value is updated just before each\ncommit by the framework and can contain multiple rows corresponding to entries within the\n`ExecutionContext`. Being notified before a commit requires one of the various\n`StepListener` implementations (or an `ItemStream`), which are discussed in more detail\nlater in this guide. As with the previous example, it is assumed that the `Job` is\nrestarted the next day. When it is restarted, the values from the `ExecutionContext` of\nthe last run are reconstituted from the database. When the `ItemReader` is opened, it can\ncheck to see if it has any stored state in the context and initialize itself from there,\nas the following example shows:\n\n[source, java]\n----\nif (executionContext.containsKey(getKey(LINES_READ_COUNT))) {\n log.debug(\"Initializing for restart. Restart data is: \" + executionContext);\n\n long lineCount = executionContext.getLong(getKey(LINES_READ_COUNT));\n\n LineReader reader = getReader();\n\n Object record = \"\";\n while (reader.getPosition() < lineCount && record != null) {\n record = readLine();\n }\n}\n----\n\nIn this case, after the preceding code runs, the current line is 40,322, letting the `Step`\nstart again from where it left off. You can also use the `ExecutionContext` for\nstatistics that need to be persisted about the run itself. For example, if a flat file\ncontains orders for processing that exist across multiple lines, it may be necessary to\nstore how many orders have been processed (which is much different from the number of\nlines read), so that an email can be sent at the end of the `Step` with the total number\nof orders processed in the body. The framework handles storing this for the developer,\nto correctly scope it with an individual `JobInstance`. It can be very difficult to\nknow whether an existing `ExecutionContext` should be used or not. For example, using the\n`EndOfDay` example from above, when the 01-01 run starts again for the second time, the\nframework recognizes that it is the same `JobInstance` and on an individual `Step` basis,\npulls the `ExecutionContext` out of the database, and hands it (as part of the\n`StepExecution`) to the `Step` itself. Conversely, for the 01-02 run, the framework\nrecognizes that it is a different instance, so an empty context must be handed to the\n`Step`. There are many of these types of determinations that the framework makes for the\ndeveloper, to ensure the state is given to them at the correct time. It is also important\nto note that exactly one `ExecutionContext` exists per `StepExecution` at any given time.\nClients of the `ExecutionContext` should be careful, because this creates a shared\nkeyspace. As a result, care should be taken when putting values in to ensure no data is\noverwritten. However, the `Step` stores absolutely no data in the context, so there is no\nway to adversely affect the framework.\n\nNote that there is at least one `ExecutionContext` per\n`JobExecution` and one for every `StepExecution`. For example, consider the following\ncode snippet:\n\n[source, java]\n----\nExecutionContext ecStep = stepExecution.getExecutionContext();\nExecutionContext ecJob = jobExecution.getExecutionContext();\n----\n\nAs noted in the comment, `ecStep` does not equal `ecJob`. They are two different\n`ExecutionContexts`. The one scoped to the `Step` is saved at every commit point in the\n`Step`, whereas the one scoped to the Job is saved in between every `Step` execution.\n\nNOTE: In the `ExecutionContext`, all non-transient entries must be `Serializable`.\nProper serialization of the execution context underpins the restart capability of steps and jobs.\nShould you use keys or values that are not natively serializable, you are required to\nemploy a tailored serialization approach. Failing to serialize the execution context\nmay jeopardize the state persistence process, making failed jobs impossible to recover properly.\n\n[[jobrepository]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "ExecutionContext", "heading_level": 2, "file_order": 47, "section_index": 7, "content_hash": "7bf1d92d7e877edea160eff3a8c812a325a699e40479803c909608cd79b28314", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:a3b6be569e9ee136d1f9d65a843fbe4311ba320b0ef5d88aebbf1682a1e072cb", "content": "`JobRepository` is the persistence mechanism for all of the stereotypes mentioned earlier.\nIt provides CRUD operations for `JobLauncher`, `Job`, and `Step` implementations. When a\n`Job` is first launched, a `JobExecution` is obtained from the repository. Also, during\nthe course of execution, `StepExecution` and `JobExecution` implementations are persisted\nby passing them to the repository.\n\n[tabs]\n====\nJava::\n+\nWhen using Java configuration, the `@EnableBatchProcessing` annotation provides a\n`JobRepository` as one of the components that is automatically configured.\n\nXML::\n+\nThe Spring Batch XML namespace provides support for configuring a `JobRepository` instance\nwith the `<job-repository>` tag, as the following example shows:\n+\n[source, xml]\n----\n<job-repository id=\"jobRepository\"/>\n----\n====\n\n[[jobOperator]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "JobRepository", "heading_level": 2, "file_order": 47, "section_index": 8, "content_hash": "a3b6be569e9ee136d1f9d65a843fbe4311ba320b0ef5d88aebbf1682a1e072cb", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:449dceca8d5b591106b8103863a19c4c74855709fddab81ac508e682c6d09a06", "content": "`JobOperator` represents a simple interface for operations like starting, stopping and restarting\njobs, as the following example shows:\n\n[source, java]\n----\npublic interface JobOperator {\n\n JobExecution start(Job job, JobParameters jobParameters) throws Exception;\n JobExecution startNextInstance(Job job) throws Exception;\n boolean stop(JobExecution jobExecution) throws Exception;\n JobExecution restart(JobExecution jobExecution) throws Exception;\n JobExecution abandon(JobExecution jobExecution) throws Exception;\n\n}\n----\n\nA `Job` is started with a given set of `JobParameters`. It is expected that implementations obtain\na valid `JobExecution` from the `JobRepository` and execute the `Job`.\n\n[[itemreader]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "JobOperator", "heading_level": 2, "file_order": 47, "section_index": 9, "content_hash": "449dceca8d5b591106b8103863a19c4c74855709fddab81ac508e682c6d09a06", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:73ceaf6daadfd3d999a49bc05491a3406c543b21f2e5c02e2b8c6baf9b46a711", "content": "`ItemReader` is an abstraction that represents the retrieval of input for a `Step`, one\nitem at a time. When the `ItemReader` has exhausted the items it can provide, it\nindicates this by returning `null`. You can find more details about the `ItemReader` interface and its\nvarious implementations in\nxref:readersAndWriters.adoc[Readers And Writers].\n\n[[itemwriter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "ItemReader", "heading_level": 2, "file_order": 47, "section_index": 10, "content_hash": "73ceaf6daadfd3d999a49bc05491a3406c543b21f2e5c02e2b8c6baf9b46a711", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:2032d786387c7d7f6ee0dc4fde0dfcf05aff992cc3c2d70a54e2e74164b847d9", "content": "`ItemWriter` is an abstraction that represents the output of a `Step`, one batch or chunk\nof items at a time. Generally, an `ItemWriter` has no knowledge of the input it should\nreceive next and knows only the item that was passed in its current invocation. You can find more\ndetails about the `ItemWriter` interface and its various implementations in\nxref:readersAndWriters.adoc[Readers And Writers].\n\n[[itemprocessor]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "ItemWriter", "heading_level": 2, "file_order": 47, "section_index": 11, "content_hash": "2032d786387c7d7f6ee0dc4fde0dfcf05aff992cc3c2d70a54e2e74164b847d9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:2afd58214503c4eca902fc2d26a08a946e951df9d640054d23a68a0e7978ee90", "content": "`ItemProcessor` is an abstraction that represents the business processing of an item.\nWhile the `ItemReader` reads one item, and the `ItemWriter` writes one item, the\n`ItemProcessor` provides an access point to transform or apply other business processing.\nIf, while processing the item, it is determined that the item is not valid, returning\n`null` indicates that the item should not be written out. You can find more details about the\n`ItemProcessor` interface in\nxref:readersAndWriters.adoc[Readers And Writers].\n\n[role=\"xmlContent\"]\n[[batch-namespace]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "ItemProcessor", "heading_level": 2, "file_order": 47, "section_index": 12, "content_hash": "2afd58214503c4eca902fc2d26a08a946e951df9d640054d23a68a0e7978ee90", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:5ef50062eba16ab766dc95f1f03c1a4489233917518499c29a49f46676dcf890", "content": "Many of the domain concepts listed previously need to be configured in a Spring\n`ApplicationContext`. While there are implementations of the interfaces above that you can\nuse in a standard bean definition, a namespace has been provided for ease of\nconfiguration, as the following example shows:\n\n[source, xml, role=\"xmlContent\"]\n----\n<beans:beans xmlns=\"http://www.springframework.org/schema/batch\"\nxmlns:beans=\"http://www.springframework.org/schema/beans\"\nxmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\nxsi:schemaLocation=\"\n http://www.springframework.org/schema/beans\n https://www.springframework.org/schema/beans/spring-beans.xsd\n http://www.springframework.org/schema/batch\n https://www.springframework.org/schema/batch/spring-batch.xsd\">\n\n<job id=\"ioSampleJob\">\n <step id=\"step1\">\n <tasklet>\n <chunk reader=\"itemReader\" writer=\"itemWriter\" commit-interval=\"2\"/>\n </tasklet>\n </step>\n</job>\n\n</beans:beans>\n----\n\nAs long as the batch namespace has been declared, any of its elements can be used. You can find more\ninformation on configuring a Job in xref:job.adoc[Configuring and Running a Job]\n. You can find more information on configuring a `Step` in\nxref:step.adoc[Configuring a Step].\n\nWARNING: The batch XML namespace is deprecated as of Spring Batch 6.0 and will be removed in version 7.0.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/domain.adoc", "title": "domain", "heading": "Batch Namespace", "heading_level": 2, "file_order": 47, "section_index": 13, "content_hash": "5ef50062eba16ab766dc95f1f03c1a4489233917518499c29a49f46676dcf890", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/domain.adoc"}}
{"id": "sha256:fd49226e7182207fbca7885aacc8f94704b5fdef9a8920dfa1df502573571e29", "content": "[[faq]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/faq.adoc", "title": "faq", "heading": "faq", "heading_level": 1, "file_order": 48, "section_index": 0, "content_hash": "fd49226e7182207fbca7885aacc8f94704b5fdef9a8920dfa1df502573571e29", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/faq.adoc"}}
{"id": "sha256:e78cdc490befaec740680bcab6cc04791667d139e3b2c27fa208da63a82db87b", "content": "There are three ways to approach this - but we recommend exercising caution in the analysis of such requirements (is it really necessary?).\n\n* Add a `TaskExecutor` to the step. The `StepBuilder`s provided for configuring Steps have a \"taskExecutor\" property you can set.This works as long as the step is intrinsically restartable (idempotent effectively). The parallel job sample shows how it might work in practice - this uses a \"process indicator\" pattern to mark input records as complete, inside the business transaction.\n* Use the `PartitionStep` to split your step execution explicitly amongst several Step instances. Spring Batch has a local multi-threaded implementation of the main strategy for this (`PartitionHandler`), which makes it a great choice for IO intensive jobs. Remember to use `scope=\"step\"` for the stateful components in a step executing in this fashion, so that separate instances are created per step execution, and there is no cross talk between threads.\n* Use the Remote Chunking approach as implemented in the `spring-batch-integration` module. This requires some durable middleware (e.g. JMS) for reliable communication between the driving step and the remote workers. The basic idea is to use a special `ItemWriter` on the driving process, and a listener pattern on the worker processes (via a `ChunkProcessor`).", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/faq.adoc", "title": "faq", "heading": "Is it possible to execute jobs in multiple threads or multiple processes?", "heading_level": 2, "file_order": 48, "section_index": 1, "content_hash": "e78cdc490befaec740680bcab6cc04791667d139e3b2c27fa208da63a82db87b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/faq.adoc"}}
{"id": "sha256:87575ebced622ca553e5d001caad29723008f9a36eae54e41268a2b74362a472", "content": "You can synchronize the `read()` method (e.g. by wrapping it in a delegator that does the synchronization).\nRemember that you will lose restartability, so best practice is to mark the step as not restartable and to be safe (and efficient) you can also set `saveState=false` on the reader.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/faq.adoc", "title": "faq", "heading": "How can I make an item reader thread safe?", "heading_level": 2, "file_order": 48, "section_index": 2, "content_hash": "87575ebced622ca553e5d001caad29723008f9a36eae54e41268a2b74362a472", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/faq.adoc"}}
{"id": "sha256:9c205d9f331795440e636efaca6d5d7ad4f6e31e9ce783b76904ac24601205c5", "content": "There are many extension points in Spring Batch for the framework developer (as opposed to the implementor of business logic).\nWe expect clients to create their own more specific strategies that can be plugged in to control things like commit intervals ( `CompletionPolicy` ),\nrules about how to deal with exceptions ( `ExceptionHandler` ), and many others.\n\nIn general we try to dissuade users from extending framework classes. The Java language doesn't give us as much flexibility to mark classes and interfaces as internal.\nGenerally you can expect anything at the top level of the source tree in packages `org.springframework.batch.*` to be public, but not necessarily sub-classable.\nExtending our concrete implementations of most strategies is discouraged in favour of a composition or forking approach.\nIf your code can use only the interfaces from Spring Batch, that gives you the greatest possible portability.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/faq.adoc", "title": "faq", "heading": "What is the Spring Batch philosophy on the use of flexible strategies and default implementations? Can you add a public getter for this or that property?", "heading_level": 2, "file_order": 48, "section_index": 3, "content_hash": "9c205d9f331795440e636efaca6d5d7ad4f6e31e9ce783b76904ac24601205c5", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/faq.adoc"}}
{"id": "sha256:376a9f9f91d285da2a4592fa228199b3b71f157729e8a9aa959fcc2c88056b3d", "content": "Spring Batch and Quartz have different goals. Spring Batch provides functionality for processing large volumes of data and Quartz provides functionality for scheduling tasks.\nSo Quartz could complement Spring Batch, but are not excluding technologies. A common combination would be to use Quartz as a trigger for a Spring Batch job using a Cron expression\nand the Spring Core convenience `SchedulerFactoryBean` .", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/faq.adoc", "title": "faq", "heading": "How does Spring Batch differ from Quartz? Is there a place for them both in a solution?", "heading_level": 2, "file_order": 48, "section_index": 4, "content_hash": "376a9f9f91d285da2a4592fa228199b3b71f157729e8a9aa959fcc2c88056b3d", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/faq.adoc"}}
{"id": "sha256:5ccc206c25a89c8cab2a9e41d7ace103ed744f6150c322415c11504fc4fec9e9", "content": "Use a scheduling tool. There are plenty of them out there. Examples: Quartz, Control-M, Autosys.\nQuartz doesn't have all the features of Control-M or Autosys - it is supposed to be lightweight.\nIf you want something even more lightweight you can just use the OS (`cron`, `at`, etc.).\n\nSimple sequential dependencies can be implemented using the job-steps model of Spring Batch, and the non-sequential features in Spring Batch.\nWe think this is quite common. And in fact it makes it easier to correct a common mis-use of schedulers - having hundreds of jobs configured,\nmany of which are not independent, but only depend on one other.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/faq.adoc", "title": "faq", "heading": "How do I schedule a job with Spring Batch?", "heading_level": 2, "file_order": 48, "section_index": 5, "content_hash": "5ccc206c25a89c8cab2a9e41d7ace103ed744f6150c322415c11504fc4fec9e9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/faq.adoc"}}
{"id": "sha256:69cda12252e4f479d61a6f06671906c131f8353fee5a96fb157cf42a010e0a62", "content": "We see this as one of the roles of the `Job` or `Step`. A specific implementation of the Step deals with the concern of breaking apart the business logic\nand sharing it efficiently between parallel processes or processors (see `PartitionStep` ). There are a number of technologies that could play a role here.\nThe essence is just a set of concurrent remote calls to distributed agents that can handle some business processing.\nSince the business processing is already typically modularised - e.g. input an item, process it - Spring Batch can strategise the distribution in a number of ways.\nOne implementation that we have had some experience with is a set of remote web services handling the business processing.\nWe send a specific range of primary keys for the inputs to each of a number of remote calls.\nThe same basic strategy would work with any of the Spring Remoting protocols (plain RMI, HttpInvoker, JMS, Hessian etc.) with little more than a couple of lines change\nin the execution layer configuration.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/faq.adoc", "title": "faq", "heading": "How does Spring Batch allow project to optimize for performance and scalability (through parallel processing or other)?", "heading_level": 2, "file_order": 48, "section_index": 6, "content_hash": "69cda12252e4f479d61a6f06671906c131f8353fee5a96fb157cf42a010e0a62", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/faq.adoc"}}
{"id": "sha256:99eb7364ee980202a83d3deb56845c4ccd927955547aa51538fe9143e4f94cf6", "content": "There is a good deal of practical evidence from existing projects that a pipeline approach to batch processing is highly beneficial, leading to resilience and high throughput.\nWe are often faced with mission-critical applications where audit trails are essential, and guaranteed processing is demanded, but where there are extremely tight limits\non performance under load, or where high throughput gives a competitive advantage.\n\nMatt Welsh's work shows that a Staged Event Driven Architecture (SEDA) has enormous benefits over more rigid processing architectures,\nand message-oriented middleware (JMS, AQ, MQ, Tibco etc.) gives us a lot of resilience out of the box. There are particular benefits in\na system where there is feedback between downstream and upstream stages, so the number of consumers can be adjusted to account for the amount of demand.\nSo how does this fit into Spring Batch? The spring-batch-integration project has this pattern implemented in Spring Integration,\nand can be used to scale up the remote processing of any step with many items to process.\nSee in particular the \"chunk\" package, and the `ItemWriter` and `ChunkRequestHandler` implementations in there.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/faq.adoc", "title": "faq", "heading": "How can messaging be used to scale batch architectures?", "heading_level": 2, "file_order": 48, "section_index": 7, "content_hash": "99eb7364ee980202a83d3deb56845c4ccd927955547aa51538fe9143e4f94cf6", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/faq.adoc"}}
{"id": "sha256:12f983524fe021a982d2d93185b36e7dc78e008f5ee77019e14d5b0cb834f4c1", "content": "[[glossary]]\n[appendix]\n[[glossary]]\n\n[glossary]\n[[spring-batch-glossary]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/glossary.adoc", "title": "glossary", "heading": "glossary", "heading_level": 1, "file_order": 49, "section_index": 0, "content_hash": "12f983524fe021a982d2d93185b36e7dc78e008f5ee77019e14d5b0cb834f4c1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/glossary.adoc"}}
{"id": "sha256:5a618ccd1db8968afe697dd24d11f999d3ec7ef145918c1c97d0348de8ee2ed9", "content": "Batch::\n An accumulation of business transactions over time.\n\nBatch Application Style::\n Term used to designate batch as an application style in its own right, similar to\n online, Web, or SOA. It has standard elements of input, validation, transformation of\n information to business model, business processing, and output. In addition, it\n requires monitoring at a macro level.\n\nBatch Processing::\n The handling of a batch of many business transactions that have accumulated over a\n period of time (such as an hour, a day, a week, a month, or a year). It is the\n application of a process or set of processes to many data entities or objects in a\n repetitive and predictable fashion with either no manual element or a separate manual\n element for error processing.\n\nBatch Window::\n The time frame within which a batch job must complete. This can be constrained by other\n systems coming online, other dependent jobs needing to execute, or other factors\n specific to the batch environment.\n\nStep::\n The main batch task or unit of work. It initializes the business logic and controls the\n transaction environment, based on the commit interval setting and other factors.\n\nTasklet::\n A component created by an application developer to process the business logic for a\n Step.\n\nBatch Job Type::\n Job types describe application of jobs for particular types of processing. Common areas\n are interface processing (typically flat files), forms processing (either for online\n PDF generation or print formats), and report processing.\n\nDriving Query::\n A driving query identifies the set of work for a job to do. The job then breaks that\n work into individual units of work. For instance, a driving query might be to identify\n all financial transactions that have a status of \"`pending transmission`\" and send them\n to a partner system. The driving query returns a set of record IDs to process. Each\n record ID then becomes a unit of work. A driving query may involve a join (if the\n criteria for selection falls across two or more tables) or it may work with a single\n table.\n\nItem::\n An item represents the smallest amount of complete data for processing. In the simplest\n terms, this might be a line in a file, a row in a database table, or a particular\n element in an XML file.\n\nLogicial Unit of Work (LUW)::\n A batch job iterates through a driving query (or other input source, such as a file) to\n perform the set of work that the job must accomplish. Each iteration of work performed\n is a unit of work.\n\nCommit Interval::\n A set of LUWs processed within a single transaction.\n\nPartitioning::\n Splitting a job into multiple threads where each thread is responsible for a subset of\n the overall data to be processed. The threads of execution may be within the same JVM\n or they may span JVMs in a clustered environment that supports workload balancing.\n\nStaging Table::\n A table that holds temporary data while it is being processed.\n\nRestartable::\n A job that can be executed again and assumes the same identity as when run initially.\n In other words, it has the same job instance ID.\n\nRerunnable::\n A job that is restartable and manages its own state in terms of the previous run's\n record processing. An example of a re-runnable step is one based on a driving query. If\n the driving query can be formed so that it limits the processed rows when the job is\n restarted, then it is re-runnable. This is managed by the application logic. Often, a\n condition is added to the `where` statement to limit the rows returned by the driving\n query with logic resembling `and processedFlag!= true`.\n\nRepeat::\n One of the most basic units of batch processing, it defines by repeatedly calling a\n portion of code until it is finished and while there is no error. Typically, a batch\n process would be repeatable as long as there is input.\n\nRetry::\n Simplifies the execution of operations with retry semantics most frequently associated\n with handling transactional output exceptions. Retry is slightly different from repeat.\n Rather than continually calling a block of code, retry is stateful and continually\n calls the same block of code with the same input, until it either succeeds or some type\n of retry limit has been exceeded. It is generally useful only when a subsequent\n invocation of the operation might succeed because something in the environment has\n improved.\n\nRecover::\n Recover operations handle an exception in such a way that a repeat process is able to\n continue.\n\nSkip::\n Skip is a recovery strategy often used on file input sources as the strategy for\n ignoring bad input records that failed validation.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/glossary.adoc", "title": "glossary", "heading": "Spring Batch Glossary", "heading_level": 2, "file_order": 49, "section_index": 1, "content_hash": "5a618ccd1db8968afe697dd24d11f999d3ec7ef145918c1c97d0348de8ee2ed9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/glossary.adoc"}}
{"id": "sha256:47e3208c9a1e848ac4072fdee3f8b5d612427a8036d87f85d23de6fbefaa5722", "content": "The reference documentation is divided into several sections:\n\n[horizontal]\nxref:spring-batch-intro.adoc[Spring Batch Introduction] :: Background, usage\n scenarios, and general guidelines.\nxref:whatsnew.adoc[What's new in Spring Batch 6.0] :: New features introduced in version 6.0.\nxref:spring-batch-architecture.adoc[Spring Batch Architecture] :: Spring Batch\narchitecture, general batch principles, batch processing strategies.\nxref:domain.adoc[The Domain Language of Batch] :: Core concepts and abstractions\nof the Batch domain language.\nxref:job.adoc[Configuring and Running a Job] :: Job configuration, execution, and\nadministration.\nxref:step.adoc[Configuring a Step] :: Step configuration, different types of steps, and\ncontrolling step flow.\nxref:readersAndWriters.adoc[Item reading and writing] :: `ItemReader`\nand `ItemWriter` interfaces and how to use them.\nxref:processor.adoc[Item processing] :: `ItemProcessor` interface and how to use it.\nxref:scalability.adoc#scalability[Scaling and Parallel Processing] :: Multi-threaded steps,\nparallel steps, remote chunking, and partitioning.\n<<repeat.adoc#repeat,Repeat>> :: Completion policies and exception handling of repetitive actions.\n<<retry.adoc#retry,Retry>> :: Retry and backoff policies of retryable operations.\nxref:testing.adoc[Unit and Integration Testing] :: Job and Step testing facilities and APIs.\nxref:common-patterns.adoc#commonPatterns[Common Patterns] :: Common batch processing patterns\nand guidelines.\nxref:spring-batch-integration.adoc[Spring Batch Integration] :: Integration\nbetween Spring Batch and Spring Integration projects.\nxref:spring-batch-observability.adoc[Spring Batch Observability] :: Batch jobs\nmonitoring and metrics.\n\nThe following appendices are available:\n\n[horizontal]\nxref:appendix.adoc#listOfReadersAndWriters[List of ItemReaders and ItemWriters] :: List of\nall provided item readers and writers.\nxref:schema-appendix.adoc#metaDataSchema[Meta-Data Schema] :: Core tables used by the Batch\ndomain model.\n<<glossary.adoc#glossary,Glossary>> :: Glossary of common terms, concepts, and vocabulary of\nthe Batch domain.\n<<faq.adoc#faq,Frequently Asked Questions>> :: Frequently Asked Questions about Spring Batch.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/index.adoc", "title": "Overview", "heading": "Overview", "heading_level": 1, "file_order": 50, "section_index": 0, "content_hash": "47e3208c9a1e848ac4072fdee3f8b5d612427a8036d87f85d23de6fbefaa5722", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/index.adoc"}}
{"id": "sha256:51339c6eb4092cf4d52857bd624f49d248748f3eeb3f9af6b37dd48bd87d9474", "content": "[[configureJob]]\n\nifndef::onlyonetoggle[]\nendif::onlyonetoggle[]\n\nIn the xref:domain.adoc[domain section] , the overall\narchitecture design was discussed, using the following diagram as a\nguide:\n\n.Batch Stereotypes\nimage::spring-batch-reference-model.png[Figure 2.1: Batch Stereotypes, scaledwidth=\"60%\"]\n\nWhile the `Job` object may seem like a simple\ncontainer for steps, you must be aware of many configuration options.\nFurthermore, you must consider many options about\nhow a `Job` can be run and how its metadata can be\nstored during that run. This chapter explains the various configuration\noptions and runtime concerns of a `Job`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/job.adoc", "title": "job", "heading": "job", "heading_level": 1, "file_order": 51, "section_index": 0, "content_hash": "51339c6eb4092cf4d52857bd624f49d248748f3eeb3f9af6b37dd48bd87d9474", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/job.adoc"}}
{"id": "sha256:0b66246d2f8e12122079eda1ae710b02c02d8d4f3ad0e7c27b1757de9d5b6e9b", "content": "[[itemProcessor]]\n\nThe xref:readersAndWriters.adoc[ItemReader and ItemWriter interfaces] are both very useful for their specific\ntasks, but what if you want to insert business logic before writing? One option for both\nreading and writing is to use the composite pattern: Create an `ItemWriter` that contains\nanother `ItemWriter` or an `ItemReader` that contains another `ItemReader`. The following\ncode shows an example:\n\n[source, java]\n----\npublic class CompositeItemWriter<T> implements ItemWriter<T> {\n\n ItemWriter<T> itemWriter;\n\n public CompositeItemWriter(ItemWriter<T> itemWriter) {\n this.itemWriter = itemWriter;\n }\n\n public void write(Chunk<? extends T> items) throws Exception {\n //Add business logic here\n itemWriter.write(items);\n }\n\n public void setDelegate(ItemWriter<T> itemWriter){\n this.itemWriter = itemWriter;\n }\n}\n----\n\nThe preceding class contains another `ItemWriter` to which it delegates after having\nprovided some business logic. This pattern could easily be used for an `ItemReader` as\nwell, perhaps to obtain more reference data based on the input that was provided by the\nmain `ItemReader`. It is also useful if you need to control the call to `write` yourself.\nHowever, if you only want to \"`transform`\" the item passed in for writing before it is\nactually written, you need not `write` yourself. You can just modify the item. For this\nscenario, Spring Batch provides the `ItemProcessor` interface, as the following\ninterface definition shows:\n\n[source, java]\n----\npublic interface ItemProcessor<I, O> {\n\n O process(I item) throws Exception;\n}\n----\n\nAn `ItemProcessor` is simple. Given one object, transform it and return another. The\nprovided object may or may not be of the same type. The point is that business logic may\nbe applied within the process, and it is completely up to the developer to create that\nlogic. An `ItemProcessor` can be wired directly into a step. For example, assume an\n`ItemReader` provides a class of type `Foo` and that it needs to be converted to type `Bar`\nbefore being written out. The following example shows an `ItemProcessor` that performs\nthe conversion:\n\n[source, java]\n----\npublic class Foo {}\n\npublic class Bar {\n public Bar(Foo foo) {}\n}\n\npublic class FooProcessor implements ItemProcessor<Foo, Bar> {\n public Bar process(Foo foo) throws Exception {\n //Perform simple transformation, convert a Foo to a Bar\n return new Bar(foo);\n }\n}\n\npublic class BarWriter implements ItemWriter<Bar> {\n public void write(Chunk<? extends Bar> bars) throws Exception {\n //write bars\n }\n}\n----\n\nIn the preceding example, there is a class named `Foo`, a class named `Bar`, and a class\nnamed `FooProcessor` that adheres to the `ItemProcessor` interface. The transformation is\nsimple, but any type of transformation could be done here. The `BarWriter` writes `Bar`\nobjects, throwing an exception if any other type is provided. Similarly, the\n`FooProcessor` throws an exception if anything but a `Foo` is provided. The\n`FooProcessor` can then be injected into a `Step`, as the following example shows:\n\n[tabs]\n====\nJava::\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job ioSampleJob(JobRepository jobRepository, Step step1) {\n\treturn new JobBuilder(\"ioSampleJob\", jobRepository)\n .start(step1)\n .build();\n}\n\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<Foo, Bar>chunk(2).transactionManager(transactionManager)\n .reader(fooReader())\n .processor(fooProcessor())\n .writer(barWriter())\n .build();\n}\n----\n\nXML::\n+\n.XML Configuration\n[source, xml]\n----\n<job id=\"ioSampleJob\">\n <step name=\"step1\">\n <tasklet>\n <chunk reader=\"fooReader\" processor=\"fooProcessor\" writer=\"barWriter\"\n commit-interval=\"2\"/>\n </tasklet>\n </step>\n</job>\n----\n\n====\n\nA difference between `ItemProcessor` and `ItemReader` or `ItemWriter` is that an `ItemProcessor`\nis optional for a `Step`.\n\n[[chainingItemProcessors]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/processor.adoc", "title": "processor", "heading": "processor", "heading_level": 1, "file_order": 52, "section_index": 0, "content_hash": "0b66246d2f8e12122079eda1ae710b02c02d8d4f3ad0e7c27b1757de9d5b6e9b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/processor.adoc"}}
{"id": "sha256:1688f45a9b001cfcfdfc86222ddb52781f366a506316b56567d5d1db31c29447", "content": "Performing a single transformation is useful in many scenarios, but what if you want to\n\"`chain`\" together multiple `ItemProcessor` implementations? You can do so by using\nthe composite pattern mentioned previously. To update the previous, single\ntransformation, example, `Foo` is transformed to `Bar`, which is transformed to `Foobar`\nand written out, as the following example shows:\n\n[source, java]\n----\npublic class Foo {}\n\npublic class Bar {\n public Bar(Foo foo) {}\n}\n\npublic class Foobar {\n public Foobar(Bar bar) {}\n}\n\npublic class FooProcessor implements ItemProcessor<Foo, Bar> {\n public Bar process(Foo foo) throws Exception {\n //Perform simple transformation, convert a Foo to a Bar\n return new Bar(foo);\n }\n}\n\npublic class BarProcessor implements ItemProcessor<Bar, Foobar> {\n public Foobar process(Bar bar) throws Exception {\n return new Foobar(bar);\n }\n}\n\npublic class FoobarWriter implements ItemWriter<Foobar>{\n public void write(Chunk<? extends Foobar> items) throws Exception {\n //write items\n }\n}\n----\n\nA `FooProcessor` and a `BarProcessor` can be 'chained' together to give the resultant\n`Foobar`, as shown in the following example:\n\n[source, java]\n----\nCompositeItemProcessor<Foo,Foobar> compositeProcessor =\n new CompositeItemProcessor<Foo,Foobar>();\nList itemProcessors = new ArrayList();\nitemProcessors.add(new FooProcessor());\nitemProcessors.add(new BarProcessor());\ncompositeProcessor.setDelegates(itemProcessors);\n----\n\nJust as with the previous example, you can configure the composite processor into the\n`Step`:\n\n[tabs]\n====\nJava::\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job ioSampleJob(JobRepository jobRepository, Step step1) {\n\treturn new JobBuilder(\"ioSampleJob\", jobRepository)\n .start(step1)\n .build();\n}\n\n@Bean\npublic Step step1(JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"step1\", jobRepository)\n .<Foo, Foobar>chunk(2).transactionManager(transactionManager)\n .reader(fooReader())\n .processor(compositeProcessor())\n .writer(foobarWriter())\n .build();\n}\n\n@Bean\npublic CompositeItemProcessor compositeProcessor() {\n\tList<ItemProcessor> delegates = new ArrayList<>(2);\n\tdelegates.add(new FooProcessor());\n\tdelegates.add(new BarProcessor());\n\n\tCompositeItemProcessor processor = new CompositeItemProcessor();\n\n\tprocessor.setDelegates(delegates);\n\n\treturn processor;\n}\n----\n\nXML::\n+\n.XML Configuration\n[source,xml]\n----\n<job id=\"ioSampleJob\">\n <step name=\"step1\">\n <tasklet>\n <chunk reader=\"fooReader\" processor=\"compositeItemProcessor\" writer=\"foobarWriter\"\n commit-interval=\"2\"/>\n </tasklet>\n </step>\n</job>\n\n<bean id=\"compositeItemProcessor\"\n class=\"org.springframework.batch.infrastructure.item.support.CompositeItemProcessor\">\n <property name=\"delegates\">\n <list>\n <bean class=\"..FooProcessor\" />\n <bean class=\"..BarProcessor\" />\n </list>\n </property>\n</bean>\n----\n\n====\n\n[[filteringRecords]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/processor.adoc", "title": "processor", "heading": "Chaining ItemProcessors", "heading_level": 2, "file_order": 52, "section_index": 1, "content_hash": "1688f45a9b001cfcfdfc86222ddb52781f366a506316b56567d5d1db31c29447", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/processor.adoc"}}
{"id": "sha256:78b44db93703a5b8af138f0fed10866e80f6f27acb71029995123238ae1f162c", "content": "One typical use for an item processor is to filter out records before they are passed to\nthe `ItemWriter`. Filtering is an action distinct from skipping. Skipping indicates that\na record is invalid, while filtering indicates that a record should not be\nwritten.\n\nFor example, consider a batch job that reads a file containing three different types of\nrecords: records to insert, records to update, and records to delete. If record deletion\nis not supported by the system, we would not want to send any deletable records to\nthe `ItemWriter`. However, since these records are not actually bad records, we would want to\nfilter them out rather than skip them. As a result, the `ItemWriter` would receive only\ninsertable and updatable records.\n\nTo filter a record, you can return `null` from the `ItemProcessor`. The framework detects\nthat the result is `null` and avoids adding that item to the list of records delivered to\nthe `ItemWriter`. An exception thrown from the `ItemProcessor` results in a\nskip.\n\n[[validatingInput]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/processor.adoc", "title": "processor", "heading": "Filtering Records", "heading_level": 2, "file_order": 52, "section_index": 2, "content_hash": "78b44db93703a5b8af138f0fed10866e80f6f27acb71029995123238ae1f162c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/processor.adoc"}}
{"id": "sha256:5e82e6a5e311f064948e10591bf66cfecc1f6640ea695f7f1e17deacf2958686", "content": "The xref:readersAndWriters.adoc[ItemReaders and ItemWriters] chapter discusses multiple approaches to parsing input.\nEach major implementation throws an exception if it is not \"`well formed.`\" The\n`FixedLengthTokenizer` throws an exception if a range of data is missing. Similarly,\nattempting to access an index in a `RowMapper` or `FieldSetMapper` that does not exist or\nis in a different format than the one expected causes an exception to be thrown. All of\nthese types of exceptions are thrown before `read` returns. However, they do not address\nthe issue of whether or not the returned item is valid. For example, if one of the fields\nis an age, it cannot be negative. It may parse correctly, because it exists and\nis a number, but it does not cause an exception. Since there are already a plethora of\nvalidation frameworks, Spring Batch does not attempt to provide yet another. Rather, it\nprovides a simple interface, called `Validator`, that you can implement by any number of\nframeworks, as the following interface definition shows:\n\n[source, java]\n----\npublic interface Validator<T> {\n\n void validate(T value) throws ValidationException;\n\n}\n----\n\nThe contract is that the `validate` method throws an exception if the object is invalid\nand returns normally if it is valid. Spring Batch provides an\n`ValidatingItemProcessor`, as the following bean definition shows:\n\n[tabs]\n====\nJava::\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic ValidatingItemProcessor itemProcessor() {\n\tValidatingItemProcessor processor = new ValidatingItemProcessor();\n\n\tprocessor.setValidator(validator());\n\n\treturn processor;\n}\n\n@Bean\npublic SpringValidator validator() {\n\tSpringValidator validator = new SpringValidator();\n\n\tvalidator.setValidator(new TradeValidator());\n\n\treturn validator;\n}\n----\n\nXML::\n+\n.XML Configuration\n[source,xml]\n----\n<bean class=\"org.springframework.batch.infrastructure.item.validator.ValidatingItemProcessor\">\n <property name=\"validator\" ref=\"validator\" />\n</bean>\n\n<bean id=\"validator\" class=\"org.springframework.batch.infrastructure.item.validator.SpringValidator\">\n\t<property name=\"validator\">\n <bean class=\"org.springframework.batch.samples.domain.trade.internal.validator.TradeValidator\"/>\n\t</property>\n</bean>\n----\n\n====\n\nYou can also use the `BeanValidatingItemProcessor` to validate items annotated with\nthe Bean Validation API (JSR-303) annotations. For example, consider the following type `Person`:\n\n[source, java]\n----\nclass Person {\n\n @NotEmpty\n private String name;\n\n public Person(String name) {\n this.name = name;\n }\n\n public String getName() {\n return name;\n }\n\n public void setName(String name) {\n this.name = name;\n }\n\n}\n----\n\nYou can validate items by declaring a `BeanValidatingItemProcessor` bean in your\napplication context and register it as a processor in your chunk-oriented step:\n\n[source, java]\n----\n@Bean\npublic BeanValidatingItemProcessor<Person> beanValidatingItemProcessor() throws Exception {\n BeanValidatingItemProcessor<Person> beanValidatingItemProcessor = new BeanValidatingItemProcessor<>();\n beanValidatingItemProcessor.setFilter(true);\n\n return beanValidatingItemProcessor;\n}\n----\n\n[[faultTolerant]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/processor.adoc", "title": "processor", "heading": "Validating Input", "heading_level": 2, "file_order": 52, "section_index": 3, "content_hash": "5e82e6a5e311f064948e10591bf66cfecc1f6640ea695f7f1e17deacf2958686", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/processor.adoc"}}
{"id": "sha256:fd342afadf50e45cd1ea80323b5132efedeead4a29ac5c9f4074ef6e52ed72ea", "content": "When a chunk is rolled back, items that have been cached during reading may be\nreprocessed. If a step is configured to be fault-tolerant (typically by using skip or\nretry processing), any `ItemProcessor` used should be implemented in a way that is\nidempotent. Typically that would consist of performing no changes on the input item for\nthe `ItemProcessor` and updating only the\ninstance that is the result.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/processor.adoc", "title": "processor", "heading": "Fault Tolerance", "heading_level": 2, "file_order": 52, "section_index": 4, "content_hash": "fd342afadf50e45cd1ea80323b5132efedeead4a29ac5c9f4074ef6e52ed72ea", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/processor.adoc"}}
{"id": "sha256:272424d2cff65f44fb9e095a2acf4398f7e265947e4108883da223f0b31a3172", "content": "[[readersAndWriters]]\n\nifndef::onlyonetoggle[]\nendif::onlyonetoggle[]\n\nAll batch processing can be described in its most simple form as reading in large amounts\nof data, performing some type of calculation or transformation, and writing the result\nout. Spring Batch provides three key interfaces to help perform bulk reading and writing:\n`ItemReader`, `ItemProcessor`, and `ItemWriter`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/readersAndWriters.adoc", "title": "readersAndWriters", "heading": "readersAndWriters", "heading_level": 1, "file_order": 53, "section_index": 0, "content_hash": "272424d2cff65f44fb9e095a2acf4398f7e265947e4108883da223f0b31a3172", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/readersAndWriters.adoc"}}
{"id": "sha256:6f562efe4c43328e2d909724202096f2c786b8658136e42640b3b3277f13b147", "content": "[[repeat]]\n\n[[repeattemplate]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/repeat.adoc", "title": "repeat", "heading": "repeat", "heading_level": 1, "file_order": 54, "section_index": 0, "content_hash": "6f562efe4c43328e2d909724202096f2c786b8658136e42640b3b3277f13b147", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/repeat.adoc"}}
{"id": "sha256:3b5af15e73daf844c45ccf4a660bb31817f8a7b00dad988116ce68de3dd92cb1", "content": "Batch processing is about repetitive actions, either as a simple optimization or as part\nof a job. To strategize and generalize the repetition and to provide what amounts to an\niterator framework, Spring Batch has the `RepeatOperations` interface. The\n`RepeatOperations` interface has the following definition:\n\n[source, java]\n----\npublic interface RepeatOperations {\n\n RepeatStatus iterate(RepeatCallback callback) throws RepeatException;\n\n}\n----\n\nThe callback is an interface, shown in the following definition, that lets you insert\nsome business logic to be repeated:\n\n[source, java]\n----\npublic interface RepeatCallback {\n\n RepeatStatus doInIteration(RepeatContext context) throws Exception;\n\n}\n----\n\nThe callback is executed repeatedly until the implementation determines that the\niteration should end. The return value in these interfaces is an enumeration value that can\nbe either `RepeatStatus.CONTINUABLE` or `RepeatStatus.FINISHED`. A `RepeatStatus`\nenumeration conveys information to the caller of the repeat operations about whether\nany work remains. Generally speaking, implementations of `RepeatOperations`\nshould inspect `RepeatStatus` and use it as part of the decision to end the\niteration. Any callback that wishes to signal to the caller that there is no work remains\ncan return `RepeatStatus.FINISHED`.\n\nThe simplest general purpose implementation of `RepeatOperations` is `RepeatTemplate`:\n\n[source, java]\n----\nRepeatTemplate template = new RepeatTemplate();\n\ntemplate.setCompletionPolicy(new SimpleCompletionPolicy(2));\n\ntemplate.iterate(new RepeatCallback() {\n\n public RepeatStatus doInIteration(RepeatContext context) {\n // Do stuff in batch...\n return RepeatStatus.CONTINUABLE;\n }\n\n});\n----\n\nIn the preceding example, we return `RepeatStatus.CONTINUABLE`, to show that there is\nmore work to do. The callback can also return `RepeatStatus.FINISHED`, to signal to the\ncaller that there is no work remains. Some iterations can be terminated by\nconsiderations intrinsic to the work being done in the callback. Others are effectively\ninfinite loops (as far as the callback is concerned), and the completion decision is\ndelegated to an external policy, as in the case shown in the preceding example.\n\n[[repeatcontext]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/repeat.adoc", "title": "repeat", "heading": "RepeatTemplate", "heading_level": 2, "file_order": 54, "section_index": 1, "content_hash": "3b5af15e73daf844c45ccf4a660bb31817f8a7b00dad988116ce68de3dd92cb1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/repeat.adoc"}}
{"id": "sha256:147840e6a5d91f79230c50dfdb7eff3a8d07aacb36307e924c8656b02c4ba48b", "content": "The method parameter for the `RepeatCallback` is a `RepeatContext`. Many callbacks ignore\nthe context. However, if necessary, you can use it as an attribute bag to store transient\ndata for the duration of the iteration. After the `iterate` method returns, the context\nno longer exists.\n\nIf there is a nested iteration in progress, a `RepeatContext` has a parent context. The\nparent context is occasionally useful for storing data that need to be shared between\ncalls to `iterate`. This is the case, for instance, if you want to count the number of\noccurrences of an event in the iteration and remember it across subsequent calls.\n\n[[repeatStatus]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/repeat.adoc", "title": "repeat", "heading": "RepeatContext", "heading_level": 3, "file_order": 54, "section_index": 2, "content_hash": "147840e6a5d91f79230c50dfdb7eff3a8d07aacb36307e924c8656b02c4ba48b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/repeat.adoc"}}
{"id": "sha256:fc877375b6a3647797e8b54cc1740a7e5e29831704fee042e325bff6f8ea008a", "content": "`RepeatStatus` is an enumeration used by Spring Batch to indicate whether processing has\nfinished. It has two possible `RepeatStatus` values:\n\n.RepeatStatus Properties\n\n|===============\n|__Value__|__Description__\n|`CONTINUABLE`|There is more work to do.\n|`FINISHED`|No more repetitions should take place.\n\n|===============\n\nYou can combine `RepeatStatus` values with a logical AND operation by using the\n`and()` method in `RepeatStatus`. The effect of this is to do a logical AND on the\ncontinuable flag. In other words, if either status is `FINISHED`, the result is\n`FINISHED`.\n\n[[completionPolicies]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/repeat.adoc", "title": "repeat", "heading": "RepeatStatus", "heading_level": 3, "file_order": 54, "section_index": 3, "content_hash": "fc877375b6a3647797e8b54cc1740a7e5e29831704fee042e325bff6f8ea008a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/repeat.adoc"}}
{"id": "sha256:26b171b45383b44d89991535fe2b564a2bda679fb00cf5628d277d8cc808428e", "content": "Inside a `RepeatTemplate`, the termination of the loop in the `iterate` method is\ndetermined by a `CompletionPolicy`, which is also a factory for the `RepeatContext`. The\n`RepeatTemplate` has the responsibility to use the current policy to create a\n`RepeatContext` and pass that in to the `RepeatCallback` at every stage in the iteration.\nAfter a callback completes its `doInIteration`, the `RepeatTemplate` has to make a call\nto the `CompletionPolicy` to ask it to update its state (which will be stored in the\n`RepeatContext`). Then it asks the policy if the iteration is complete.\n\nSpring Batch provides some simple general purpose implementations of `CompletionPolicy`.\n`SimpleCompletionPolicy` allows execution up to a fixed number of times (with\n`RepeatStatus.FINISHED` forcing early completion at any time).\n\nUsers might need to implement their own completion policies for more complicated\ndecisions. For example, a batch processing window that prevents batch jobs from executing\nonce the online systems are in use would require a custom policy.\n\n[[repeatExceptionHandling]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/repeat.adoc", "title": "repeat", "heading": "Completion Policies", "heading_level": 2, "file_order": 54, "section_index": 4, "content_hash": "26b171b45383b44d89991535fe2b564a2bda679fb00cf5628d277d8cc808428e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/repeat.adoc"}}
{"id": "sha256:dd180ae074d10fda190d10f366bc4d20b66383e227599041720e41fc4fe8f4ce", "content": "If there is an exception thrown inside a `RepeatCallback`, the `RepeatTemplate` consults\nan `ExceptionHandler`, which can decide whether or not to re-throw the exception.\n\nThe following listing shows the `ExceptionHandler` interface definition:\n\n[source, java]\n----\npublic interface ExceptionHandler {\n\n void handleException(RepeatContext context, Throwable throwable)\n throws Throwable;\n\n}\n----\n\nA common use case is to count the number of exceptions of a given type and fail when a\nlimit is reached. For this purpose, Spring Batch provides the\n`SimpleLimitExceptionHandler` and a slightly more flexible\n`RethrowOnThresholdExceptionHandler`. The `SimpleLimitExceptionHandler` has a limit\nproperty and an exception type that should be compared with the current exception. All\nsubclasses of the provided type are also counted. Exceptions of the given type are\nignored until the limit is reached, and then they are rethrown. Exceptions of other types\nare always rethrown.\n\nAn important optional property of the `SimpleLimitExceptionHandler` is the boolean flag\ncalled `useParent`. It is `false` by default, so the limit is only accounted for in the\ncurrent `RepeatContext`. When set to `true`, the limit is kept across sibling contexts in\na nested iteration (such as a set of chunks inside a step).\n\n[[repeatListeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/repeat.adoc", "title": "repeat", "heading": "Exception Handling", "heading_level": 2, "file_order": 54, "section_index": 5, "content_hash": "dd180ae074d10fda190d10f366bc4d20b66383e227599041720e41fc4fe8f4ce", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/repeat.adoc"}}
{"id": "sha256:e0157cab1d6a5229a9c895ae7c265d713c0fb2c034a5a923da30106a7a0c268d", "content": "Often, it is useful to be able to receive additional callbacks for cross-cutting concerns\nacross a number of different iterations. For this purpose, Spring Batch provides the\n`RepeatListener` interface. The `RepeatTemplate` lets users register `RepeatListener`\nimplementations, and they are given callbacks with the `RepeatContext` and `RepeatStatus`\nwhere available during the iteration.\n\nThe `RepeatListener` interface has the following definition:\n\n[source, java]\n----\npublic interface RepeatListener {\n void before(RepeatContext context);\n void after(RepeatContext context, RepeatStatus result);\n void open(RepeatContext context);\n void onError(RepeatContext context, Throwable e);\n void close(RepeatContext context);\n}\n----\n\nThe `open` and `close` callbacks come before and after the entire iteration. `before`,\n`after`, and `onError` apply to the individual `RepeatCallback` calls.\n\nNote that, when there is more than one listener, they are in a list, so there is an\norder. In this case, `open` and `before` are called in the same order while `after`,\n`onError`, and `close` are called in reverse order.\n\n[[repeatParallelProcessing]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/repeat.adoc", "title": "repeat", "heading": "Listeners", "heading_level": 2, "file_order": 54, "section_index": 6, "content_hash": "e0157cab1d6a5229a9c895ae7c265d713c0fb2c034a5a923da30106a7a0c268d", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/repeat.adoc"}}
{"id": "sha256:00eac3029eca4b3b475853bfd5aec701e2b6a784904c32c99721d68433a9cae6", "content": "Implementations of `RepeatOperations` are not restricted to executing the callback\nsequentially. It is quite important that some implementations are able to execute their\ncallbacks in parallel. To this end, Spring Batch provides the\n`TaskExecutorRepeatTemplate`, which uses the Spring `TaskExecutor` strategy to run the\n`RepeatCallback`. The default is to use a `SynchronousTaskExecutor`, which has the effect\nof executing the whole iteration in the same thread (the same as a normal\n`RepeatTemplate`).\n\n[[declarativeIteration]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/repeat.adoc", "title": "repeat", "heading": "Parallel Processing", "heading_level": 2, "file_order": 54, "section_index": 7, "content_hash": "00eac3029eca4b3b475853bfd5aec701e2b6a784904c32c99721d68433a9cae6", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/repeat.adoc"}}
{"id": "sha256:2f83142bc083e717d3d3f1272839568d91065482805ca91495c2f2bea77a1a63", "content": "Sometimes, there is some business processing that you know you want to repeat every time\nit happens. The classic example of this is the optimization of a message pipeline.\nIf a batch of messages arrives frequently, it is more efficient to process them than to\nbear the cost of a separate transaction for every message. Spring Batch provides an AOP\ninterceptor that wraps a method call in a `RepeatOperations` object for this\npurpose. The `RepeatOperationsInterceptor` executes the intercepted method and repeats\naccording to the `CompletionPolicy` in the provided `RepeatTemplate`.\n\n[tabs]\n====\nJava::\n+\nThe following example uses Java configuration to\nrepeat a service call to a method called `processMessage` (for more detail on how to\nconfigure AOP interceptors, see the\nhttps://docs.spring.io/spring-framework/docs/current/reference/html/core.html#aop[Spring User Guide]):\n+\n[source, java]\n----\n@Bean\npublic MyService myService() {\n\tProxyFactory factory = new ProxyFactory(RepeatOperations.class.getClassLoader());\n\tfactory.setInterfaces(MyService.class);\n\tfactory.setTarget(new MyService());\n\n\tMyService service = (MyService) factory.getProxy();\n\tJdkRegexpMethodPointcut pointcut = new JdkRegexpMethodPointcut();\n\tpointcut.setPatterns(\".*processMessage.*\");\n\n\tRepeatOperationsInterceptor interceptor = new RepeatOperationsInterceptor();\n\n\t((Advised) service).addAdvisor(new DefaultPointcutAdvisor(pointcut, interceptor));\n\n\treturn service;\n}\n----\n\nXML::\n+\nThe following example shows declarative iteration that uses the Spring AOP namespace to\nrepeat a service call to a method called `processMessage` (for more detail on how to\nconfigure AOP interceptors, see the\nhttps://docs.spring.io/spring-framework/docs/current/reference/html/core.html#aop[Spring User Guide]):\n+\n[source, xml]\n----\n<aop:config>\n <aop:pointcut id=\"transactional\"\n expression=\"execution(* com..*Service.processMessage(..))\" />\n <aop:advisor pointcut-ref=\"transactional\"\n advice-ref=\"retryAdvice\" order=\"-1\"/>\n</aop:config>\n\n<bean id=\"retryAdvice\" class=\"org.spr...RepeatOperationsInterceptor\"/>\n----\n====\n\nThe preceding example uses a default `RepeatTemplate` inside the interceptor. To change\nthe policies, listeners, and other details, you can inject an instance of\n`RepeatTemplate` into the interceptor.\n\nIf the intercepted method returns `void`, the interceptor always returns\n`RepeatStatus.CONTINUABLE` (so there is a danger of an infinite loop if the\n`CompletionPolicy` does not have a finite end point). Otherwise, it returns\n`RepeatStatus.CONTINUABLE` until the return value from the intercepted method is `null`.\nAt that point, it returns `RepeatStatus.FINISHED`. Consequently, the business logic\ninside the target method can signal that there is no more work to do by returning `null`\nor by throwing an exception that is rethrown by the `ExceptionHandler` in the provided\n`RepeatTemplate`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/repeat.adoc", "title": "repeat", "heading": "Declarative Iteration", "heading_level": 2, "file_order": 54, "section_index": 8, "content_hash": "2f83142bc083e717d3d3f1272839568d91065482805ca91495c2f2bea77a1a63", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/repeat.adoc"}}
{"id": "sha256:95008db69713e4c0104f4aba951e79c6778a95c036024dbd725a81d7c0d04751", "content": "[[retry]]\n\n[[retry]]\n\nTo make processing more robust and less prone to failure, it sometimes helps to\nautomatically retry a failed operation in case it might succeed on a subsequent attempt.\nErrors that are susceptible to intermittent failure are often transient in nature.\nExamples include remote calls to a web service that fails because of a network glitch or a\n`DeadlockLoserDataAccessException` in a database update.\n\n[NOTE]\n====\nAs of v6.0, Spring Batch does *not* use https://github.com/spring-projects/spring-retry[Spring Retry] to automate retry operations within the framework,\nand is now based on the https://docs.spring.io/spring-framework/reference/7.0/core/resilience.html[core retry feature] provided by Spring Framework 7.0.\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/retry.adoc", "title": "retry", "heading": "retry", "heading_level": 1, "file_order": 55, "section_index": 0, "content_hash": "95008db69713e4c0104f4aba951e79c6778a95c036024dbd725a81d7c0d04751", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/retry.adoc"}}
{"id": "sha256:6ca83e221c56ff6abb2cb357a1f483fa97c31a1e304dbc8b41aef88f9abdbcb6", "content": "[[scalability]]\n\n[[scaling-and-parallel-processing]]\n\nMany batch processing problems can be solved with single-threaded, single-process jobs,\nso it is always a good idea to properly check if that meets your needs before thinking\nabout more complex implementations. Measure the performance of a realistic job and see if\nthe simplest implementation meets your needs first. You can read and write a file of\nseveral hundred megabytes in well under a minute, even with standard hardware.\n\nWhen you are ready to start implementing a job with some parallel processing, Spring\nBatch offers a range of options, which are described in this chapter, although some\nfeatures are covered elsewhere. At a high level, there are two modes of parallel\nprocessing:\n\n* Single-process, multi-threaded\n* Multi-process\n\nThese break down into categories as well, as follows:\n\n* Multi-threaded Step (single-process)\n* Parallel Steps (single-process)\n* Local Chunking of Step (single-process)\n* Remote Chunking of Step (multi-process)\n* Partitioning a Step (single or multi-process)\n* Remote Step (multi-process)\n\nFirst, we review the single-process options. Then we review the multi-process options.\n\n[[multithreadedStep]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "scalability", "heading_level": 1, "file_order": 56, "section_index": 0, "content_hash": "6ca83e221c56ff6abb2cb357a1f483fa97c31a1e304dbc8b41aef88f9abdbcb6", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:de72a7c8fefcb92b7c77c831def0dc31e77606a88750ba9f85dee14432c2fc7e", "content": "The simplest way to start parallel processing is to add a `TaskExecutor` to your Step\nconfiguration.\n\n[tabs]\n====\nJava::\n+\nWhen using Java configuration, you can add a `TaskExecutor` to the step,\nas the following example shows:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic TaskExecutor taskExecutor() {\n return new SimpleAsyncTaskExecutor(\"spring_batch\");\n}\n\n@Bean\npublic Step sampleStep(TaskExecutor taskExecutor, JobRepository jobRepository, PlatformTransactionManager transactionManager) {\n\treturn new StepBuilder(\"sampleStep\", jobRepository)\n .<String, String>chunk(10).transactionManager(transactionManager)\n .reader(itemReader())\n .processor(itemProcessor())\n .writer(itemWriter())\n .taskExecutor(taskExecutor)\n .build();\n}\n----\n\nXML::\n+\nFor example, you might add an attribute TO the `tasklet`, as follows:\n+\n[source, xml]\n----\n<step id=\"loading\">\n <tasklet task-executor=\"taskExecutor\">...</tasklet>\n</step>\n----\n\n====\n\nIn this example, the `taskExecutor` is a reference to another bean definition that\nimplements the `TaskExecutor` interface.\nhttps://docs.spring.io/spring/docs/current/javadoc-api/org/springframework/core/task/TaskExecutor.html[`TaskExecutor`]\nis a standard Spring interface, so consult the Spring User Guide for details of available\nimplementations. The simplest multi-threaded `TaskExecutor` is a\n`SimpleAsyncTaskExecutor`.\n\nThe result of the preceding configuration is that the `Step` will use multiple threads from\nthe task executor to process items concurrently. Therefore, the `ItemProcessor` will be\ncalled from multiple threads at the same time. This means that the `ItemProcessor` must be\nthread-safe. If you are using stateful components in the processing, you must ensure that\nthey are properly synchronized for concurrent access.\n\nThe reading and writing of items is still done in serial by the main thread executing the step,\nso the `ItemReader` and `ItemWriter` do not have to be thread-safe or synchronized. However,\nthe throughput of the step may be limited by the speed of reading and writing. If this is\nthe case, consider using a different concurrency technique, such as local chunking or local partitioning.\n\nNote also that there may be limits placed on concurrency by any pooled resources used in\nyour step, such as a `DataSource`. Be sure to make the pool in those resources at least\nas large as the desired number of concurrent threads in the step.\n\n[[scalabilityParallelSteps]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "Multi-threaded Step", "heading_level": 2, "file_order": 56, "section_index": 1, "content_hash": "de72a7c8fefcb92b7c77c831def0dc31e77606a88750ba9f85dee14432c2fc7e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:cbd5c3024f1e731774f3c016d608f0657be1edf560790cfcd5c8f1e5ae2f29c7", "content": "As long as the application logic that needs to be parallelized can be split into distinct\nresponsibilities and assigned to individual steps, it can be parallelized in a\nsingle process. Parallel Step execution is easy to configure and use.\n\n[tabs]\n====\nJava::\n+\nWhen using Java configuration, executing steps `(step1,step2)` in parallel with `step3`\nis straightforward, as follows:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Job job(JobRepository jobRepository) {\n return new JobBuilder(\"job\", jobRepository)\n .start(splitFlow())\n .next(step4())\n .build() //builds FlowJobBuilder instance\n .build(); //builds Job instance\n}\n\n@Bean\npublic Flow splitFlow() {\n return new FlowBuilder<SimpleFlow>(\"splitFlow\")\n .split(taskExecutor())\n .add(flow1(), flow2())\n .build();\n}\n\n@Bean\npublic Flow flow1() {\n return new FlowBuilder<SimpleFlow>(\"flow1\")\n .start(step1())\n .next(step2())\n .build();\n}\n\n@Bean\npublic Flow flow2() {\n return new FlowBuilder<SimpleFlow>(\"flow2\")\n .start(step3())\n .build();\n}\n\n@Bean\npublic TaskExecutor taskExecutor() {\n return new SimpleAsyncTaskExecutor(\"spring_batch\");\n}\n----\n\nXML::\n+\nFor example, executing steps `(step1,step2)` in parallel with `step3` is straightforward,\nas follows:\n+\n[source, xml]\n----\n<job id=\"job1\">\n <split id=\"split1\" task-executor=\"taskExecutor\" next=\"step4\">\n <flow>\n <step id=\"step1\" parent=\"s1\" next=\"step2\"/>\n <step id=\"step2\" parent=\"s2\"/>\n </flow>\n <flow>\n <step id=\"step3\" parent=\"s3\"/>\n </flow>\n </split>\n <step id=\"step4\" parent=\"s4\"/>\n</job>\n\n<beans:bean id=\"taskExecutor\" class=\"org.spr...SimpleAsyncTaskExecutor\"/>\n----\n\n====\n\nThe configurable task executor is used to specify which `TaskExecutor`\nimplementation should execute the individual flows. The default is\n`SyncTaskExecutor`, but an asynchronous `TaskExecutor` is required to run the steps in\nparallel. Note that the job ensures that every flow in the split completes before\naggregating the exit statuses and transitioning.\n\nSee the section on xref:step/controlling-flow.adoc#split-flows[Split Flows] for more detail.\n\n[[localChunking]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "Parallel Steps", "heading_level": 2, "file_order": 56, "section_index": 2, "content_hash": "cbd5c3024f1e731774f3c016d608f0657be1edf560790cfcd5c8f1e5ae2f29c7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:9b69b37258c2bcb47f75ae32633ac8df61a2672dd23cd1446ed5bdb57d7613e1", "content": "Local chunking is a new feature in v6.0 that allows you to process chunks of items in parallel, locally within the same JVM using multiple threads.\nThis is particularly useful when you have a large number of items to process and want to take advantage of multi-core processors.\nWith local chunking, you can configure a chunk-oriented step to use multiple threads to process chunks of items concurrently.\nEach thread will read, process and write its own chunk of items independently, while the step will manage the overall execution and commit the results.\n\nThis feature is possible by using the `ChunkMessageChannelItemWriter`, which is an item writer that submits chunk\nrequests to local workers from a `TaskExecutor`:\n\n[source, java]\n----\n@Bean\npublic ChunkTaskExecutorItemWriter<Vet> itemWriter(ChunkProcessor<Vet> chunkProcessor) {\n ThreadPoolTaskExecutor taskExecutor = new ThreadPoolTaskExecutor();\n taskExecutor.setCorePoolSize(4);\n taskExecutor.setThreadNamePrefix(\"worker-thread-\");\n taskExecutor.setWaitForTasksToCompleteOnShutdown(true);\n taskExecutor.afterPropertiesSet();\n return new ChunkTaskExecutorItemWriter<>(chunkProcessor, taskExecutor);\n}\n----\n\nThe `ChunkMessageChannelItemWriter` requires a `TaskExecutor` to process chunk concurrently\nas well as a `ChunkProcessor` to define what to do with each chunk. Here is an example of\na chunk processor that writes each chunk of items to a relational database table:\n\n[source, java]\n----\n@Bean\npublic ChunkProcessor<Vet> chunkProcessor(DataSource dataSource, TransactionTemplate transactionTemplate) {\n String sql = \"insert into vets (firstname, lastname) values (?, ?)\";\n JdbcBatchItemWriter<Vet> itemWriter = new JdbcBatchItemWriterBuilder<Vet>().dataSource(dataSource)\n .sql(sql)\n .itemPreparedStatementSetter((item, ps) -> {\n ps.setString(1, item.firstname());\n ps.setString(2, item.lastname());\n })\n .build();\n\n return (chunk, contribution) -> transactionTemplate.executeWithoutResult(transactionStatus -> {\n try {\n itemWriter.write(chunk);\n contribution.incrementWriteCount(chunk.size());\n contribution.setExitStatus(ExitStatus.COMPLETED);\n }\n catch (Exception e) {\n transactionStatus.setRollbackOnly();\n contribution.setExitStatus(ExitStatus.FAILED.addExitDescription(e));\n }\n });\n}\n----\n\nYou can find an example of this scaling technique in the https://github.com/spring-projects/spring-batch/tree/main/spring-batch-samples/src/main/java/org/springframework/batch/samples/chunking/local[Local Chunking Sample].\n\n[[remoteChunking]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "Local Chunking", "heading_level": 2, "file_order": 56, "section_index": 3, "content_hash": "9b69b37258c2bcb47f75ae32633ac8df61a2672dd23cd1446ed5bdb57d7613e1", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:7908f9f67e0c5c7dcdf40908619936fb8575048ce38a6da9d173171aa6f79018", "content": "In remote chunking, the `Step` processing is split across multiple processes,\ncommunicating with each other through some middleware. The following image shows the\npattern:\n\n.Remote Chunking\nimage::remote-chunking.png[Remote Chunking, scaledwidth=\"60%\"]\n\nThe manager component is a single process, and the workers are multiple remote processes.\nThis pattern works best if the manager is not a bottleneck, so the processing must be more\nexpensive than the reading of items (as is often the case in practice).\n\nThe manager is an implementation of a Spring Batch `Step` with the `ItemWriter` replaced\nby a generic version that knows how to send chunks of items to the middleware as\nmessages. The workers are standard listeners for whatever middleware is being used (for\nexample, with JMS, they would be `MesssageListener` implementations), and their role is\nto process the chunks of items by using a standard `ItemWriter` or `ItemProcessor` plus an\n`ItemWriter`, through the `ChunkProcessor` interface. One of the advantages of using this\npattern is that the reader, processor, and writer components are off-the-shelf (the same\nas would be used for a local execution of the step). The items are divided up dynamically,\nand work is shared through the middleware, so that, if the listeners are all eager\nconsumers, load balancing is automatic.\n\nThe middleware has to be durable, with guaranteed delivery and a single consumer for each\nmessage. JMS is the obvious candidate, but other options (such as JavaSpaces) exist in\nthe grid computing and shared memory product space.\n\nSee the section on\nxref:spring-batch-integration/externalizing-execution.adoc#remote-chunking[Spring Batch Integration - Remote Chunking]\nfor more detail.\n\n[[partitioning]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "Remote Chunking", "heading_level": 2, "file_order": 56, "section_index": 4, "content_hash": "7908f9f67e0c5c7dcdf40908619936fb8575048ce38a6da9d173171aa6f79018", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:df64e471468ac6467a8520a46345d5fe808f0553e6057e3f8d89e1cfd44c02af", "content": "Spring Batch also provides an SPI for partitioning a `Step` execution and executing it\nremotely. In this case, the remote participants are `Step` instances that could just as\neasily have been configured and used for local processing. The following image shows the\npattern:\n\n.Partitioning\nimage::partitioning-overview.png[Partitioning Overview, scaledwidth=\"60%\"]\n\nThe `Job` runs on the left-hand side as a sequence of `Step` instances, and one of the\n`Step` instances is labeled as a manager. The workers in this picture are all identical\ninstances of a `Step`, which could in fact take the place of the manager, resulting in the\nsame outcome for the `Job`. The workers are typically going to be remote services but\ncould also be local threads of execution. The messages sent by the manager to the workers\nin this pattern do not need to be durable or have guaranteed delivery. Spring Batch\nmetadata in the `JobRepository` ensures that each worker is executed once and only once for\neach `Job` execution.\n\nThe SPI in Spring Batch consists of a special implementation of `Step` (called the\n`PartitionStep`) and two strategy interfaces that need to be implemented for the specific\nenvironment. The strategy interfaces are `PartitionHandler` and `StepExecutionSplitter`,\nand the following sequence diagram shows their role:\n\n.Partitioning SPI\nimage::partitioning-spi.png[Partitioning SPI, scaledwidth=\"60%\"]\n\nThe `Step` on the right in this case is the \"`remote`\" worker, so, potentially, there are\nmany objects and or processes playing this role, and the `PartitionStep` is shown driving\nthe execution.\n\n[tabs]\n====\nJava::\n+\nThe following example shows the `PartitionStep` configuration when using Java\nconfiguration:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step step1Manager(JobRepository jobRepository) {\n return new StepBuilder(\"step1.manager\", jobRepository)\n .<String, String>partitioner(\"step1\", partitioner())\n .step(step1())\n .gridSize(10)\n .taskExecutor(taskExecutor())\n .build();\n}\n----\n+\nSimilar to the multi-threaded step's `throttleLimit` method, the `gridSize`\nmethod prevents the task executor from being saturated with requests from a single\nstep.\n\nXML::\n+\nThe following example shows the `PartitionStep` configuration when using XML\nconfiguration:\n+\n[source, xml]\n----\n<step id=\"step1.manager\">\n <partition step=\"step1\" partitioner=\"partitioner\">\n <handler grid-size=\"10\" task-executor=\"taskExecutor\"/>\n </partition>\n</step>\n----\n+\nSimilar to the multi-threaded step's `throttle-limit` attribute, the `grid-size`\nattribute prevents the task executor from being saturated with requests from a single\nstep.\n\n====\n\nThe unit test suite for\nhttps://github.com/spring-projects/spring-batch/tree/main/spring-batch-samples/src/main/resources/jobs[Spring\nBatch Samples] (see `partition*Job.xml` configuration) has a simple example that you can copy and extend.\n\nSpring Batch creates step executions for the partition called `step1:partition0` and so\non. Many people prefer to call the manager step `step1:manager` for consistency. You can\nuse an alias for the step (by specifying the `name` attribute instead of the `id`\nattribute).\n\n[[partitionHandler]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "Partitioning", "heading_level": 2, "file_order": 56, "section_index": 5, "content_hash": "df64e471468ac6467a8520a46345d5fe808f0553e6057e3f8d89e1cfd44c02af", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:7010e51677573efa269dc2754fbd3f8bdc6600ae4456783ec9c62bc3cf8e8af7", "content": "`PartitionHandler` is the component that knows about the fabric of the remoting or\ngrid environment. It is able to send `StepExecution` requests to the remote `Step`\ninstances, wrapped in some fabric-specific format, like a DTO. It does not have to know\nhow to split the input data or how to aggregate the result of multiple `Step` executions.\nGenerally speaking, it probably also does not need to know about resilience or failover,\nsince those are features of the fabric in many cases. In any case, Spring Batch always\nprovides restartability independent of the fabric. A failed `Job` can always be restarted,\nand, in that case, only the failed `Steps` are re-executed.\n\nThe `PartitionHandler` interface can have specialized implementations for a variety of\nfabric types, including simple RMI remoting, EJB remoting, custom web service, JMS, Java\nSpaces, shared memory grids (such as Terracotta or Coherence), and grid execution fabrics\n(such as GridGain). Spring Batch does not contain implementations for any proprietary grid\nor remoting fabrics.\n\nSpring Batch does, however, provide a useful implementation of `PartitionHandler` that\nexecutes `Step` instances locally in separate threads of execution, using the\n`TaskExecutor` strategy from Spring. The implementation is called\n`TaskExecutorPartitionHandler`.\n\n[tabs]\n====\nJava::\n+\nYou can explicitly configure the `TaskExecutorPartitionHandler` with Java configuration,\nas follows:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic Step step1Manager(JobRepository jobRepository) {\n return new StepBuilder(\"step1.manager\", jobRepository)\n .partitioner(\"step1\", partitioner())\n .partitionHandler(partitionHandler())\n .build();\n}\n\n@Bean\npublic PartitionHandler partitionHandler() {\n TaskExecutorPartitionHandler retVal = new TaskExecutorPartitionHandler();\n retVal.setTaskExecutor(taskExecutor());\n retVal.setStep(step1());\n retVal.setGridSize(10);\n return retVal;\n}\n----\n\nXML::\n+\nThe `TaskExecutorPartitionHandler` is the default for a step configured with the XML\nnamespace shown previously. You can also configure it explicitly, as follows:\n+\n[source, xml]\n----\n<step id=\"step1.manager\">\n <partition step=\"step1\" handler=\"handler\"/>\n</step>\n\n<bean class=\"org.spr...TaskExecutorPartitionHandler\">\n <property name=\"taskExecutor\" ref=\"taskExecutor\"/>\n <property name=\"step\" ref=\"step1\" />\n <property name=\"gridSize\" value=\"10\" />\n</bean>\n----\n====\n\nThe `gridSize` attribute determines the number of separate step executions to create, so\nit can be matched to the size of the thread pool in the `TaskExecutor`. Alternatively, it\ncan be set to be larger than the number of threads available, which makes the blocks of\nwork smaller.\n\nThe `TaskExecutorPartitionHandler` is useful for IO-intensive `Step` instances, such as\ncopying large numbers of files or replicating filesystems into content management\nsystems. It can also be used for remote execution by providing a `Step` implementation\nthat is a proxy for a remote invocation (such as using Spring Remoting).\n\n[[partitioner]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "PartitionHandler", "heading_level": 3, "file_order": 56, "section_index": 6, "content_hash": "7010e51677573efa269dc2754fbd3f8bdc6600ae4456783ec9c62bc3cf8e8af7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:a8d9a26bddc886ac6a8d6079be3bf8108389a6c11f4d4646c684bbc8327c5bc0", "content": "The `Partitioner` has a simpler responsibility: to generate execution contexts as input\nparameters for new step executions only (no need to worry about restarts). It has a\nsingle method, as the following interface definition shows:\n\n[source, java]\n----\npublic interface Partitioner {\n Map<String, ExecutionContext> partition(int gridSize);\n}\n----\n\nThe return value from this method associates a unique name for each step execution (the\n`String`) with input parameters in the form of an `ExecutionContext`. The names show up\nlater in the Batch metadata as the step name in the partitioned `StepExecutions`. The\n`ExecutionContext` is just a bag of name-value pairs, so it might contain a range of\nprimary keys, line numbers, or the location of an input file. The remote `Step` then\nnormally binds to the context input by using `#{...}` placeholders (late binding in step\nscope), as shown in the next section.\n\nThe names of the step executions (the keys in the `Map` returned by `Partitioner`) need\nto be unique amongst the step executions of a `Job` but do not have any other specific\nrequirements. The easiest way to do this (and to make the names meaningful for users) is\nto use a prefix+suffix naming convention, where the prefix is the name of the step that\nis being executed (which itself is unique in the `Job`) and the suffix is just a\ncounter. There is a `SimplePartitioner` in the framework that uses this convention.\n\nYou can use an optional interface called `PartitionNameProvider` to provide the partition\nnames separately from the partitions themselves. If a `Partitioner` implements this\ninterface, only the names are queried on a restart. If partitioning is expensive,\nthis can be a useful optimization. The names provided by the `PartitionNameProvider` must\nmatch those provided by the `Partitioner`.\n\n[[bindingInputDataToSteps]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "Partitioner", "heading_level": 3, "file_order": 56, "section_index": 7, "content_hash": "a8d9a26bddc886ac6a8d6079be3bf8108389a6c11f4d4646c684bbc8327c5bc0", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:cc9efbe76414dc799a782dc410105f71f26b81fde9bdc9f250b39789174954d3", "content": "It is very efficient for the steps that are executed by the `PartitionHandler` to have\nidentical configuration and for their input parameters to be bound at runtime from the\n`ExecutionContext`. This is easy to do with the StepScope feature of Spring Batch\n(covered in more detail in the section on xref:step/late-binding.adoc[Late Binding]). For\nexample, if the `Partitioner` creates `ExecutionContext` instances with an attribute key\ncalled `fileName`, pointing to a different file (or directory) for each step invocation,\nthe `Partitioner` output might resemble the content of the following table:\n\n.Example step execution name to execution context provided by `Partitioner` targeting directory processing\n|===============\n|__Step Execution Name (key)__|__ExecutionContext (value)__\n|filecopy:partition0|fileName=/home/data/one\n|filecopy:partition1|fileName=/home/data/two\n|filecopy:partition2|fileName=/home/data/three\n|===============\n\nThen the file name can be bound to a step by using late binding to the execution context.\n\n[tabs]\n====\nJava::\n+\nThe following example shows how to define late binding in Java:\n+\n.Java Configuration\n[source, java]\n----\n@Bean\npublic MultiResourceItemReader itemReader(\n\t@Value(\"#{stepExecutionContext['fileName']}/*\") Resource [] resources) {\n\treturn new MultiResourceItemReaderBuilder<String>()\n .delegate(fileReader())\n .name(\"itemReader\")\n .resources(resources)\n .build();\n}\n----\n\nXML::\n+\nThe following example shows how to define late binding in XML:\n+\n.XML Configuration\n[source, xml]\n----\n<bean id=\"itemReader\" scope=\"step\"\n class=\"org.spr...MultiResourceItemReader\">\n <property name=\"resources\" value=\"#{stepExecutionContext[fileName]}/*\"/>\n</bean>\n----\n\n====\n\n[[remoteStep]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "Binding Input Data to Steps", "heading_level": 3, "file_order": 56, "section_index": 8, "content_hash": "cc9efbe76414dc799a782dc410105f71f26b81fde9bdc9f250b39789174954d3", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:33360232f8bbee9a833874e180c9f8f8812b9a483445e3d5b160abaa031208b0", "content": "As of v6.0, Spring Batch provides support for remote step executions, allowing you to execute steps of a batch job on remote machines or clusters.\nThis feature is particularly useful for large-scale batch processing scenarios where you want to distribute the workload across multiple nodes to improve performance and scalability.\nRemote step execution is provided by the `RemoteStep` class, which uses Spring Integration messaging channels to enable communication between the local job execution environment and the remote step executors.\n\nA `RemoteStep` is configured as a regular step by providing the remote step name and a messaging template to send step execution requests to remote workers:\n\n[source, java]\n----\n@Bean\npublic Step step(MessagingTemplate messagingTemplate, JobRepository jobRepository) {\n return new RemoteStep(\"step\", \"workerStep\", jobRepository, messagingTemplate);\n}\n----\n\nOn the worker side, you need to define the remote step to execute (`workerStep` in this example) and configure\na Spring Integration flow to intercept step execution requests and invoke the `StepExecutionRequestHandler`:\n\n[source, java]\n----\n@Bean\npublic Step workerStep(JobRepository jobRepository, JdbcTransactionManager transactionManager) {\n return new StepBuilder(\"workerStep\", jobRepository)\n // define step logic\n .build();\n}\n\n/*\n * Configure inbound flow (requests coming from the manager)\n */\n@Bean\npublic DirectChannel requests() {\n return new DirectChannel();\n}\n\n@Bean\npublic IntegrationFlow inboundFlow(ActiveMQConnectionFactory connectionFactory, JobRepository jobRepository,\n StepLocator stepLocator) {\n StepExecutionRequestHandler stepExecutionRequestHandler = new StepExecutionRequestHandler();\n stepExecutionRequestHandler.setJobRepository(jobRepository);\n stepExecutionRequestHandler.setStepLocator(stepLocator);\n return IntegrationFlow.from(Jms.messageDrivenChannelAdapter(connectionFactory).destination(\"requests\"))\n .channel(requests())\n .handle(stepExecutionRequestHandler, \"handle\")\n .get();\n}\n\n@Bean\npublic StepLocator stepLocator(BeanFactory beanFactory) {\n BeanFactoryStepLocator beanFactoryStepLocator = new BeanFactoryStepLocator();\n beanFactoryStepLocator.setBeanFactory(beanFactory);\n return beanFactoryStepLocator;\n}\n----\n\nYou can find a complete example in the https://github.com/spring-projects/spring-batch/tree/main/spring-batch-samples/src/main/java/org/springframework/batch/samples/remotestep[Remote Step Sample].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/scalability.adoc", "title": "scalability", "heading": "Remote Step execution", "heading_level": 2, "file_order": 56, "section_index": 9, "content_hash": "33360232f8bbee9a833874e180c9f8f8812b9a483445e3d5b160abaa031208b0", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/scalability.adoc"}}
{"id": "sha256:15ab4b605b3c894bd022d544be97df557cfcd56d76b56020cefd5c2acf635601", "content": "[[metaDataSchema]]\n[appendix]\n[[meta-data-schema]]\n\n[[metaDataSchemaOverview]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "schema-appendix", "heading_level": 1, "file_order": 57, "section_index": 0, "content_hash": "15ab4b605b3c894bd022d544be97df557cfcd56d76b56020cefd5c2acf635601", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:a7cd9f400059a2c4dd72f5752733d448e6300dcc69dfe8b7fc1fd67d3f52d0a2", "content": "The Spring Batch Metadata tables closely match the domain objects that represent them in\nJava. For example, `JobInstance`, `JobExecution`, `JobParameters`, and `StepExecution`\nmap to `BATCH_JOB_INSTANCE`, `BATCH_JOB_EXECUTION`, `BATCH_JOB_EXECUTION_PARAMS`, and\n`BATCH_STEP_EXECUTION`, respectively. `ExecutionContext` maps to both\n`BATCH_JOB_EXECUTION_CONTEXT` and `BATCH_STEP_EXECUTION_CONTEXT`. The `JobRepository` is\nresponsible for saving and storing each Java object into its correct table. This appendix\ndescribes the metadata tables in detail, along with many of the design decisions that\nwere made when creating them. When viewing the various table creation statements described\nlater in this appendix, note that the data types used are as generic as possible. Spring\nBatch provides many schemas as examples. All of them have varying data types, due to\nvariations in how individual database vendors handle data types. The following image\nshows an ERD model of all six tables and their relationships to one another:\n\n.Spring Batch Meta-Data ERD\nimage::meta-data-erd.png[Spring Batch Meta-Data ERD, scaledwidth=\"60%\"]\n\n[[exampleDDLScripts]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "Overview", "heading_level": 2, "file_order": 57, "section_index": 1, "content_hash": "a7cd9f400059a2c4dd72f5752733d448e6300dcc69dfe8b7fc1fd67d3f52d0a2", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:b9ec957dcb11fa5197ff92ee7ff2885380021ac1fb14c0e7ab064b5e57384f38", "content": "The Spring Batch Core JAR file contains example scripts to create the relational tables\nfor a number of database platforms (which are, in turn, auto-detected by the job\nrepository factory bean or namespace equivalent). These scripts can be used as is or\nmodified with additional indexes and constraints, as desired. The file names are in the\nform `schema-\\*.sql`, where `*` is the short name of the target database platform.\nThe scripts are in the package `org.springframework.batch.core`.\n\n[[migrationDDLScripts]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "Example DDL Scripts", "heading_level": 3, "file_order": 57, "section_index": 2, "content_hash": "b9ec957dcb11fa5197ff92ee7ff2885380021ac1fb14c0e7ab064b5e57384f38", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:9b00ce21a719f9e06f865a922e641811776a7c54cfc0a79c715ca962b5582035", "content": "Spring Batch provides migration DDL scripts that you need to execute when you upgrade versions.\nThese scripts can be found in the Core Jar file under `org/springframework/batch/core/migration`.\nMigration scripts are organized into folders corresponding to version numbers in which they were introduced:\n\n* `2.2`: Contains scripts you need to migrate from a version before `2.2` to version `2.2`\n* `4.1`: Contains scripts you need to migrate from a version before `4.1` to version `4.1`\n\n[[metaDataVersion]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "Migration DDL Scripts", "heading_level": 3, "file_order": 57, "section_index": 3, "content_hash": "9b00ce21a719f9e06f865a922e641811776a7c54cfc0a79c715ca962b5582035", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:3d7182170184dc6b412c425501fca22f8c7e51dbf238a7cc8231e1abe320a192", "content": "Many of the database tables discussed in this appendix contain a version column. This\ncolumn is important, because Spring Batch employs an optimistic locking strategy when\ndealing with updates to the database. This means that each time a record is \"`touched`\"\n(updated), the value in the version column is incremented by one. When the repository goes\nback to save the value, if the version number has changed, it throws an\n`OptimisticLockingFailureException`, indicating that there has been an error with concurrent\naccess. This check is necessary, since, even though different batch jobs may be running\nin different machines, they all use the same database tables.\n\n[[metaDataIdentity]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "Version", "heading_level": 3, "file_order": 57, "section_index": 4, "content_hash": "3d7182170184dc6b412c425501fca22f8c7e51dbf238a7cc8231e1abe320a192", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:8377b307e69fe2aa68a19b8336d4df9d9b15c8e755299f4b9bb6666aee37d44b", "content": "`BATCH_JOB_INSTANCE`, `BATCH_JOB_EXECUTION`, and `BATCH_STEP_EXECUTION` each contain\ncolumns ending in `_ID`. These fields act as primary keys for their respective tables.\nHowever, they are not database generated keys. Rather, they are generated by separate\nsequences. This is necessary because, after inserting one of the domain objects into the\ndatabase, the key it is given needs to be set on the actual object so that they can be\nuniquely identified in Java. Newer database drivers (JDBC 3.0 and up) support this\nfeature with database-generated keys. However, rather than require that feature,\nsequences are used. Each variation of the schema contains some form of the following\nstatements:\n\n[source, sql]\n----\nCREATE SEQUENCE BATCH_STEP_EXECUTION_SEQ;\nCREATE SEQUENCE BATCH_JOB_EXECUTION_SEQ;\nCREATE SEQUENCE BATCH_JOB_INSTANCE_SEQ;\n----\n\nMany database vendors do not support sequences. In these cases, work-arounds are used,\nsuch as the following statements for MySQL:\n\n[source, sql]\n----\nCREATE TABLE BATCH_STEP_EXECUTION_SEQ (ID BIGINT NOT NULL) type=InnoDB;\nINSERT INTO BATCH_STEP_EXECUTION_SEQ values(0);\nCREATE TABLE BATCH_JOB_EXECUTION_SEQ (ID BIGINT NOT NULL) type=InnoDB;\nINSERT INTO BATCH_JOB_EXECUTION_SEQ values(0);\nCREATE TABLE BATCH_JOB_INSTANCE_SEQ (ID BIGINT NOT NULL) type=InnoDB;\nINSERT INTO BATCH_JOB_INSTANCE_SEQ values(0);\n----\n\nIn the preceding case, a table is used in place of each sequence. The Spring core class,\n`MySQLMaxValueIncrementer`, then increments the one column in this sequence to\ngive similar functionality.\n\n[[metaDataBatchJobInstance]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "Identity", "heading_level": 3, "file_order": 57, "section_index": 5, "content_hash": "8377b307e69fe2aa68a19b8336d4df9d9b15c8e755299f4b9bb6666aee37d44b", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:f36964f2a6544f7f71a85a23da05c186e9a20d634bc73d585b1d272fb7aa0596", "content": "The `BATCH_JOB_INSTANCE` table holds all information relevant to a `JobInstance` and\nserves as the top of the overall hierarchy. The following generic DDL statement is used\nto create it:\n\n[source, sql]\n----\nCREATE TABLE BATCH_JOB_INSTANCE (\n JOB_INSTANCE_ID BIGINT PRIMARY KEY ,\n VERSION BIGINT,\n JOB_NAME VARCHAR(100) NOT NULL ,\n JOB_KEY VARCHAR(32) NOT NULL\n);\n----\n\nThe following list describes each column in the table:\n\n* `JOB_INSTANCE_ID`: The unique ID that identifies the instance. It is also the primary\nkey. The value of this column should be obtainable by calling the `getId` method on\n`JobInstance`.\n* `VERSION`: See xref:schema-appendix.adoc#metaDataVersion[Version].\n* `JOB_NAME`: Name of the job obtained from the `Job` object. Because it is required to\nidentify the instance, it must not be null.\n* `JOB_KEY`: A serialization of the `JobParameters` that uniquely identifies separate\ninstances of the same job from one another. (`JobInstances` with the same job name must\nhave different `JobParameters` and, thus, different `JOB_KEY` values).\n\n[[metaDataBatchJobParams]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "The `BATCH_JOB_INSTANCE` Table", "heading_level": 2, "file_order": 57, "section_index": 6, "content_hash": "f36964f2a6544f7f71a85a23da05c186e9a20d634bc73d585b1d272fb7aa0596", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:94486d7bfe5fa34a44039c34b34bc02ad8cbb50231bfbc0636169c4c8953e592", "content": "The `BATCH_JOB_EXECUTION_PARAMS` table holds all information relevant to the\n`JobParameters` object. It contains 0 or more key/value pairs passed to a `Job` and\nserves as a record of the parameters with which a job was run. For each parameter that\ncontributes to the generation of a job's identity, the `IDENTIFYING` flag is set to true.\nNote that the table has been denormalized. Rather than creating a separate table for each\ntype, there is one table with a column indicating the type, as the following\nlisting shows:\n\n[source, sql]\n----\nCREATE TABLE BATCH_JOB_EXECUTION_PARAMS (\n\tJOB_EXECUTION_ID BIGINT NOT NULL ,\n\tPARAMETER_NAME VARCHAR(100) NOT NULL ,\n\tPARAMETER_TYPE VARCHAR(100) NOT NULL ,\n\tPARAMETER_VALUE VARCHAR(2500) ,\n\tIDENTIFYING CHAR(1) NOT NULL ,\n\tconstraint JOB_EXEC_PARAMS_FK foreign key (JOB_EXECUTION_ID)\n\treferences BATCH_JOB_EXECUTION(JOB_EXECUTION_ID)\n);\n----\n\nThe following list describes each column:\n\n* `JOB_EXECUTION_ID`: Foreign key from the `BATCH_JOB_EXECUTION` table that indicates the\njob execution to which the parameter entry belongs. Note that multiple rows (that is,\nkey/value pairs) may exist for each execution.\n* PARAMETER_NAME: The parameter name.\n* PARAMETER_TYPE: The fully qualified name of the type of the parameter.\n* PARAMETER_VALUE: Parameter value\n* IDENTIFYING: Flag indicating whether the parameter contributed to the identity of the\nrelated `JobInstance`.\n\nNote that there is no primary key for this table. This is because the framework has no\nuse for one and, thus, does not require it. If need be, you can add a primary key\nwith a database generated key without causing any issues to the framework itself.\n\n[[metaDataBatchJobExecution]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "The `BATCH_JOB_EXECUTION_PARAMS` Table", "heading_level": 2, "file_order": 57, "section_index": 7, "content_hash": "94486d7bfe5fa34a44039c34b34bc02ad8cbb50231bfbc0636169c4c8953e592", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:b6afd70fdf1b42ddd6ee60bf5b6626f6b810be6124199d3c7bc75ce71be4aa7a", "content": "The `BATCH_JOB_EXECUTION` table holds all information relevant to the `JobExecution`\nobject. Every time a `Job` is run, there is always a new called `JobExecution` and a new row in\nthis table. The following listing shows the definition of the `BATCH_JOB_EXECUTION`\ntable:\n\n[source, sql]\n----\nCREATE TABLE BATCH_JOB_EXECUTION (\n JOB_EXECUTION_ID BIGINT PRIMARY KEY ,\n VERSION BIGINT,\n JOB_INSTANCE_ID BIGINT NOT NULL,\n CREATE_TIME TIMESTAMP NOT NULL,\n START_TIME TIMESTAMP DEFAULT NULL,\n END_TIME TIMESTAMP DEFAULT NULL,\n STATUS VARCHAR(10),\n EXIT_CODE VARCHAR(20),\n EXIT_MESSAGE VARCHAR(2500),\n LAST_UPDATED TIMESTAMP,\n constraint JOB_INSTANCE_EXECUTION_FK foreign key (JOB_INSTANCE_ID)\n references BATCH_JOB_INSTANCE(JOB_INSTANCE_ID)\n) ;\n----\n\nThe following list describes each column:\n\n* `JOB_EXECUTION_ID`: Primary key that uniquely identifies this execution. The value of\nthis column is obtainable by calling the `getId` method of the `JobExecution` object.\n* `VERSION`: See xref:schema-appendix.adoc#metaDataVersion[Version].\n* `JOB_INSTANCE_ID`: Foreign key from the `BATCH_JOB_INSTANCE` table. It indicates the\ninstance to which this execution belongs. There may be more than one execution per\ninstance.\n* `CREATE_TIME`: Timestamp representing the time when the execution was created.\n* `START_TIME`: Timestamp representing the time when the execution was started.\n* `END_TIME`: Timestamp representing the time when the execution finished, regardless of\nsuccess or failure. An empty value in this column when the job is not currently running\nindicates that there has been some type of error and the framework was unable to perform\na last save before failing.\n* `STATUS`: Character string representing the status of the execution. This may be\n`COMPLETED`, `STARTED`, and others. The object representation of this column is the\n`BatchStatus` enumeration.\n* `EXIT_CODE`: Character string representing the exit code of the execution. In the case\nof a command-line job, this may be converted into a number.\n* `EXIT_MESSAGE`: Character string representing a more detailed description of how the\njob exited. In the case of failure, this might include as much of the stack trace as is\npossible.\n* `LAST_UPDATED`: Timestamp representing the last time this execution was persisted.\n\n[[metaDataBatchStepExecution]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "The `BATCH_JOB_EXECUTION` Table", "heading_level": 2, "file_order": 57, "section_index": 8, "content_hash": "b6afd70fdf1b42ddd6ee60bf5b6626f6b810be6124199d3c7bc75ce71be4aa7a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:42a79eb1400bf343b13c00abef2095b767c43c834fead374d61c2a0da85a020c", "content": "The `BATCH_STEP_EXECUTION` table holds all information relevant to the `StepExecution`\nobject. This table is similar in many ways to the `BATCH_JOB_EXECUTION` table, and there\nis always at least one entry per `Step` for each `JobExecution` created. The following\nlisting shows the definition of the `BATCH_STEP_EXECUTION` table:\n\n[source, sql]\n----\nCREATE TABLE BATCH_STEP_EXECUTION (\n STEP_EXECUTION_ID BIGINT NOT NULL PRIMARY KEY ,\n VERSION BIGINT NOT NULL,\n STEP_NAME VARCHAR(100) NOT NULL,\n JOB_EXECUTION_ID BIGINT NOT NULL,\n CREATE_TIME TIMESTAMP NOT NULL,\n START_TIME TIMESTAMP DEFAULT NULL ,\n END_TIME TIMESTAMP DEFAULT NULL,\n STATUS VARCHAR(10),\n COMMIT_COUNT BIGINT ,\n READ_COUNT BIGINT ,\n FILTER_COUNT BIGINT ,\n WRITE_COUNT BIGINT ,\n READ_SKIP_COUNT BIGINT ,\n WRITE_SKIP_COUNT BIGINT ,\n PROCESS_SKIP_COUNT BIGINT ,\n ROLLBACK_COUNT BIGINT ,\n EXIT_CODE VARCHAR(20) ,\n EXIT_MESSAGE VARCHAR(2500) ,\n LAST_UPDATED TIMESTAMP,\n constraint JOB_EXECUTION_STEP_FK foreign key (JOB_EXECUTION_ID)\n references BATCH_JOB_EXECUTION(JOB_EXECUTION_ID)\n) ;\n----\n\nThe following list describes each column:\n\n* `STEP_EXECUTION_ID`: Primary key that uniquely identifies this execution. The value of\nthis column should be obtainable by calling the `getId` method of the `StepExecution`\nobject.\n* `VERSION`: See xref:schema-appendix.adoc#metaDataVersion[Version].\n* `STEP_NAME`: The name of the step to which this execution belongs.\n* `JOB_EXECUTION_ID`: Foreign key from the `BATCH_JOB_EXECUTION` table. It indicates the\n`JobExecution` to which this `StepExecution` belongs. There may be only one\n`StepExecution` for a given `JobExecution` for a given `Step` name.\n* `START_TIME`: Timestamp representing the time when the execution was started.\n* `END_TIME`: Timestamp representing the time the when execution was finished, regardless\nof success or failure. An empty value in this column, even though the job is not\ncurrently running, indicates that there has been some type of error and the framework was\nunable to perform a last save before failing.\n* `STATUS`: Character string representing the status of the execution. This may be\n`COMPLETED`, `STARTED`, and others. The object representation of this column is the\n`BatchStatus` enumeration.\n* `COMMIT_COUNT`: The number of times in which the step has committed a transaction\nduring this execution.\n* `READ_COUNT`: The number of items read during this execution.\n* `FILTER_COUNT`: The number of items filtered out of this execution.\n* `WRITE_COUNT`: The number of items written and committed during this execution.\n* `READ_SKIP_COUNT`: The number of items skipped on read during this execution.\n* `WRITE_SKIP_COUNT`: The number of items skipped on write during this execution.\n* `PROCESS_SKIP_COUNT`: The number of items skipped during processing during this\nexecution.\n* `ROLLBACK_COUNT`: The number of rollbacks during this execution. Note that this count\nincludes each time rollback occurs, including rollbacks for retry and those in the skip\nrecovery procedure.\n* `EXIT_CODE`: Character string representing the exit code of the execution. In the case\nof a command-line job, this may be converted into a number.\n* `EXIT_MESSAGE`: Character string representing a more detailed description of how the\njob exited. In the case of failure, this might include as much of the stack trace as is\npossible.\n* `LAST_UPDATED`: Timestamp representing the last time this execution was persisted.\n\n[[metaDataBatchJobExecutionContext]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "The `BATCH_STEP_EXECUTION` Table", "heading_level": 2, "file_order": 57, "section_index": 9, "content_hash": "42a79eb1400bf343b13c00abef2095b767c43c834fead374d61c2a0da85a020c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:ad64d23aa1a0038416453dc4ab188503dc1de10450b1597f2e6bf845032a38f7", "content": "The `BATCH_JOB_EXECUTION_CONTEXT` table holds all information relevant to the\n`ExecutionContext` of a `Job`. There is exactly one `Job` `ExecutionContext` for each\n`JobExecution`, and it contains all of the job-level data that is needed for a particular\njob execution. This data typically represents the state that must be retrieved after a\nfailure, so that a `JobInstance` can \"`start where it left off`\". The following\nlisting shows the definition of the `BATCH_JOB_EXECUTION_CONTEXT` table:\n\n[source, sql]\n----\nCREATE TABLE BATCH_JOB_EXECUTION_CONTEXT (\n JOB_EXECUTION_ID BIGINT PRIMARY KEY,\n SHORT_CONTEXT VARCHAR(2500) NOT NULL,\n SERIALIZED_CONTEXT CLOB,\n constraint JOB_EXEC_CTX_FK foreign key (JOB_EXECUTION_ID)\n references BATCH_JOB_EXECUTION(JOB_EXECUTION_ID)\n) ;\n----\n\nThe following list describes each column:\n\n* `JOB_EXECUTION_ID`: Foreign key representing the `JobExecution` to which the context\nbelongs. There may be more than one row associated with a given execution.\n* `SHORT_CONTEXT`: A string version of the `SERIALIZED_CONTEXT`.\n* `SERIALIZED_CONTEXT`: The entire context, serialized.\n\n[[metaDataBatchStepExecutionContext]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "The `BATCH_JOB_EXECUTION_CONTEXT` Table", "heading_level": 2, "file_order": 57, "section_index": 10, "content_hash": "ad64d23aa1a0038416453dc4ab188503dc1de10450b1597f2e6bf845032a38f7", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:502dbf8376f2cf0e3755eaf015bc7e2d3890ea4198fc9663c543b1120a7f3f83", "content": "The `BATCH_STEP_EXECUTION_CONTEXT` table holds all information relevant to the\n`ExecutionContext` of a `Step`. There is exactly one `ExecutionContext` per\n`StepExecution`, and it contains all of the data that\nneeds to be persisted for a particular step execution. This data typically represents the\nstate that must be retrieved after a failure so that a `JobInstance` can \"`start\nwhere it left off`\". The following listing shows the definition of the\n`BATCH_STEP_EXECUTION_CONTEXT` table:\n\n[source, sql]\n----\nCREATE TABLE BATCH_STEP_EXECUTION_CONTEXT (\n STEP_EXECUTION_ID BIGINT PRIMARY KEY,\n SHORT_CONTEXT VARCHAR(2500) NOT NULL,\n SERIALIZED_CONTEXT CLOB,\n constraint STEP_EXEC_CTX_FK foreign key (STEP_EXECUTION_ID)\n references BATCH_STEP_EXECUTION(STEP_EXECUTION_ID)\n) ;\n----\n\nThe following list describes each column:\n\n* `STEP_EXECUTION_ID`: Foreign key representing the `StepExecution` to which the context\nbelongs. There may be more than one row associated with a given execution.\n* `SHORT_CONTEXT`: A string version of the `SERIALIZED_CONTEXT`.\n* `SERIALIZED_CONTEXT`: The entire context, serialized.\n\n[[metaDataArchiving]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "The `BATCH_STEP_EXECUTION_CONTEXT` Table", "heading_level": 2, "file_order": 57, "section_index": 11, "content_hash": "502dbf8376f2cf0e3755eaf015bc7e2d3890ea4198fc9663c543b1120a7f3f83", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:3e7af3f392acddefc4ba0a5203a6026568f60b9e04b9b74f4681792e2430e087", "content": "Because there are entries in multiple tables every time a batch job is run, it is common\nto create an archive strategy for the metadata tables. The tables themselves are designed\nto show a record of what happened in the past and generally do not affect the run of any\njob, with a few notable exceptions pertaining to restart:\n\n* The framework uses the metadata tables to determine whether a particular `JobInstance`\nhas been run before. If it has been run and if the job is not restartable, an\nexception is thrown.\n* If an entry for a `JobInstance` is removed without having completed successfully, the\nframework thinks that the job is new rather than a restart.\n* If a job is restarted, the framework uses any data that has been persisted to the\n`ExecutionContext` to restore the `Job's` state. Therefore, removing any entries from\nthis table for jobs that have not completed successfully prevents them from starting at\nthe correct point if they are run again.\n\n[[multiByteCharacters]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "Archiving", "heading_level": 2, "file_order": 57, "section_index": 12, "content_hash": "3e7af3f392acddefc4ba0a5203a6026568f60b9e04b9b74f4681792e2430e087", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:cf977c96d89015528a2aae26d5d81a4a199ec7f12b0d9bf013b6b2d957d8f625", "content": "If you use multi-byte character sets (such as Chinese or Cyrillic) in your business\nprocessing, those characters might need to be persisted in the Spring Batch schema.\nMany users find that simply changing the schema to double the length of the `VARCHAR`\ncolumns is enough. Others prefer to configure the\nxref:job/configuring-repository.adoc[JobRepository] with `max-varchar-length` half the\nvalue of the `VARCHAR` column length. Some users have also reported that they use\n`NVARCHAR` in place of `VARCHAR` in their schema definitions. The best result depends on\nthe database platform and the way the database server has been configured locally.\n\n[[recommendationsForIndexingMetaDataTables]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "International and Multi-byte Characters", "heading_level": 2, "file_order": 57, "section_index": 13, "content_hash": "cf977c96d89015528a2aae26d5d81a4a199ec7f12b0d9bf013b6b2d957d8f625", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:2030df86f297d5bfe5c00f89d5832ea44cba5eb758112163230aed49f5dc659f", "content": "Spring Batch provides DDL samples for the metadata tables in the core jar file for\nseveral common database platforms. Index declarations are not included in that DDL,\nbecause there are too many variations in how users may want to index, depending on their\nprecise platform, local conventions, and the business requirements of how the jobs are\noperated. The following table provides some indication as to which columns are going to\nbe used in a `WHERE` clause by the DAO implementations provided by Spring Batch and how\nfrequently they might be used so that individual projects can make up their own minds\nabout indexing:\n\n.Where clauses in SQL statements (excluding primary keys) and their approximate frequency of use.\n\n|===============\n|Default Table Name|Where Clause|Frequency\n|`BATCH_JOB_INSTANCE`|`JOB_NAME = ? and JOB_KEY = ?`|Every time a job is launched\n|`BATCH_JOB_EXECUTION`|`JOB_INSTANCE_ID = ?`|Every time a job is restarted\n|`BATCH_STEP_EXECUTION`|`VERSION = ?`|On commit interval, a.k.a. chunk (and at start and end of\n step)\n|`BATCH_STEP_EXECUTION`|`STEP_NAME = ? and JOB_EXECUTION_ID = ?`|Before each step execution\n\n|===============", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc", "title": "schema-appendix", "heading": "Recommendations for Indexing Metadata Tables", "heading_level": 2, "file_order": 57, "section_index": 14, "content_hash": "2030df86f297d5bfe5c00f89d5832ea44cba5eb758112163230aed49f5dc659f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/schema-appendix.adoc"}}
{"id": "sha256:e9886f4a25e6bb2c5d04df86467af515fa07f3078e1b622965bcacc90fc7ca82", "content": "[[springBatchArchitecture]]\n\nSpring Batch is designed with extensibility and a diverse group of end users in mind. The\nfollowing image shows the layered architecture that supports the extensibility and ease of\nuse for end-user developers.\n\n.Spring Batch Layered Architecture\nimage::spring-batch-layers.png[Figure 1.1: Spring Batch Layered Architecture, scaledwidth=\"60%\"]\n\nThis layered architecture highlights three major high-level components: Application,\nCore, and Infrastructure. The application contains all batch jobs and custom code written\nby developers using Spring Batch. The Batch Core contains the core runtime classes\nnecessary to launch and control a batch job. It includes implementations for\n`JobOperator`, `Job`, and `Step`. Both Application and Core are built on top of a common\ninfrastructure. This infrastructure contains common readers and writers and services\n(such as the `RetryTemplate`), which are used both by application developers(readers and\nwriters, such as `ItemReader` and `ItemWriter`), and the core framework itself (retry,\nwhich is its own library).\n\n[[batchArchitectureConsiderations]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-architecture.adoc", "title": "spring-batch-architecture", "heading": "spring-batch-architecture", "heading_level": 1, "file_order": 58, "section_index": 0, "content_hash": "e9886f4a25e6bb2c5d04df86467af515fa07f3078e1b622965bcacc90fc7ca82", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-architecture.adoc"}}
{"id": "sha256:30528468fa06f668ca66b90dbbe98245cac0e7e8ec5e388b731969862f147b36", "content": "The following key principles, guidelines, and general considerations should be considered\nwhen building a batch solution.\n\n* Remember that a batch architecture typically affects on-line architecture and vice\nversa. Design with both architectures and environments in mind by using common building\nblocks when possible.\n\n* Simplify as much as possible and avoid building complex logical structures in single\nbatch applications.\n\n* Keep the processing and storage of data physically close together (in other words, keep\nyour data where your processing occurs).\n\n* Minimize system resource use, especially I/O. Perform as many operations as possible in\ninternal memory.\n\n* Review application I/O (analyze SQL statements) to ensure that unnecessary physical I/O\nis avoided. In particular, the following four common flaws need to be looked for:\n** Reading data for every transaction when the data could be read once and cached or kept\nin the working storage.\n** Rereading data for a transaction where the data was read earlier in the same\ntransaction.\n** Causing unnecessary table or index scans.\n** Not specifying key values in the `WHERE` clause of an SQL statement.\n\n* Do not do things twice in a batch run. For instance, if you need data summarization for\nreporting purposes, you should (if possible) increment stored totals when data is being\ninitially processed, so your reporting application does not have to reprocess the same\ndata.\n\n* Allocate enough memory at the beginning of a batch application to avoid time-consuming\nreallocation during the process.\n\n* Always assume the worst with regard to data integrity. Insert adequate checks and\nrecord validation to maintain data integrity.\n\n* Implement checksums for internal validation where possible. For example, flat files\nshould have a trailer record telling the total of records in the file and an aggregate of\nthe key fields.\n\n* Plan and execute stress tests as early as possible in a production-like environment\nwith realistic data volumes.\n\n* In large batch systems, backups can be challenging, especially if the system is running\nconcurrent with online applications on a 24-7 basis. Database backups are typically well taken care\nof in online design, but file backups should be considered to be just as important.\nIf the system depends on flat files, file backup procedures should not only be in place\nand documented but be regularly tested as well.\n\n[[batchProcessingStrategy]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-architecture.adoc", "title": "spring-batch-architecture", "heading": "General Batch Principles and Guidelines", "heading_level": 2, "file_order": 58, "section_index": 1, "content_hash": "30528468fa06f668ca66b90dbbe98245cac0e7e8ec5e388b731969862f147b36", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-architecture.adoc"}}
{"id": "sha256:712d12a4bcf80bd21dadea362c074b991ad67e38b8be37e41805bf857bcf39f6", "content": "To help design and implement batch systems, basic batch application building blocks and\npatterns should be provided to the designers and programmers in the form of sample\nstructure charts and code shells. When starting to design a batch job, the business logic\nshould be decomposed into a series of steps that can be implemented by using the following\nstandard building blocks:\n\n* __Conversion Applications:__ For each type of file supplied by or generated for an\nexternal system, a conversion application must be created to convert the transaction\nrecords supplied into a standard format required for processing. This type of batch\napplication can partly or entirely consist of translation utility modules (see Basic\nBatch Services).\n* __Validation Applications:__ A validation application ensures that all input and output\nrecords are correct and consistent. Validation is typically based on file headers and\ntrailers, checksums and validation algorithms, and record-level cross-checks.\n* __Extract Applications:__ An extract application reads a set of records from a database or\ninput file, selects records based on predefined rules, and writes the records to an\noutput file.\n* __Extract/Update Applications:__ An extract/update applications reads records from a database or\nan input file and makes changes to a database or an output file, driven by the data found\nin each input record.\n* __Processing and Updating Applications:__ A processing and updating application performs processing on\ninput transactions from an extract or a validation application. The processing usually\ninvolves reading a database to obtain data required for processing, potentially updating\nthe database and creating records for output processing.\n* __Output/Format Applications:__ An output/format applications reads an input file, restructures data\nfrom this record according to a standard format, and produces an output file for printing\nor transmission to another program or system.\n\nAdditionally, a basic application shell should be provided for business logic that cannot\nbe built by using the previously mentioned building blocks.\n\nIn addition to the main building blocks, each application may use one or more standard\nutility steps, such as:\n\n* Sort: A program that reads an input file and produces an output file where records\nhave been re-sequenced according to a sort key field in the records. Sorts are usually\nperformed by standard system utilities.\n* Split: A program that reads a single input file and writes each record to one of\nseveral output files based on a field value. Splits can be tailored or performed by\nparameter-driven standard system utilities.\n* Merge: A program that reads records from multiple input files and produces one output\nfile with combined data from the input files. Merges can be tailored or performed by\nparameter-driven standard system utilities.\n\nBatch applications can additionally be categorized by their input source:\n\n* Database-driven applications are driven by rows or values retrieved from the database.\n* File-driven applications are driven by records or values retrieved from a file.\n* Message-driven applications are driven by messages retrieved from a message queue.\n\nThe foundation of any batch system is the processing strategy. Factors affecting the\nselection of the strategy include: estimated batch system volume, concurrency with\nonline systems or with other batch systems, available batch windows. (Note that, with\nmore enterprises wanting to be up and running 24x7, clear batch windows are\ndisappearing).\n\nTypical processing options for batch are (in increasing order of implementation\ncomplexity):\n\n* Normal processing during a batch window in offline mode.\n* Concurrent batch or online processing.\n* Parallel processing of many different batch runs or jobs at the same time.\n* Partitioning (processing of many instances of the same job at the same time).\n* A combination of the preceding options.\n\nSome or all of these options may be supported by a commercial scheduler.\n\nThe remainder of this section discusses these processing options in more detail.\nNote that, as a rule of thumb, the commit and locking strategy adopted by batch\nprocesses depends on the type of processing performed and that the online locking\nstrategy should also use the same principles. Therefore, the batch architecture cannot be\nsimply an afterthought when designing an overall architecture.\n\nThe locking strategy can be to use only normal database locks or to implement an\nadditional custom locking service in the architecture. The locking service would track\ndatabase locking (for example, by storing the necessary information in a dedicated\ndatabase table) and give or deny permissions to the application programs requesting a database\noperation. Retry logic could also be implemented by this architecture to avoid aborting a\nbatch job in case of a lock situation.\n\n*1. Normal processing in a batch window* For simple batch processes running in a separate\nbatch window where the data being updated is not required by online users or other batch\nprocesses, concurrency is not an issue and a single commit can be done at the end of the\nbatch run.\n\nIn most cases, a more robust approach is more appropriate. Keep in mind that batch\nsystems have a tendency to grow as time goes by, both in terms of complexity and the data\nvolumes they handle. If no locking strategy is in place and the system still relies on a\nsingle commit point, modifying the batch programs can be painful. Therefore, even with\nthe simplest batch systems, consider the need for commit logic for restart-recovery\noptions as well as the information concerning the more complex cases described later in\nthis section.\n\n*2. Concurrent batch or on-line processing* Batch applications processing data that can\nbe simultaneously updated by online users should not lock any data (either in the\ndatabase or in files) that could be required by on-line users for more than a few\nseconds. Also, updates should be committed to the database at the end of every few\ntransactions. Doing so minimizes the portion of data that is unavailable to other processes\nand the elapsed time the data is unavailable.\n\nAnother option to minimize physical locking is to have logical row-level locking\nimplemented with either an optimistic locking pattern or a pessimistic locking pattern.\n\n* Optimistic locking assumes a low likelihood of record contention. It typically means\ninserting a timestamp column in each database table that is used concurrently by both batch and\nonline processing. When an application fetches a row for processing, it also fetches the\ntimestamp. As the application then tries to update the processed row, the update uses the\noriginal timestamp in the `WHERE` clause. If the timestamp matches, the data and the\ntimestamp are updated. If the timestamp does not match, this indicates that another\napplication has updated the same row between the fetch and the update attempt. Therefore,\nthe update cannot be performed.\n\n* Pessimistic locking is any locking strategy that assumes there is a high likelihood of\nrecord contention and, therefore, either a physical or a logical lock needs to be obtained at\nretrieval time. One type of pessimistic logical locking uses a dedicated lock-column in\nthe database table. When an application retrieves the row for update, it sets a flag in\nthe lock column. With the flag in place, other applications attempting to retrieve the\nsame row logically fail. When the application that sets the flag updates the row, it also\nclears the flag, enabling the row to be retrieved by other applications. Note that\nthe integrity of data must be maintained also between the initial fetch and the setting\nof the flag -- for example, by using database locks (such as `SELECT FOR UPDATE`). Note also that\nthis method suffers from the same downside as physical locking except that it is somewhat\neasier to manage building a time-out mechanism that gets the lock released if the user\ngoes to lunch while the record is locked.\n\nThese patterns are not necessarily suitable for batch processing, but they might be used\nfor concurrent batch and online processing (such as in cases where the database does not\nsupport row-level locking). As a general rule, optimistic locking is more suitable for\nonline applications, while pessimistic locking is more suitable for batch applications.\nWhenever logical locking is used, the same scheme must be used for all applications\nthat access the data entities protected by logical locks.\n\nNote that both of these solutions only address locking a single record. Often, we may\nneed to lock a logically related group of records. With physical locks, you have to\nmanage these very carefully to avoid potential deadlocks. With logical locks, it\nis usually best to build a logical lock manager that understands the logical record\ngroups you want to protect and that can ensure that locks are coherent and\nnon-deadlocking. This logical lock manager usually uses its own tables for lock\nmanagement, contention reporting, time-out mechanism, and other concerns.\n\n*3. Parallel Processing* Parallel processing lets multiple batch runs or jobs run in\nparallel to minimize the total elapsed batch processing time. This is not a problem as\nlong as the jobs are not sharing the same files, database tables, or index spaces. If they do,\nthis service should be implemented by using partitioned data. Another option is to build an\narchitecture module for maintaining interdependencies by using a control table. A control\ntable should contain a row for each shared resource and whether it is in use by an\napplication or not. The batch architecture or the application in a parallel job would\nthen retrieve information from that table to determine whether it can get access to the\nresource it needs.\n\nIf the data access is not a problem, parallel processing can be implemented through the\nuse of additional threads to process in parallel. In a mainframe environment, parallel\njob classes have traditionally been used, to ensure adequate CPU time for all\nthe processes. Regardless, the solution has to be robust enough to ensure time slices for\nall the running processes.\n\nOther key issues in parallel processing include load balancing and the availability of\ngeneral system resources, such as files, database buffer pools, and so on. Also, note that\nthe control table itself can easily become a critical resource.\n\n*4. Partitioning* Using partitioning lets multiple versions of large batch applications\nrun concurrently. The purpose of this is to reduce the elapsed time required to\nprocess long batch jobs. Processes that can be successfully partitioned are those where\nthe input file can be split or the main database tables partitioned to let the\napplication run against different sets of data.\n\nIn addition, processes that are partitioned must be designed to process only their\nassigned data set. A partitioning architecture has to be closely tied to the database\ndesign and the database partitioning strategy. Note that database partitioning does not\nnecessarily mean physical partitioning of the database (although, in most cases, this is\nadvisable). The following image illustrates the partitioning approach:\n\n.Partitioned Process\nimage::partitioned.png[Figure 1.2: Partitioned Process, scaledwidth=\"60%\"]\n\nThe architecture should be flexible enough to allow dynamic configuration of the number\nof partitions. You should consider both automatic and user controlled configuration.\nAutomatic configuration may be based on such parameters as the input file size and the\nnumber of input records.\n\n*4.1 Partitioning Approaches* Selecting a partitioning approach has to be done on a\ncase-by-case basis. The following list describes some of the possible partitioning\napproaches:\n\n_1. Fixed and Even Break-Up of Record Set_\n\nThis involves breaking the input record set into an even number of portions (for example,\n10, where each portion has exactly 1/10th of the entire record set). Each portion is then\nprocessed by one instance of the batch/extract application.\n\nTo use this approach, preprocessing is required to split the record set up. The\nresult of this split is a lower and upper bound placement number that you can use\nas input to the batch/extract application to restrict its processing to only its\nportion.\n\nPreprocessing could be a large overhead, as it has to calculate and determine the bounds\nof each portion of the record set.\n\n_2. Break up by a Key Column_\n\nThis involves breaking up the input record set by a key column, such as a location code,\nand assigning data from each key to a batch instance. To achieve this, column\nvalues can be either:\n\n* Assigned to a batch instance by a partitioning table (described later in this\nsection).\n\n* Assigned to a batch instance by a portion of the value (such as 0000-0999, 1000 - 1999,\nand so on).\n\nUnder option 1, adding new values means a manual reconfiguration of the batch or extract to\nensure that the new value is added to a particular instance.\n\nUnder option 2, this ensures that all values are covered by an instance of the batch\njob. However, the number of values processed by one instance is dependent on the\ndistribution of column values (there may be a large number of locations in the 0000-0999\nrange and few in the 1000-1999 range). Under this option, the data range should be\ndesigned with partitioning in mind.\n\nUnder both options, the optimal even distribution of records to batch instances cannot be\nrealized. There is no dynamic configuration of the number of batch instances used.\n\n_3. Breakup by Views_\n\nThis approach is basically breakup by a key column but on the database level. It involves\nbreaking up the record set into views. These views are used by each instance of the batch\napplication during its processing. The breakup is done by grouping the data.\n\nWith this option, each instance of a batch application has to be configured to hit a\nparticular view (instead of the main table). Also, with the addition of new data\nvalues, this new group of data has to be included into a view. There is no dynamic\nconfiguration capability, as a change in the number of instances results in a change to\nthe views.\n\n_4. Addition of a Processing Indicator_\n\nThis involves the addition of a new column to the input table, which acts as an\nindicator. As a preprocessing step, all indicators are marked as being non-processed.\nDuring the record fetch stage of the batch application, records are read on the condition\nthat an individual record is marked as being non-processed, and, once it is read (with lock),\nit is marked as being in processing. When that record is completed, the indicator is\nupdated to either complete or error. You can start many instances of a batch application\nwithout a change, as the additional column ensures that a record is only processed once.\n\nWith this option, I/O on the table increases dynamically. In the case of an updating\nbatch application, this impact is reduced, as a write must occur anyway.\n\n_5. Extract Table to a Flat File_\n\nThis approach involves the extraction of the table into a flat file. This file can then be split into\nmultiple segments and used as input to the batch instances.\n\nWith this option, the additional overhead of extracting the table into a file and\nsplitting it may cancel out the effect of multi-partitioning. Dynamic configuration can\nbe achieved by changing the file splitting script.\n\n_6. Use of a Hashing Column_\n\nThis scheme involves the addition of a hash column (key or index) to the database tables\nused to retrieve the driver record. This hash column has an indicator to determine which\ninstance of the batch application processes this particular row. For example, if there\nare three batch instances to be started, an indicator of 'A' marks a row for\nprocessing by instance 1, an indicator of 'B' marks a row for processing by instance 2,\nand an indicator of 'C' marks a row for processing by instance 3.\n\nThe procedure used to retrieve the records would then have an additional `WHERE` clause\nto select all rows marked by a particular indicator. The inserts in this table would\ninvolve the addition of the marker field, which would be defaulted to one of the\ninstances (such as 'A').\n\nA simple batch application would be used to update the indicators, such as to\nredistribute the load between the different instances. When a sufficiently large number\nof new rows have been added, this batch can be run (anytime, except in the batch window)\nto redistribute the new rows to other instances.\n\nAdditional instances of the batch application require only the running of the batch\napplication (as described in the preceding paragraphs) to redistribute the indicators to\nwork with a new number of instances.\n\n*4.2 Database and Application Design Principles*\n\nAn architecture that supports multi-partitioned applications that run against\npartitioned database tables and use the key column approach should include a central\npartition repository for storing partition parameters. This provides flexibility and\nensures maintainability. The repository generally consists of a single table, known as\nthe partition table.\n\nInformation stored in the partition table is static and, in general, should be maintained\nby the DBA. The table should consist of one row of information for each partition of a\nmulti-partitioned application. The table should have columns for Program ID Code,\nPartition Number (the logical ID of the partition), Low Value of the database key column for this\npartition, and High Value of the database key column for this partition.\n\nOn program start-up, the program `id` and partition number should be passed to the\napplication from the architecture (specifically, from the control processing tasklet). If\na key column approach is used, these variables are used to read the partition table\nto determine what range of data the application is to process. In addition, the\npartition number must be used throughout the processing to:\n\n* Add to the output files or database updates, for the merge process to work\nproperly.\n* Report normal processing to the batch log and any errors to the architecture error\nhandler.\n\n*4.3 Minimizing Deadlocks*\n\nWhen applications run in parallel or are partitioned, contention for database resources\nand deadlocks may occur. It is critical that the database design team eliminate\npotential contention situations as much as possible, as part of the database design.\n\nAlso, the developers must ensure that the database index tables are designed with\ndeadlock prevention and performance in mind.\n\nDeadlocks or hot spots often occur in administration or architecture tables, such as log\ntables, control tables, and lock tables. The implications of these should be taken into\naccount as well. Realistic stress tests are crucial for identifying the possible\nbottlenecks in the architecture.\n\nTo minimize the impact of conflicts on data, the architecture should provide services\n(such as wait-and-retry intervals) when attaching to a database or when encountering a\ndeadlock. This means a built-in mechanism to react to certain database return codes and,\ninstead of issuing an immediate error, waiting a predetermined amount of time and\nretrying the database operation.\n\n*4.4 Parameter Passing and Validation*\n\nThe partition architecture should be relatively transparent to application developers.\nThe architecture should perform all tasks associated with running the application in a\npartitioned mode, including:\n\n* Retrieving partition parameters before application start-up.\n* Validating partition parameters before application start-up.\n* Passing parameters to the application at start-up.\n\nThe validation should include checks to ensure that:\n\n* The application has sufficient partitions to cover the whole data range.\n* There are no gaps between partitions.\n\nIf the database is partitioned, some additional validation may be necessary to ensure\nthat a single partition does not span database partitions.\n\nAlso, the architecture should take into consideration the consolidation of partitions.\nKey questions include:\n\n* Must all the partitions be finished before going into the next job step?\n* What happens if one of the partitions aborts?", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-architecture.adoc", "title": "spring-batch-architecture", "heading": "Batch Processing Strategies", "heading_level": 2, "file_order": 58, "section_index": 2, "content_hash": "712d12a4bcf80bd21dadea362c074b991ad67e38b8be37e41805bf857bcf39f6", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-architecture.adoc"}}
{"id": "sha256:2ff18a312eb9bbd1e411ae8a701bf54271c1a6bd82ea41f1fbc3dec0fe095f2f", "content": "[[springBatchIntegration]]\n\nifndef::onlyonetoggle[]\nendif::onlyonetoggle[]\n\nMany users of Spring Batch may encounter requirements that are\noutside the scope of Spring Batch but that may be efficiently and\nconcisely implemented by using Spring Integration. Conversely, Spring\nIntegration users may encounter Spring Batch requirements and need a way\nto efficiently integrate both frameworks. In this context, several\npatterns and use-cases emerge, and Spring Batch Integration\naddresses those requirements.\n\nThe line between Spring Batch and Spring Integration is not always\nclear, but two pieces of advice can\nhelp: Thinking about granularity and applying common patterns. Some\nof those common patterns are described in this section.\n\nAdding messaging to a batch process enables automation of\noperations and also separation and strategizing of key concerns.\nFor example, a message might trigger a job to execute, and then\nsending the message can be exposed in a variety of ways. Alternatively, when\na job completes or fails, that event might trigger a message to be sent,\nand the consumers of those messages might have operational concerns\nthat have nothing to do with the application itself. Messaging can\nalso be embedded in a job (for example, reading or writing items for\nprocessing through channels). Remote partitioning and remote chunking\nprovide methods to distribute workloads over a number of workers.\n\nThis section covers the following key concepts:\n\n[role=\"xmlContent\"]\n* xref:spring-batch-integration/namespace-support.adoc[Namespace Support]\n* xref:spring-batch-integration/launching-jobs-through-messages.adoc[Launching Batch Jobs through Messages]\n* xref:spring-batch-integration/available-attributes-of-the-job-launching-gateway.adoc[Available Attributes of the Job-Launching Gateway]\n* xref:spring-batch-integration/providing-feedback-with-informational-messages.adoc[Providing Feedback with Informational Messages]\n* xref:spring-batch-integration/asynchronous-processing.adoc[Asynchronous Processors]\n* xref:spring-batch-integration/externalizing-execution.adoc[Externalizing Batch Process Execution]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-integration.adoc", "title": "spring-batch-integration", "heading": "spring-batch-integration", "heading_level": 1, "file_order": 59, "section_index": 0, "content_hash": "2ff18a312eb9bbd1e411ae8a701bf54271c1a6bd82ea41f1fbc3dec0fe095f2f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-integration.adoc"}}
{"id": "sha256:f4205e66a11ede5045e6ddbe021546b82af5fdbf91c40900f7f5a9fcc106dbbf", "content": "[[spring-batch-intro]]\n\nMany applications within the enterprise domain require bulk processing to perform\nbusiness operations in mission-critical environments. These business operations include:\n\n* Automated, complex processing of large volumes of information that is most efficiently\nprocessed without user interaction. These operations typically include time-based events\n(such as month-end calculations, notices, or correspondence).\n* Periodic application of complex business rules processed repetitively across very large\ndata sets (for example, insurance benefit determination or rate adjustments).\n* Integration of information that is received from internal and external systems that\ntypically requires formatting, validation, and processing in a transactional manner into\nthe system of record. Batch processing is used to process billions of transactions every\nday for enterprises.\n\nSpring Batch is a lightweight, comprehensive batch framework designed to enable the\ndevelopment of robust batch applications that are vital for the daily operations of enterprise\nsystems. Spring Batch builds upon the characteristics of the Spring Framework that people\nhave come to expect (productivity, POJO-based development approach, and general ease of\nuse), while making it easy for developers to access and use more advanced enterprise\nservices when necessary. Spring Batch is not a scheduling framework. There are many good\nenterprise schedulers (such as Quartz, Tivoli, Control-M, and others) available in both the\ncommercial and open source spaces. Spring Batch is intended to work in conjunction with a\nscheduler rather than replace a scheduler.\n\nSpring Batch provides reusable functions that are essential in processing large volumes\nof records, including logging and tracing, transaction management, job processing statistics,\njob restart, skip, and resource management. It also provides more advanced technical\nservices and features that enable extremely high-volume and high performance batch jobs\nthrough optimization and partitioning techniques. You can use Spring Batch in both simple\nuse cases (such as reading a file into a database or running a stored procedure) and\ncomplex, high volume use cases (such as moving high volumes of data between databases,\ntransforming it, and so on). High-volume batch jobs can use the framework in a\nhighly scalable manner to process significant volumes of information.\n\n[[springBatchBackground]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc", "title": "spring-batch-intro", "heading": "spring-batch-intro", "heading_level": 1, "file_order": 60, "section_index": 0, "content_hash": "f4205e66a11ede5045e6ddbe021546b82af5fdbf91c40900f7f5a9fcc106dbbf", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc"}}
{"id": "sha256:cb99df84a76a50b7fce522b7fecaf0a3447c1d18c050c0899ada6cff4264dcce", "content": "While open source software projects and associated communities have focused greater\nattention on web-based and microservices-based architecture frameworks, there has been a\nnotable lack of focus on reusable architecture frameworks to accommodate Java-based batch\nprocessing needs, despite continued needs to handle such processing within enterprise IT\nenvironments. The lack of a standard, reusable batch architecture has resulted in the\nproliferation of many one-off, in-house solutions developed within client enterprise IT\nfunctions.\n\nSpringSource (now VMware) and Accenture collaborated to change this. Accenture's\nhands-on industry and technical experience in implementing batch architectures,\nSpringSource's depth of technical experience, and Spring's proven programming model\ntogether made a natural and powerful partnership to create high-quality, market-relevant\nsoftware aimed at filling an important gap in enterprise Java. Both companies worked with\na number of clients who were solving similar problems by developing Spring-based batch\narchitecture solutions. This input provided some useful additional detail and real-life\nconstraints that helped to ensure the solution can be applied to the real-world problems\nposed by clients.\n\nAccenture contributed previously proprietary batch processing architecture frameworks to\nthe Spring Batch project, along with committer resources to drive support, enhancements,\nand the existing feature set. Accenture's contribution was based upon decades of\nexperience in building batch architectures with the last several generations of\nplatforms: COBOL on mainframes, C++ on Unix, and, now, Java anywhere.\n\nThe collaborative effort between Accenture and SpringSource aimed to promote the\nstandardization of software processing approaches, frameworks, and tools\nenterprise users can consistently use when creating batch applications. Companies\nand government agencies desiring to deliver standard, proven solutions to their\nenterprise IT environments can benefit from Spring Batch.\n\n[[springBatchUsageScenarios]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc", "title": "spring-batch-intro", "heading": "Background", "heading_level": 2, "file_order": 60, "section_index": 1, "content_hash": "cb99df84a76a50b7fce522b7fecaf0a3447c1d18c050c0899ada6cff4264dcce", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc"}}
{"id": "sha256:aa87c180b64868ff2bc272fc8e8cedc6147f9845bc73bfca747499eb37799044", "content": "A typical batch program generally:\n\n* Reads a large number of records from a database, file, or queue.\n* Processes the data in some fashion.\n* Writes back data in a modified form.\n\nSpring Batch automates this basic batch iteration, providing the capability to process\nsimilar transactions as a set, typically in an offline environment without any user\ninteraction. Batch jobs are part of most IT projects, and Spring Batch is the only open\nsource framework that provides a robust, enterprise-scale solution.\n\n[[business-scenarios]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc", "title": "spring-batch-intro", "heading": "Usage Scenarios", "heading_level": 2, "file_order": 60, "section_index": 2, "content_hash": "aa87c180b64868ff2bc272fc8e8cedc6147f9845bc73bfca747499eb37799044", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc"}}
{"id": "sha256:5ecb038b8414f8cb3a9dbf56a466836c54896dccd15f4279e674ec7c81fccb12", "content": "Spring Batch supports the following business scenarios:\n\n* Commit batch process periodically.\n* Concurrent batch processing: parallel processing of a job.\n* Staged, enterprise message-driven processing.\n* Massively parallel batch processing.\n* Manual or scheduled restart after failure.\n* Sequential processing of dependent steps (with extensions to workflow-driven batches).\n* Partial processing: skip records (for example, on rollback).\n* Whole-batch transaction, for cases with a small batch size or existing stored\nprocedures or scripts.\n\n[[technical-objectives]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc", "title": "spring-batch-intro", "heading": "Business Scenarios", "heading_level": 3, "file_order": 60, "section_index": 3, "content_hash": "5ecb038b8414f8cb3a9dbf56a466836c54896dccd15f4279e674ec7c81fccb12", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc"}}
{"id": "sha256:413aefa885c070eb2757793ca90b76588f754455f42165893fbf4324a4b7b17c", "content": "Spring Batch has the following technical objectives:\n\n* Let batch developers use the Spring programming model: Concentrate on business logic and\nlet the framework take care of the infrastructure.\n* Provide clear separation of concerns between the infrastructure, the batch execution\nenvironment, and the batch application.\n* Provide common, core execution services as interfaces that all projects can implement.\n* Provide simple and default implementations of the core execution interfaces that can be\nused \"`out of the box`\".\n* Make it easy to configure, customize, and extend services, by using the Spring framework\nin all layers.\n* All existing core services should be easy to replace or extend, without any impact to\nthe infrastructure layer.\n* Provide a simple deployment model, with the architecture JARs completely separate from\nthe application, built by using Maven.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc", "title": "spring-batch-intro", "heading": "Technical Objectives", "heading_level": 3, "file_order": 60, "section_index": 4, "content_hash": "413aefa885c070eb2757793ca90b76588f754455f42165893fbf4324a4b7b17c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-intro.adoc"}}
{"id": "sha256:882f89f1fa842a749448923ad7d2a498d03badb3d5d92604f0d56a53c8faa0eb", "content": "[[springBatchObservability]]\n\nObservability is a critical aspect of modern applications, and Spring Batch provides robust support for monitoring and tracing batch jobs.\n\nThis section covers the integration of Spring Batch with popular observability tools such as Micrometer and Java Flight Recorder (JFR):\n\n[role=\"xmlContent\"]\n* xref:spring-batch-observability/micrometer.adoc[Micrometer Support]\n* xref:spring-batch-observability/jfr.adoc[Java Flight Recorder Support]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/spring-batch-observability.adoc", "title": "spring-batch-observability", "heading": "spring-batch-observability", "heading_level": 1, "file_order": 61, "section_index": 0, "content_hash": "882f89f1fa842a749448923ad7d2a498d03badb3d5d92604f0d56a53c8faa0eb", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/spring-batch-observability.adoc"}}
{"id": "sha256:dc7a824ac5498e73b2615a830305df41072934ccc729b4a12666bee754ae60a0", "content": "[[configureStep]]\n\nifndef::onlyonetoggle[]\nendif::onlyonetoggle[]\n\nAs discussed in xref:domain.adoc[the domain chapter], a `Step` is a\ndomain object that encapsulates an independent, sequential phase of a batch job and\ncontains all of the information necessary to define and control the actual batch\nprocessing. This is a necessarily vague description because the contents of any given\n`Step` are at the discretion of the developer writing a `Job`. A `Step` can be as simple\nor complex as the developer desires. A simple `Step` might load data from a file into the\ndatabase, requiring little or no code (depending upon the implementations used). A more\ncomplex `Step` might have complicated business rules that are applied as part of the\nprocessing, as the following image shows:\n\n.Step\nimage::step.png[Step, scaledwidth=\"60%\"]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/step.adoc", "title": "step", "heading": "step", "heading_level": 1, "file_order": 62, "section_index": 0, "content_hash": "dc7a824ac5498e73b2615a830305df41072934ccc729b4a12666bee754ae60a0", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/step.adoc"}}
{"id": "sha256:a7ddb26bea3f43f3c693b3370882cd48907b255944e1dd1d3b912dc1da3b1476", "content": "[[testing]]\n\nAs with other application styles, it is extremely important to unit test any code written\nas part of a batch job. The Spring core documentation covers how to unit and integration\ntest with Spring in great detail, so it will not be repeated here. It is important, however,\nto think about how to \"`end to end`\" test a batch job, which is what this chapter covers.\nThe `spring-batch-test` project includes classes that facilitate this end-to-end test\napproach.\n\n[[creatingUnitTestClass]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "testing", "heading_level": 1, "file_order": 63, "section_index": 0, "content_hash": "a7ddb26bea3f43f3c693b3370882cd48907b255944e1dd1d3b912dc1da3b1476", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:991fcfc72621189b438faa765eaade0fc2392bbabaedb8519acc62e52ee6514e", "content": "For the unit test to run a batch job, the framework must load the job's\n`ApplicationContext`. Two annotations are used to trigger this behavior:\n\n* `@SpringJUnitConfig` indicates that the class should use Spring's\nJUnit facilities\n* `@SpringBatchTest` injects Spring Batch test utilities (such as the\n`JobOperatorTestUtils` and `JobRepositoryTestUtils`) in the test context\n\nNOTE: If the test context contains a single `Job` bean definition, this\nbean will be autowired in `JobOperatorTestUtils`. Otherwise, the job\nunder test should be manually set on the `JobOperatorTestUtils`.\n\nWARNING: As of Spring Batch 6.0, JUnit 4 is no longer supported. Migration to JUnit Jupiter is recommended.\n\n[tabs]\n====\nJava::\n+\nThe following Java example shows the annotations in use:\n+\n.Using Java Configuration\n[source, java]\n----\n@SpringBatchTest\n@SpringJUnitConfig(SkipSampleConfiguration.class)\npublic class SkipSampleFunctionalTests { ... }\n----\n\nXML::\n+\nThe following XML example shows the annotations in use:\n+\n.Using XML Configuration\n[source, java]\n----\n@SpringBatchTest\n@SpringJUnitConfig(locations = { \"/skip-sample-configuration.xml\" })\npublic class SkipSampleFunctionalTests { ... }\n----\n\n====\n\n[[endToEndTesting]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Creating a Unit Test Class", "heading_level": 2, "file_order": 63, "section_index": 1, "content_hash": "991fcfc72621189b438faa765eaade0fc2392bbabaedb8519acc62e52ee6514e", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:ee2c4c77dd7bd6875a449c1b9fc83044d5c168124151118263d72ebed973bb88", "content": "\"`End To end`\" testing can be defined as testing the complete run of a batch job from\nbeginning to end. This allows for a test that sets up a test condition, executes the job,\nand verifies the end result.\n\nConsider an example of a batch job that reads from the database and writes to a flat file.\nThe test method begins by setting up the database with test data. It clears the `CUSTOMER`\ntable and then inserts 10 new records. The test then launches the `Job` by using the\n`startJob()` method. The `startJob()` method is provided by the `JobOperatorTestUtils`\nclass. The `JobOperatorTestUtils` class also provides the `startJob(JobParameters)`\nmethod, which lets the test give particular parameters. The `startJob()` method\nreturns the `JobExecution` object, which is useful for asserting particular information\nabout the `Job` run. In the following case, the test verifies that the `Job` ended with\na status of `COMPLETED`.\n\n[tabs]\n====\nJava::\n+\nThe following listing shows an example with JUnit 5 in Java configuration style:\n+\n.Java Based Configuration\n[source, java]\n----\n@SpringBatchTest\n@SpringJUnitConfig(SkipSampleConfiguration.class)\npublic class SkipSampleFunctionalTests {\n\n @Autowired\n private JobOperatorTestUtils jobOperatorTestUtils;\n\n private JdbcTemplate jdbcTemplate;\n\n @Autowired\n public void setDataSource(DataSource dataSource) {\n this.jdbcTemplate = new JdbcTemplate(dataSource);\n }\n\n @Test\n public void testJob(@Autowired Job job) throws Exception {\n this.jobOperatorTestUtils.setJob(job);\n this.jdbcTemplate.update(\"delete from CUSTOMER\");\n for (int i = 1; i <= 10; i++) {\n this.jdbcTemplate.update(\"insert into CUSTOMER values (?, 0, ?, 100000)\",\n i, \"customer\" + i);\n }\n\n JobExecution jobExecution = jobOperatorTestUtils.startJob();\n\n Assert.assertEquals(\"COMPLETED\", jobExecution.getExitStatus().getExitCode());\n }\n}\n----\n\nXML::\n+\nThe following listing shows an example with JUnit 5 in XML configuration style:\n+\n.XML Based Configuration\n[source, java]\n----\n@SpringBatchTest\n@SpringJUnitConfig(locations = { \"/skip-sample-configuration.xml\" })\npublic class SkipSampleFunctionalTests {\n\n @Autowired\n private JobOperatorTestUtils jobOperatorTestUtils;\n\n private JdbcTemplate jdbcTemplate;\n\n @Autowired\n public void setDataSource(DataSource dataSource) {\n this.jdbcTemplate = new JdbcTemplate(dataSource);\n }\n\n @Test\n public void testJob(@Autowired Job job) throws Exception {\n this.jobOperatorTestUtils.setJob(job);\n this.jdbcTemplate.update(\"delete from CUSTOMER\");\n for (int i = 1; i <= 10; i++) {\n this.jdbcTemplate.update(\"insert into CUSTOMER values (?, 0, ?, 100000)\",\n i, \"customer\" + i);\n }\n\n JobExecution jobExecution = jobOperatorTestUtils.startJob();\n\n Assert.assertEquals(\"COMPLETED\", jobExecution.getExitStatus().getExitCode());\n }\n}\n----\n====\n\n[[testingIndividualSteps]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "End-To-End Testing of Batch Jobs", "heading_level": 2, "file_order": 63, "section_index": 2, "content_hash": "ee2c4c77dd7bd6875a449c1b9fc83044d5c168124151118263d72ebed973bb88", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:cd9591cfd7e164391607aa09680f49e291ac3716cbf8aa71d6a695813612905f", "content": "For complex batch jobs, test cases in the end-to-end testing approach may become\nunmanageable. It these cases, it may be more useful to have test cases to test individual\nsteps on their own. The `JobOperatorTestUtils` class contains a method called `launchStep`,\nwhich takes a step name and runs just that particular `Step`. This approach allows for\nmore targeted tests letting the test set up data for only that step and to validate its\nresults directly. The following example shows how to use the `startStep` method to start a\n`Step` by name:\n\n[source, java]\n----\nJobExecution jobExecution = jobOperatorTestUtils.startStep(\"loadFileStep\");\n----\n\n[[testing-step-scoped-components]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Testing Individual Steps", "heading_level": 2, "file_order": 63, "section_index": 3, "content_hash": "cd9591cfd7e164391607aa09680f49e291ac3716cbf8aa71d6a695813612905f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:730fe267c4bd6fcfe3b3995b896031a41eb15e5fbf43140533b9632e9f78b66c", "content": "Often, the components that are configured for your steps at runtime use step scope and\nlate binding to inject context from the step or job execution. These are tricky to test as\nstandalone components, unless you have a way to set the context as if they were in a step\nexecution. That is the goal of two components in Spring Batch:\n`StepScopeTestExecutionListener` and `StepScopeTestUtils`.\n\nThe listener is declared at the class level, and its job is to create a step execution\ncontext for each test method, as the following example shows:\n\n[source, java]\n----\n@SpringJUnitConfig\n@TestExecutionListeners( { DependencyInjectionTestExecutionListener.class,\n StepScopeTestExecutionListener.class })\npublic class StepScopeTestExecutionListenerIntegrationTests {\n\n // This component is defined step-scoped, so it cannot be injected unless\n // a step is active...\n @Autowired\n private ItemReader<String> reader;\n\n public StepExecution getStepExecution() {\n StepExecution execution = MetaDataInstanceFactory.createStepExecution();\n execution.getExecutionContext().putString(\"input.data\", \"foo,bar,spam\");\n return execution;\n }\n\n @Test\n public void testReader() {\n // The reader is initialized and bound to the input data\n assertNotNull(reader.read());\n }\n\n}\n----\n\nThere are two `TestExecutionListeners`. One is the regular Spring Test framework, which\nhandles dependency injection from the configured application context to inject the reader.\nThe other is the Spring Batch `StepScopeTestExecutionListener`. It works by looking for a\nfactory method in the test case for a `StepExecution`, using that as the context for the\ntest method, as if that execution were active in a `Step` at runtime. The factory method\nis detected by its signature (it must return a `StepExecution`). If a factory method is\nnot provided, a default `StepExecution` is created.\n\nStarting from v4.1, the `StepScopeTestExecutionListener` and\n`JobScopeTestExecutionListener` are imported as test execution listeners\nif the test class is annotated with `@SpringBatchTest`. The preceding test\nexample can be configured as follows:\n\n[source, java]\n----\n@SpringBatchTest\n@SpringJUnitConfig\npublic class StepScopeTestExecutionListenerIntegrationTests {\n\n // This component is defined step-scoped, so it cannot be injected unless\n // a step is active...\n @Autowired\n private ItemReader<String> reader;\n\n public StepExecution getStepExecution() {\n StepExecution execution = MetaDataInstanceFactory.createStepExecution();\n execution.getExecutionContext().putString(\"input.data\", \"foo,bar,spam\");\n return execution;\n }\n\n @Test\n public void testReader() {\n // The reader is initialized and bound to the input data\n assertNotNull(reader.read());\n }\n\n}\n----\n\nThe listener approach is convenient if you want the duration of the step scope to be the\nexecution of the test method. For a more flexible but more invasive approach, you can use\nthe `StepScopeTestUtils`. The following example counts the number of items available in\nthe reader shown in the previous example:\n\n[source, java]\n----\nint count = StepScopeTestUtils.doInStepScope(stepExecution,\n new Callable<Integer>() {\n public Integer call() throws Exception {\n\n int count = 0;\n\n while (reader.read() != null) {\n count++;\n }\n return count;\n }\n});\n----\n\n[[mockingDomainObjects]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Testing Step-Scoped Components", "heading_level": 2, "file_order": 63, "section_index": 4, "content_hash": "730fe267c4bd6fcfe3b3995b896031a41eb15e5fbf43140533b9632e9f78b66c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:4558eb72980778afdc56c8f36f28d6b597a5caf72dffa0d6b821bbff4908c8a2", "content": "Another common issue encountered while writing unit and integration tests for Spring Batch\ncomponents is how to mock domain objects. A good example is a `StepExecutionListener`, as\nthe following code snippet shows:\n\n[source, java]\n----\npublic class NoWorkFoundStepExecutionListener implements StepExecutionListener {\n\n public ExitStatus afterStep(StepExecution stepExecution) {\n if (stepExecution.getReadCount() == 0) {\n return ExitStatus.FAILED;\n }\n return null;\n }\n}\n----\n\nThe framework provides the preceding listener example and checks a `StepExecution`\nfor an empty read count, thus signifying that no work was done. While this example is\nfairly simple, it serves to illustrate the types of problems that you may encounter when\nyou try to unit test classes that implement interfaces requiring Spring Batch domain\nobjects. Consider the following unit test for the listener's in the preceding example:\n\n[source, java]\n----\nprivate NoWorkFoundStepExecutionListener tested = new NoWorkFoundStepExecutionListener();\n\n@Test\npublic void noWork() {\n StepExecution stepExecution = new StepExecution(\"NoProcessingStep\",\n new JobExecution(new JobInstance(1L, new JobParameters(),\n \"NoProcessingJob\")));\n\n stepExecution.setExitStatus(ExitStatus.COMPLETED);\n stepExecution.setReadCount(0);\n\n ExitStatus exitStatus = tested.afterStep(stepExecution);\n assertEquals(ExitStatus.FAILED.getExitCode(), exitStatus.getExitCode());\n}\n----\n\nBecause the Spring Batch domain model follows good object-oriented principles, the\n`StepExecution` requires a `JobExecution`, which requires a `JobInstance` and\n`JobParameters`, to create a valid `StepExecution`. While this is good in a solid domain\nmodel, it does make creating stub objects for unit testing verbose. To address this issue,\nthe Spring Batch test module includes a factory for creating domain objects:\n`MetaDataInstanceFactory`. Given this factory, the unit test can be updated to be more\nconcise, as the following example shows:\n\n[source, java]\n----\nprivate NoWorkFoundStepExecutionListener tested = new NoWorkFoundStepExecutionListener();\n\n@Test\npublic void testAfterStep() {\n StepExecution stepExecution = MetaDataInstanceFactory.createStepExecution();\n\n stepExecution.setExitStatus(ExitStatus.COMPLETED);\n stepExecution.setReadCount(0);\n\n ExitStatus exitStatus = tested.afterStep(stepExecution);\n assertEquals(ExitStatus.FAILED.getExitCode(), exitStatus.getExitCode());\n}\n----\n\nThe preceding method for creating a simple `StepExecution` is only one convenience method\navailable within the factory. You can find a full method listing in its\nlink:$$http://docs.spring.io/spring-batch/apidocs/org/springframework/batch/test/MetaDataInstanceFactory.html$$[Javadoc].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Mocking Domain Objects", "heading_level": 2, "file_order": 63, "section_index": 5, "content_hash": "4558eb72980778afdc56c8f36f28d6b597a5caf72dffa0d6b821bbff4908c8a2", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:ba3bec088b347291625b90151b8d201f9aeb0b0b1c056bb1277ffe0b60a9ba51", "content": "[[whatsNew]]\n\nThis section highlights the major changes in Spring Batch 6.0. For the complete list of changes, please refer to the https://github.com/spring-projects/spring-batch/releases[release notes].\n\nSpring Batch 6.0 includes the following features and improvements:\n\n* xref:whatsnew.adoc#dependencies-upgrade[Dependencies upgrade]\n* xref:whatsnew.adoc#batch-infrastrucutre-configuration-improvements[Batch infrastructure configuration improvements]\n* xref:whatsnew.adoc#new-implementation-of-the-chunk-oriented-processing-model[New implementation of the chunk-oriented processing model]\n* xref:whatsnew.adoc#new-concurrency-model[New concurrency model]\n* xref:whatsnew.adoc#new-command-line-operator[New command line operator]\n* xref:whatsnew.adoc#ability-to-recover-failed-job-executions[Ability to recover failed job executions]\n* xref:whatsnew.adoc#ability-to-stop-all-kind-of-steps[Ability to stop all kinds of steps]\n* xref:whatsnew.adoc#graceful-shutdown[Graceful Shutdown support]\n* xref:whatsnew.adoc#observability-with-jfr[Observability support with the Java Flight Recorder (JFR)]\n* xref:whatsnew.adoc#jspecify[Null safety annotations with JSpecify]\n* xref:whatsnew.adoc#local-chunking[Local chunking support]\n* xref:whatsnew.adoc#seda-with-si[SEDA style with Spring Integration message channels]\n* xref:whatsnew.adoc#jackson-3-support[Jackson 3 support]\n* xref:whatsnew.adoc#remote-step-support[Remote step support]\n* xref:whatsnew.adoc#lambda-style-configuration[Lambda style configuration]\n* xref:whatsnew.adoc#deprecations-and-pruning[Deprecations and pruning]\n\n[[dependencies-upgrade]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "whatsnew", "heading_level": 1, "file_order": 64, "section_index": 0, "content_hash": "ba3bec088b347291625b90151b8d201f9aeb0b0b1c056bb1277ffe0b60a9ba51", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:49963e727af779e3dfc6a54d55cba1d2d0f46468c5a3964b121a277e38336a5a", "content": "In this major release, the Spring dependencies are upgraded to the following versions:\n\n* Spring Framework 7.0\n* Spring Integration 7.0\n* Spring Data 4.0\n* Spring LDAP 4.0\n* Spring AMQP 4.0\n* Spring Kafka 4.0\n* Micrometer 1.16\n\n[[batch-infrastrucutre-configuration-improvements]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Dependencies upgrade", "heading_level": 2, "file_order": 64, "section_index": 1, "content_hash": "49963e727af779e3dfc6a54d55cba1d2d0f46468c5a3964b121a277e38336a5a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:0b45b7d0004e53b827606260eb0f8af8e8602e7fc4a0c494b6c43e8b4ef723a4", "content": "Before v6, the `@EnableBatchProcessing` annotation was tied to a JDBC-based infrastructure. This is not the case anymore. Two new annotations have been introduced to configure the underlying job repository: `@EnableJdbcJobRepository` and `@EnableMongoJobRepository`.\n\nStarting from v6, `@EnableBatchProcessing` allows you to configure common attributes for the batch infrastructure, while store-specific attributes can be specified with the new dedicated annotations.\n\nHere is an example of how to use these annotations:\n\n[source, java]\n----\n@EnableBatchProcessing(taskExecutorRef = \"batchTaskExecutor\")\n@EnableJdbcJobRepository(dataSourceRef = \"batchDataSource\", transactionManagerRef = \"batchTransactionManager\")\nclass MyJobConfiguration {\n\n\t@Bean\n\tpublic Job job(JobRepository jobRepository) {\n return new JobBuilder(\"job\", jobRepository)\n // job flow omitted\n .build();\n\t}\n}\n----\n\nSimilarly, the programmatic model based on `DefaultBatchConfiguration` has been updated by introducing two new configuration classes to define store-specific attributes: `JdbcDefaultBatchConfiguration` and `MongoDefaultBatchConfiguration`.\nThese classes can be used to configure specific attributes of each job repository as well as other batch infrastructure beans programmatically.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "New annotations and classes for batch infrastructure configuration", "heading_level": 3, "file_order": 64, "section_index": 2, "content_hash": "0b45b7d0004e53b827606260eb0f8af8e8602e7fc4a0c494b6c43e8b4ef723a4", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:9a34f7b7d51952319e425250cd517c45376918bf17cd6fb0df15f8f072e0c298", "content": "The `DefaultBatchConfiguration` class has been updated to provide a \"resourceless\" batch infrastructure by default (based on the `ResourcelessJobRepository` implementation introduced in v5.2). This means that it no longer requires an in-memory database (like H2 or HSQLDB) for the job repository, which was previously necessary for batch metadata storage.\n\nMoreover, this change will improve the default performance of batch applications when the meta-data is not used, as the `ResourcelessJobRepository` does not require any database connections or transactions.\n\nFinally, this change will help to reduce the memory footprint of batch applications, as the in-memory database is no longer required for metadata storage.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Resourceless batch infrastructure by default", "heading_level": 3, "file_order": 64, "section_index": 3, "content_hash": "9a34f7b7d51952319e425250cd517c45376918bf17cd6fb0df15f8f072e0c298", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:2884786ff29187880c122a4b5484a0e15d2575aae6c804960367cdd2029c60cb", "content": "Before v6, the typical configuration of a non-trivial Spring Batch application was quite complex and required a lot of beans: `JobRepository`, `JobLauncher`, `JobExplorer`, `JobOperator`, `JobRegistry`, `JobRegistrySmartInitializingSingleton` and so on. This required a lot of configuration code, like for example the need to configure the same execution context serializer on both the `JobRepository` and `JobExplorer`.\n\nIn this release, several changes have been made to simplify the batch infrastructure configuration:\n\n* The `JobRepository` now extends the `JobExplorer` interface, so there is no need to define a separate `JobExplorer` bean.\n* The `JobOperator` now extends the `JobLauncher` interface, so there is no need to define a separate `JobLauncher` bean.\n* The `JobRegistry` is now optional, and smart enough to register jobs automatically, so there is no need to define a separate `JobRegistrySmartInitializingSingleton` bean.\n* The transaction manager is now optional, and a default `ResourcelessTransactionManager` is used if none is provided.\n\nThis reduces the number of beans required for a typical batch application and simplifies the configuration code.\n\n[[new-implementation-of-the-chunk-oriented-processing-model]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Batch infrastructure configuration simplification", "heading_level": 3, "file_order": 64, "section_index": 4, "content_hash": "2884786ff29187880c122a4b5484a0e15d2575aae6c804960367cdd2029c60cb", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:6d462a61215dbc5413391ec32876092e1347c852991e9d1ecab6322caa0b3c20", "content": "This is not a new feature, but rather a new implementation of the chunk-oriented processing model. This new implementation was introduced as an experimental addition in version 5.1, and is now available as stable in version 6.0.\n\nThe new implementation is provided in the `ChunkOrientedStep` class, which is a replacement for the `ChunkOrientedTasklet` / `TaskletStep` classes.\n\nHere is an example of how to define a `ChunkOrientedStep` by using its builder:\n\n[source, java]\n----\n@Bean\npublic Step chunkOrientedStep(JobRepository jobRepository, ItemReader<Person> itemReader, ItemWriter<Person> itemWriter) {\n int chunkSize = 100;\n return new ChunkOrientedStepBuilder<Person, Person>(\"step\", jobRepository, chunkSize)\n .reader(itemReader)\n .writer(itemWriter)\n .build();\n}\n----\n\nMoreover, fault-tolerance features were adapted as follows:\n\n- The retry feature is now based on the retry functionality introduced in https://docs.spring.io/spring/reference/7.0/core/resilience.html[Spring Framework 7], instead of the previous Spring Retry library\n- The skip feature has been slightly adapted to the new implementation, which is now only based entirely on the `SkipPolicy` interface\n\nHere is a quick example of how to use the retry and skip features with the new `ChunkOrientedStep`:\n\n[source, java]\n----\n@Bean\npublic Step faultTolerantChunkOrientedStep(JobRepository jobRepository, ItemReader<Person> itemReader, ItemWriter<Person> itemWriter) {\n\n // retry policy configuration\n int maxRetries = 10;\n var retryableExceptions = Set.of(TransientException.class);\n RetryPolicy retryPolicy = RetryPolicy.builder()\n .maxRetries(maxRetries)\n .includes(retryableExceptions)\n .build();\n\n // skip policy configuration\n int skipLimit = 50;\n var skippableExceptions = Set.of(FlatFileParseException.class);\n SkipPolicy skipPolicy = new LimitCheckingExceptionHierarchySkipPolicy(skippableExceptions, skipLimit);\n\n // step configuration\n int chunkSize = 100;\n return new ChunkOrientedStepBuilder<Person, Person>(\"step\", jobRepository, chunkSize)\n .reader(itemReader)\n .writer(itemWriter)\n .faultTolerant()\n .retryPolicy(retryPolicy)\n .skipPolicy(skipPolicy)\n .build();\n}\n----\n\nPlease refer to the https://github.com/spring-projects/spring-batch/wiki/Spring-Batch-6.0-Migration-Guide[migration guide] for more details on how to migrate from the previous implementation to the new one.\n\n[[new-concurrency-model]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "New implementation of the chunk-oriented processing model", "heading_level": 2, "file_order": 64, "section_index": 5, "content_hash": "6d462a61215dbc5413391ec32876092e1347c852991e9d1ecab6322caa0b3c20", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:752f4ab8892b881757c298fbbd44613a011c9ccc4157e3b498c4ecb11a60c62a", "content": "Prior to this release, the concurrency model based on the \"parallel iteration\" concept required a lot of state synchronization at different levels and had several limitations related to throttling and backpressure leading to confusing transaction semantics and poor performance.\n\nThis release revisits that model and comes with a new, simplified approach to concurrency based on the producer-consumer pattern. A concurrent chunk-oriented step now uses a bounded internal queue between the producer thread and consumer threads. Items are put in the queue as soon as they are ready to be processed, and consumer threads take items from the queue as soon as they are available for processing. Once a chunk is ready to be written, the producer thread pauses until the chunk is written, and then resumes producing items.\n\nThis new model is more efficient, easier to understand and provides better performance for concurrent executions.\n\n[[new-command-line-operator]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "New concurrency model", "heading_level": 2, "file_order": 64, "section_index": 6, "content_hash": "752f4ab8892b881757c298fbbd44613a011c9ccc4157e3b498c4ecb11a60c62a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:3221e3695ad7206a6c2402e663c1962f8914cbe3dde193aaf5cf2c505fd8ee9f", "content": "Spring Batch provided a `CommandLineJobRunner` since version 1. While this runner served its purpose well over the years, it started to show some limitations when it comes to extensibility and customisation. Many issues like static initialisation, non-standard way of handling options and parameters, lack of extensibility, etc have been reported.\n\nMoreover, all these issues made it impossible to reuse that runner in Spring Boot, which resulted in duplicate code in both projects as well behaviour divergence (like job parameters incrementer behaviour differences) that is confusing to many users.\n\nThis release introduces a modern version of `CommandLineJobRunner`, named `CommandLineJobOperator`, that allows you to operate batch jobs from the command line (start, stop, restart and so on) and that is customisable, extensible and updated to the new changes introduced in Spring Batch 6.\n\n[[ability-to-recover-failed-job-executions]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "New command line operator", "heading_level": 2, "file_order": 64, "section_index": 7, "content_hash": "3221e3695ad7206a6c2402e663c1962f8914cbe3dde193aaf5cf2c505fd8ee9f", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:eb6dd1ae43ab8b6662102353cebe30413a8d563ad1b0a42e3f1b95185720401c", "content": "Prior to this release, if a job execution fails abruptly, it was not possible to recover it without a manual database update. This was error-prone and not consistent across different job repositories (as it required a few SQL statements for JDBC databases and some custom statements for NoSQL stores).\n\nThis release introduces a new method named `recover` in the `JobOperator` interface that allows you to recover failed job executions consistently across all job repositories.\n\n[[ability-to-stop-all-kind-of-steps]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Ability to recover failed job executions", "heading_level": 2, "file_order": 64, "section_index": 8, "content_hash": "eb6dd1ae43ab8b6662102353cebe30413a8d563ad1b0a42e3f1b95185720401c", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:992947156cc97e3dfad0886f0caa6f1ca090e6b27531f81d6357cad10a44dc58", "content": "As of v5.2, it is only possible to externally stop `Tasklet` steps through `JobOperator#stop`.\nIf a custom `Step` implementation wants to handle external stop signals, it just can't.\n\nThis release adds a new interface, named `StoppableStep`, that extends `Step` and which can be implemented by any step that is able to handle stop signals.\n\n[[graceful-shutdown]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Ability to stop all kinds of steps", "heading_level": 2, "file_order": 64, "section_index": 9, "content_hash": "992947156cc97e3dfad0886f0caa6f1ca090e6b27531f81d6357cad10a44dc58", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:6d79b795853d3478e1bbc40e6c740f084d4695f2261a0190baa05b49caa877d9", "content": "Spring Batch 6.0 introduces support for graceful shutdown of batch jobs. This feature allows you to stop a running job execution in a controlled manner, ensuring that interruption signals are correctly sent to running steps.\n\nWhen a graceful shutdown is initiated, the job execution will stop currently active steps and updates the job repository with a consistent state that enables restartability. Once running steps have finished, the job execution will be marked as stopped, and any necessary cleanup operations will be performed.\n\n[[observability-with-jfr]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Graceful Shutdown support", "heading_level": 2, "file_order": 64, "section_index": 10, "content_hash": "6d79b795853d3478e1bbc40e6c740f084d4695f2261a0190baa05b49caa877d9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:cae9452bf069cb02bd2ecee8a20113b5e1fef13555fb99dfeac06fc143e8904a", "content": "In addition to the existing Micrometer metrics, Spring Batch 6.0 introduces support for the Java Flight Recorder (JFR) to provide enhanced observability capabilities.\n\nJFR is a powerful profiling and event collection framework built into the Java Virtual Machine (JVM). It allows you to capture detailed information about the runtime behavior of your applications with minimal performance overhead.\n\nThis release introduces several JFR events to monitor key aspects of a batch job execution, including job and step executions, item reads and writes, as well as transaction boundaries.\n\n[[jspecify]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Observability with the Java Flight Recorder (JFR)", "heading_level": 2, "file_order": 64, "section_index": 11, "content_hash": "cae9452bf069cb02bd2ecee8a20113b5e1fef13555fb99dfeac06fc143e8904a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:6597a6d324e7b6a831c94f5a9cecebeac5b18f4ffaa7c6c4bfe00110d13eaa25", "content": "Spring Batch 6.0 APIs are now annotated with https://jspecify.dev/[JSpecify] annotations to provide better null-safety guarantees and improve code quality.\n\n[[local-chunking]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Null safety annotations with JSpecify", "heading_level": 2, "file_order": 64, "section_index": 12, "content_hash": "6597a6d324e7b6a831c94f5a9cecebeac5b18f4ffaa7c6c4bfe00110d13eaa25", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:5aff80faea28535fad47f8d9f1716e45c40d35f0dbe534bb81980cafb573e4bb", "content": "Similar to remote chunking, local chunking is a new feature that allows you to process chunks of items in parallel, locally within the same JVM using multiple threads. This is particularly useful when you have a large number of items to process and want to take advantage of multi-core processors.\nWith local chunking, you can configure a chunk-oriented step to use multiple threads to process chunks of items concurrently. Each thread will read, process and write its own chunk of items independently, while the step will manage the overall execution and commit the results.\n\n[[seda-with-si]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Local chunking support", "heading_level": 2, "file_order": 64, "section_index": 13, "content_hash": "5aff80faea28535fad47f8d9f1716e45c40d35f0dbe534bb81980cafb573e4bb", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:af72add5c977bd147f0426e431f3667dc92f7552be7ea92e89ea11b43c739c4d", "content": "In Spring Batch 5.2, we introduced the concept of SEDA (Staged Event-Driven Architecture) style processing using local threads with the `BlockingQueueItemReader` and `BlockingQueueItemWriter` components. Building on that foundation,\nSpring Batch 6.0 introduces support for SEDA style processing at scale using Spring Integration messaging channels. This allows you to decouple the different stages of a batch job and process them asynchronously using message channels. By leveraging Spring Integration, you can easily configure and manage the messaging channels, as well as take advantage of features like message transformation, filtering, and routing.\n\n[[jackson-3-support]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "SEDA style with Spring Integration message channels", "heading_level": 2, "file_order": 64, "section_index": 14, "content_hash": "af72add5c977bd147f0426e431f3667dc92f7552be7ea92e89ea11b43c739c4d", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:2e9e2fb368d0ff3e98d0e3bda83f322bef1756f2519ecb36c3861efc90c49dc9", "content": "Spring Batch 6.0 has been upgraded to support Jackson 3.x for JSON processing. This upgrade ensures compatibility with the latest features and improvements in the Jackson library, while also providing better performance and security. All JSON-related components in Spring Batch, such as the `JsonItemReader` and `JsonFileItemWriter`, as well as the `JacksonExecutionContextStringSerializer` have been updated to use Jackson 3.x by default.\n\nThe support for Jackson 2.x has been deprecated and will be removed in a future release. If you are currently using Jackson 2.x in your Spring Batch applications, it is recommended to upgrade to Jackson 3.x to take advantage of the latest features and improvements.\n\n[[remote-step-support]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Jackson 3 support", "heading_level": 2, "file_order": 64, "section_index": 15, "content_hash": "2e9e2fb368d0ff3e98d0e3bda83f322bef1756f2519ecb36c3861efc90c49dc9", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:bbe4ee55bfc63ddf25f7a0e5c79958e7d07cd097c01544fe575a25820a6bd89a", "content": "This release introduces support for remote step executions, allowing you to execute steps of a batch job on remote machines or clusters.\nThis feature is particularly useful for large-scale batch processing scenarios where you want to distribute the workload across multiple nodes to improve performance and scalability. Remote step execution is facilitated through the use of Spring Integration messaging channels, which enable communication between the local job execution environment and the remote step executors.\n\n[[lambda-style-configuration]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Remote step support", "heading_level": 2, "file_order": 64, "section_index": 16, "content_hash": "bbe4ee55bfc63ddf25f7a0e5c79958e7d07cd097c01544fe575a25820a6bd89a", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:3c8e29e1dc1c57f08a879d933852339ca09185fccb3072d57529c984892e8455", "content": "This release introduces the use of contextual lambda expressions to configure batch artefacts. This new style of configuration provides a more concise and readable way to define item readers and writers.\n\nFor example, instead of using the traditional builder pattern like this:\n\n[source, java]\n----\nvar reader = new FlatFileItemReaderBuilder()\n .resource(...)\n .delimited()\n .delimiter(\",\")\n .quoteCharacter('\"')\n ...\n .build();\n----\n\nYou can now use a lambda expression to configure the delimited options like this:\n\n[source, java]\n----\nvar reader = new FlatFileItemReaderBuilder()\n .resource(...)\n .delimited (config -> config.delimiter(',').quoteCharcter( '\"' ))\n ...\n .build();\n----\n\n[[deprecations-and-pruning]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Lambda style configuration", "heading_level": 2, "file_order": 64, "section_index": 17, "content_hash": "3c8e29e1dc1c57f08a879d933852339ca09185fccb3072d57529c984892e8455", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
{"id": "sha256:b3d6926b5b5d08bebd8f28e18076b328e80bfed55473ed15621b177dc39cd431", "content": "As with any major release, some features have been deprecated or removed in Spring Batch 6.0. The following changes are worth noting:\n\n* All deprecated APIs and features from previous versions have been removed\n* Modular configuration through `@EnableBatchProcessing(modular = true)` has been deprecated\n* Several APIs have been deprecated in this version, in order to simplify the core API and reduce its scope\n* Deprecate JUnit 4 support in the `spring-batch-test` module\n* Deprecate Jackson 2 support\n* Deprecate XML configuration through the `batch:...` namespace\n\nFore more details, please refer to the https://github.com/spring-projects/spring-batch/wiki/Spring-Batch-6.0-Migration-Guide[migration guide].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-batch", "path": "spring-batch-docs/modules/ROOT/pages/whatsnew.adoc", "title": "whatsnew", "heading": "Deprecations and pruning", "heading_level": 2, "file_order": 64, "section_index": 18, "content_hash": "b3d6926b5b5d08bebd8f28e18076b328e80bfed55473ed15621b177dc39cd431", "source_url": "https://github.com/spring-projects/spring-batch/blob/2e9755dca1e1eef9efd99cf6cb4e30727c46dbd6/spring-batch-docs/modules/ROOT/pages/whatsnew.adoc"}}
