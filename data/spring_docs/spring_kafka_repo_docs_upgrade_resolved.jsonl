{"id": "sha256:a2e8212b33f8d047e607ffa85dc35e315b7a7575fb0a6f160089bf8f4673233e", "content": "[[history]]\n\n[[whats-new-in-4-0-since-3-3]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "change-history", "heading_level": 1, "file_order": 0, "section_index": 0, "content_hash": "a2e8212b33f8d047e607ffa85dc35e315b7a7575fb0a6f160089bf8f4673233e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:167aa8857a3fbd7d3ebd485549e65f15cb520a78b256d673e3014c41561a2413", "content": "This section covers the changes made from version 3.3 to version 4.0.\nFor changes in earlier versions, see xref:appendix/change-history.adoc[Change History].\n\n[[x40-apache-kafka-4-0-upgrade]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "What's New in 4.0 Since 3.3", "heading_level": 2, "file_order": 0, "section_index": 1, "content_hash": "167aa8857a3fbd7d3ebd485549e65f15cb520a78b256d673e3014c41561a2413", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:66c79642362fbc38ab1f1c06e472c06a38724b6cfeb77ac48bd68f0cd9509308", "content": "Spring for Apache Kafka has been upgraded to use Apache Kafka client version `4.0.0`.\nThis upgrade brings several important changes:\n\n* All ZooKeeper-based functionality has been removed as Kafka 4.0 fully transitions to KRaft mode\n* The ZooKeeper dependency has been removed from the project\n* The embedded Kafka test framework now exclusively uses KRaft mode\n* The `EmbeddedKafkaZKBroker` class has been removed, and all functionality is now handled by `EmbeddedKafkaKraftBroker`\n\n[[x40-embedded-kafka-test-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Apache Kafka 4.0 Client Upgrade", "heading_level": 3, "file_order": 0, "section_index": 2, "content_hash": "66c79642362fbc38ab1f1c06e472c06a38724b6cfeb77ac48bd68f0cd9509308", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:bedff7b9b4a7d999aed2120ed2dafc1b742f7f8720cda2f1dd4c347dc9c65174", "content": "The test infrastructure has been significantly updated:\n\n* The `EmbeddedKafkaRule` JUnit 4 rule has been removed\n* The `@EmbeddedKafka` annotation has been simplified with the removal of ZooKeeper-related properties:\n* The `kraft` property has been removed as KRaft mode is now the only option\n* ZooKeeper-specific properties like `zookeeperPort`, `zkConnectionTimeout`, and `zkSessionTimeout` have been removed\n* KafkaClusterTestKit imports now use new packages for KRaft mode\n* Some tests have been updated to address limitations with static port assignments in KRaft mode\n* Adjustments have been made to replication factors in tests to accommodate KRaft requirements\n\n[[x40-consumer-records-constructor-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Embedded Kafka Test Framework Changes", "heading_level": 3, "file_order": 0, "section_index": 3, "content_hash": "bedff7b9b4a7d999aed2120ed2dafc1b742f7f8720cda2f1dd4c347dc9c65174", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:5a1e253a20d9b182389dddf04b7110de26f6c9c1d0dbf237bfc38f1a1837d640", "content": "The `ConsumerRecords` constructor now requires an additional `Map` parameter, which has been addressed throughout the framework.\nApplications that directly use this constructor will need to update their code.\n\n[[x40-producer-interface-updates]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ConsumerRecords Constructor Changes", "heading_level": 3, "file_order": 0, "section_index": 4, "content_hash": "5a1e253a20d9b182389dddf04b7110de26f6c9c1d0dbf237bfc38f1a1837d640", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ca433e50dc47c276d3c7256ffc35722537fa81bc71b987194c8f158b894cbbed", "content": "New methods from the Kafka Producer interface have been implemented:\n\n* `registerMetricForSubscription`\n* `unregisterMetricFromSubscription`\n\n[[x40-removed-deprecated-functionality]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Producer Interface Updates", "heading_level": 3, "file_order": 0, "section_index": 5, "content_hash": "ca433e50dc47c276d3c7256ffc35722537fa81bc71b987194c8f158b894cbbed", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:714c653361511851272475900bc52ddb0c0c93cbc00dd3dadcadfc9d8e4e78df", "content": "Several deprecated items have been removed:\n\n* The deprecated `partitioner` classes have been removed from runtime hints\n* The deprecated `sendOffsetsToTransaction` method that used `String consumerGroupId` has been removed\n\n[[x40-kafka-streams-updates]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Removed Deprecated Functionality", "heading_level": 3, "file_order": 0, "section_index": 6, "content_hash": "714c653361511851272475900bc52ddb0c0c93cbc00dd3dadcadfc9d8e4e78df", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:f8ee1c11c8e51cd52e75d138cf8871c8727f0e80511179bc546fe2d90db7285f", "content": "* `KafkaStreamBrancher` has been updated to use the new `split()` and `branch()` methods instead of the deprecated `branch()` method\n* The `DeserializationExceptionHandler` has been updated to use the new `ErrorHandlerContext`\n\n[[x40-internal-api-updates]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Streams API Changes", "heading_level": 3, "file_order": 0, "section_index": 7, "content_hash": "f8ee1c11c8e51cd52e75d138cf8871c8727f0e80511179bc546fe2d90db7285f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:71576bde65a68589d18e0f017ec8a8b8ae8918a5b42725a251a18303247042a9", "content": "* The `BrokerAddress` class now uses `org.apache.kafka.server.network.BrokerEndPoint` instead of the deprecated `kafka.cluster.BrokerEndPoint`\n* The `GlobalEmbeddedKafkaTestExecutionListener` has been updated to work solely with KRaft mode\n\n[[x40-new-consumer-rebalance-protocol]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Internal API Updates related to Apache Kafka 4.0.0", "heading_level": 3, "file_order": 0, "section_index": 8, "content_hash": "71576bde65a68589d18e0f017ec8a8b8ae8918a5b42725a251a18303247042a9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:cc0244c4264ee19756e72a7bf1aed0253be8906ac993e39cbaa96ba06dd5d095", "content": "Spring for Apache Kafka 4.0 supports Kafka 4.0â€™s new consumer rebalance protocol - https://cwiki.apache.org/confluence/display/KAFKA/KIP-848%3A+The+Next+Generation+of+the+Consumer+Rebalance+Protocol[KIP-848].\nFor details, see xref:kafka/receiving-messages/rebalance-listeners.adoc#new-rebalance-protocol[New Consumer Rebalance Protocol docs].\n\n[[x40-multi-value-header]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "New Consumer Rebalance Protocol", "heading_level": 3, "file_order": 0, "section_index": 9, "content_hash": "cc0244c4264ee19756e72a7bf1aed0253be8906ac993e39cbaa96ba06dd5d095", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:a74dab05fd99308566b1bdd4a7cd81a986417c65e97fc8428a0d8ca32d2b8762", "content": "The `JsonKafkaHeaderMapper` and `SimpleKafkaHeaderMapper` support multi-value header mapping for Kafka records.\nMore details are available in xref:kafka/headers.adoc#multi-value-header[Support multi-value header mapping].\n\n[[x40-add-record-interceptor]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Support multi-value header", "heading_level": 3, "file_order": 0, "section_index": 10, "content_hash": "a74dab05fd99308566b1bdd4a7cd81a986417c65e97fc8428a0d8ca32d2b8762", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:698446e57d9c8f44f60c64a49dd3936f107da119ef066c6cd18291a5b4cc7003", "content": "Listener containers now support interceptor customization via `getRecordInterceptor()`.\nSee the xref:kafka/receiving-messages/message-listener-container.adoc#message-listener-container[Message Listener Containers] section for details.\n\n[[x40-batch-observability]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Configure additional `RecordInterceptor`", "heading_level": 3, "file_order": 0, "section_index": 11, "content_hash": "698446e57d9c8f44f60c64a49dd3936f107da119ef066c6cd18291a5b4cc7003", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ba6ac4a4d16c4ae7e613b9e907eb26c7ec663a90dc09f3fb47d31046e1ccfdc7", "content": "It is now possible to get an observation for each record when using a batch listener.\nSee xref:kafka/micrometer.adoc#batch-listener-obs[Observability for Batch Listeners] for more information.\n\n[[x40-kafka-queues]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Per-Record Observation in Batch Listeners", "heading_level": 3, "file_order": 0, "section_index": 12, "content_hash": "ba6ac4a4d16c4ae7e613b9e907eb26c7ec663a90dc09f3fb47d31046e1ccfdc7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:2cfa04bac168759fac2fa027151925d9f8f1157ad72538a315947ddda655fb02", "content": "Spring for Apache Kafka now provides early access support for Kafka Queues through share consumers, which are part of Apache Kafka 4.0.0 and implement KIP-932.\nThis enables cooperative consumption where multiple consumers can consume from the same partitions simultaneously, providing better load distribution compared to traditional consumer groups.\nSee xref:kafka/kafka-queues.adoc[] for more information.\n\n[[x40-jackson3-support]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Queues (Share Consumer) Support", "heading_level": 3, "file_order": 0, "section_index": 13, "content_hash": "2cfa04bac168759fac2fa027151925d9f8f1157ad72538a315947ddda655fb02", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:75300064a39ceafe61463aa092f76706ec16ef586994d2d52b8cd3f79ce8e182", "content": "Spring for Apache Kafka now provides comprehensive support for Jackson 3 alongside existing Jackson 2 support.\nJackson 3 is automatically detected and preferred when available, providing enhanced performance and modern JSON processing capabilities.\n\nAll Jackson 2 classes now have Jackson 3 counterparts with consistent naming and improved type safety:\n\n* `JsonKafkaHeaderMapper` replaces `DefaultKafkaHeaderMapper`\n* `JacksonJsonSerializer/Deserializer` replaces `JsonSerializer/Deserializer`\n* `JacksonJsonSerde` replaces `JsonSerde`\n* `JacksonJsonMessageConverter` family replaces `JsonMessageConverter` family\n* `JacksonProjectingMessageConverter` replaces `ProjectingMessageConverter`\n* `DefaultJacksonJavaTypeMapper` replaces `DefaultJackson2JavaTypeMapper`\n\nThe new Jackson 3 classes use `JsonMapper` instead of generic `ObjectMapper` for enhanced type safety and leverage Jackson 3's improved module system and performance optimizations.\n\n**Migration Path**: Existing applications continue to work unchanged with Jackson 2.\nTo migrate to Jackson 3, simply add Jackson 3 to your classpath and update class references to use the new Jackson 3 equivalents.\nThe framework automatically detects and prefers Jackson 3 when both versions are present.\n\n**Backward Compatibility**: All Jackson 2 classes are deprecated but remain fully functional.\nThey will be removed in a future major version.\n\nSee xref:kafka/serdes.adoc[Serialization, Deserialization, and Message Conversion] for configuration examples.\n\n[[x40-spring-retry-replacement]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Jackson 3 Support", "heading_level": 3, "file_order": 0, "section_index": 14, "content_hash": "75300064a39ceafe61463aa092f76706ec16ef586994d2d52b8cd3f79ce8e182", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:b19263b55b0913ae96aaf52580f4973158a05ede019d2b47b806e1f420d84130", "content": "Spring for Apache Kafka has removed its dependency on Spring Retry in favor of the core retry support introduced in Spring Framework 7.\nThis is a breaking change that affects retry configuration and APIs throughout the framework.\n\n`BackOffValuesGenerator` that generates the required `BackOff` values upfront, now works directly with Spring Framework's `BackOff` interface instead of `BackOffPolicy`.\nThese values are then managed by the listener infrastructure and Spring Retry is no longer involved.\n\nFrom a configuration standpoint, Spring Kafka relied heavily on Spring Retry's `@Backoff` annotation.\nAs there is no equivalent in Spring Framework, the annotation has been moved to Spring Kafka as `@BackOff` with the following improvements:\n\n* Harmonized naming: Uses `@BackOff` instead of `@Backoff` for consistency\n* Expression evaluation: All string attributes support SpEL expressions and property placeholders\n* Duration format support: String attributes accept `java.util.Duration` formats (e.g., \"2s\", \"500ms\")\n* Enhanced documentation: Improved Javadoc with clearer explanations\n\nMigration example:\n[source,java]\n----\n@RetryableTopic(backoff = @Backoff(delay = 2000, maxDelay = 10000, multiplier = 2))\n\n@RetryableTopic(backOff = @BackOff(delay = 2000, maxDelay = 10000, multiplier = 2))\n\n@RetryableTopic(backOff = @BackOff(delayString = \"2s\", maxDelayString = \"10s\", multiplier = 2))\n\n@RetryableTopic(backOff = @BackOff(delayString = \"${retry.delay}\", multiplierString = \"${retry.multiplier}\"))\n----\n\n`RetryingDeserializer` no longer offers a `RecoveryCallback` but an equivalent function that takes `RetryException` as input.\nThis contains the exceptions thrown as well as the number of retry attempts:\n\n[source,java]\n----\nretryingDeserializer.setRecoveryCallback(context -> {\n return fallbackValue;\n});\n\nretryingDeserializer.setRecoveryCallback(retryException -> {\n return fallbackValue;\n});\n----\n\nThe use of `BinaryExceptionClassifier` has been replaced by the newly introduced `ExceptionMatcher`, which provides a polished API.\n\nAdditional changes include:\n\n* `DestinationTopicPropertiesFactory` uses `ExceptionMatcher` instead of `BinaryExceptionClassifier`\n* The `uniformRandomBackoff` method in `RetryTopicConfigurationBuilder` has been deprecated in favor of jitter support\n* Error handling utilities have been updated to work with the new exception matching system\n* Kafka Streams retry templates now use Spring Framework's retry support\n\nApplications must update their configuration to use the new Spring Framework retry APIs, but the retry behavior and functionality remain the same.\n\n[[what-s-new-in-3-3-since-3-2]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Spring Retry Dependency Removal", "heading_level": 3, "file_order": 0, "section_index": 15, "content_hash": "b19263b55b0913ae96aaf52580f4973158a05ede019d2b47b806e1f420d84130", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:4cf517cec8ea48b0d7a56d72aef686675461088430f8e0f259ee8ae8a6884f2b", "content": "This section covers the changes made from version 3.2 to version 3.3.\nFor changes in earlier version, see xref:appendix/change-history.adoc[Change History].\n\n[[x33-dlt-topic-naming]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "What's New in 3.3 Since 3.2", "heading_level": 2, "file_order": 0, "section_index": 16, "content_hash": "4cf517cec8ea48b0d7a56d72aef686675461088430f8e0f259ee8ae8a6884f2b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:90eb94d3bd8a87f48e4f238892d02bd0cf303d44d6197d8a51c1b258c904cd9f", "content": "The naming convention for DLT topics has been standardized to use the \"-dlt\" suffix consistently. This change ensures compatibility and avoids conflicts when transitioning between different retry solutions. Users who wish to retain the \".DLT\" suffix behavior need to opt-in explicitly by setting the appropriate DLT name property.\n\n[[x33-seek-with-group-id]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "DLT Topic Naming Convention", "heading_level": 3, "file_order": 0, "section_index": 17, "content_hash": "90eb94d3bd8a87f48e4f238892d02bd0cf303d44d6197d8a51c1b258c904cd9f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:197ce3577142b3822c4098346f11e225aebedd909ef646a7b72c37ec515239a6", "content": "A new method, `getGroupId()`, has been added to the `ConsumerSeekCallback` interface.\nThis method allows for more selective seek operations by targeting only the desired consumer group.\nThe `AbstractConsumerSeekAware` can also now register, retrieve, and remove all callbacks for each topic partition in a multi-group listener scenario without missing any.\nSee the new APIs (`getSeekCallbacksFor(TopicPartition topicPartition)`, `getTopicsAndCallbacks()`) for more details.\nFor more details, see xref:kafka/seek.adoc#seek[Seek API Docs].\n\n[[x33-new-option-ignore-empty-batch]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Enhanced Seek Operations for Consumer Groups", "heading_level": 3, "file_order": 0, "section_index": 18, "content_hash": "197ce3577142b3822c4098346f11e225aebedd909ef646a7b72c37ec515239a6", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:80290958a4f5b24753a8e19f3a8bc7d51ebd47878e19e14d6b53741411ff62e6", "content": "`RecordFilterStrategy` now supports ignoring empty batches that result from filtering.\nThis can be configured through overriding default method `ignoreEmptyBatch()`, which defaults to false, ensuring `KafkaListener` is invoked even if all `ConsumerRecords` are filtered out.\nFor more details, see xref:kafka/receiving-messages/filtering.adoc[Message receive filtering Docs].\n\n[[x33-concurrent-container-stopped-event]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Configurable Handling of Empty Batches in Kafka Listener with RecordFilterStrategy", "heading_level": 3, "file_order": 0, "section_index": 19, "content_hash": "80290958a4f5b24753a8e19f3a8bc7d51ebd47878e19e14d6b53741411ff62e6", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:41b0ac0148e7cdad38da11021e8dd17ade673d61d98dc1d11e9104d11286ced4", "content": "The `ConcurentContainerMessageListenerContainer` emits now a `ConcurrentContainerStoppedEvent` when all of its child containers are stopped.\nFor more details, see xref:kafka/events.adoc[Application Events] and `ConcurrentContainerStoppedEvent` Javadocs.\n\n[[x33-original-record-key-in-reply]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ConcurrentContainerStoppedEvent", "heading_level": 3, "file_order": 0, "section_index": 20, "content_hash": "41b0ac0148e7cdad38da11021e8dd17ade673d61d98dc1d11e9104d11286ced4", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:340b34102978bf0ef9eb2666344fbf66c00ef0a120ed50fbd05f7adf2b864f68", "content": "When using `ReplyingKafkaTemplate`, if the original record from the request contains a key, then that same key will be part of the reply as well.\nFor more details, see xref:kafka/sending-messages.adoc[Sending Messages] section of the reference docs.\n\n[[x33-customize-logging-in-DeadLetterPublishingRecovererFactory]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Original Record Key in Reply", "heading_level": 3, "file_order": 0, "section_index": 21, "content_hash": "340b34102978bf0ef9eb2666344fbf66c00ef0a120ed50fbd05f7adf2b864f68", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:4b9a7fe13cb80aff5e8013e9e557fcd5bf92210cec2314ddb7518c05a2af9e70", "content": "When using `DeadLetterPublishingRecovererFactory`, the user applications can override the `maybeLogListenerException` method to customize the logging behavior.\n\n[[x33-customize-admin-client-in-KafkaAdmin]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Customizing Logging in DeadLetterPublishingRecovererFactory", "heading_level": 3, "file_order": 0, "section_index": 22, "content_hash": "4b9a7fe13cb80aff5e8013e9e557fcd5bf92210cec2314ddb7518c05a2af9e70", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:85c1ebdeb205961a27f95a4705365531e1e8748102082f9d5c90183df55147ac", "content": "When extending `KafkaAdmin`, user applications may override the `createAdmin` method to customize Admin client creation.\n\n[[x33-customize-kafka-streams-implementation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Customize Admin client in KafkaAdmin", "heading_level": 3, "file_order": 0, "section_index": 23, "content_hash": "85c1ebdeb205961a27f95a4705365531e1e8748102082f9d5c90183df55147ac", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:f025d6235cd827a9928d880c7f003bfe21bfb172f3d1a079c0925992902f696d", "content": "When using `KafkaStreamsCustomizer` it is now possible to return a custom implementation of the `KafkaStreams` object by overriding the `initKafkaStreams` method.\n\n[[x33-kafka-headers-for-batch-listeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Customizing The Implementation of Kafka Streams", "heading_level": 3, "file_order": 0, "section_index": 24, "content_hash": "f025d6235cd827a9928d880c7f003bfe21bfb172f3d1a079c0925992902f696d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:29e64a2a031375f16918a56ab1dc81c993dedb0287ff7f03767a2f64c090878c", "content": "When using a `BatchListener`, the `ConsumerRecord` can have the `KafkaHeaders.DELIVERY_ATTMPT` header in its headers fields.\nIf the `DeliveryAttemptAwareRetryListener` is set to error handler as retry listener, each `ConsumerRecord` has delivery attempt header.\nFor more details, see xref:kafka/annotation-error-handling.adoc#delivery-attempts-header-for-batch-listener[Kafka Headers for Batch Listener].\n\n[[x33-task-scheduler-for-kafka-metrics]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "KafkaHeaders.DELIVERY_ATTEMPT for batch listeners", "heading_level": 3, "file_order": 0, "section_index": 25, "content_hash": "29e64a2a031375f16918a56ab1dc81c993dedb0287ff7f03767a2f64c090878c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7ed66dae71f92c3de76ed038e1d9b739abe808ab2cab49c3bf907e250161c661", "content": "The `MicrometerProducerListener`, `MicrometerConsumerListener` and `KafkaStreamsMicrometerListener` can now be configured with a `TaskScheduler`.\nSee `KafkaMetricsSupport` JavaDocs and xref:kafka/micrometer.adoc[Micrometer Support] for more information.\n\n[[what-s-new-in-3-2-since-3-1]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Metrics Listeners and `TaskScheduler`", "heading_level": 3, "file_order": 0, "section_index": 26, "content_hash": "7ed66dae71f92c3de76ed038e1d9b739abe808ab2cab49c3bf907e250161c661", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:47a38185a0f2b1140c1e0f3df1ba33ee2bcdd54f7e91100669dbe0db4841bad9", "content": "This section covers the changes made from version 3.1 to version 3.2.\nFor changes in earlier version, see xref:appendix/change-history.adoc[Change History].\n\n[[x32-kafka-client-version]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "What's New in 3.2 Since 3.1", "heading_level": 2, "file_order": 0, "section_index": 27, "content_hash": "47a38185a0f2b1140c1e0f3df1ba33ee2bcdd54f7e91100669dbe0db4841bad9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:a135590b9742f156213fb74ae8164636546ff0385348f5d1e30463d5b63481d7", "content": "This version requires 3.7.0 `kafka-clients`.\nThe 3.7.0 version of Kafka client introduces the new consumer group protocol.\nFore more details and it's limitations see https://cwiki.apache.org/confluence/display/KAFKA/The+Next+Generation+of+the+Consumer+Rebalance+Protocol+%28KIP-848%29+-+Early+Access+Release+Notes[KIP-848].\nThe new consumer group protocol is an early access release and not meant to be used in production.\nIt is only recommended to use for testing purposes in this version.\nTherefore, Spring for Apache Kafka supports this new consumer group protocol only to the extent of such testing level support available in the `kafka-client` itself.\nBy default, Spring for Apache Kafka uses the classic consumer group protocol and when testing the new consumer group protocol, that needs to be opted-in via the `group.protocol` property on the consumer.\n\n[[x32-testing-support-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 28, "content_hash": "a135590b9742f156213fb74ae8164636546ff0385348f5d1e30463d5b63481d7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:299cf1c8ebc0b88a00b28fc446fa2b7ea1ef355cb8b0a63c0ac7ddcaf4b43396", "content": "The `kraft` mode is disabled in `EmbeddedKafka` by default and users wanting to use the `kraft` mode must enable it.\nThis is due to certain instabilities observed while using `EmbeddedKafka` in `kraft` mode, especially when testing the new consumer group protocol.\nThe new consumer group protocol is only supported in `kraft` mode and because of this, when testing the new protocol, that needs to be done against a real Kafka cluster and not the one based on the `KafkaClusterTestKit`, which `EmbeddedKafka` is based upon.\nIn addition, there were some other race conditions observed, while running multiple `KafkaListener` methods with `EmbeddedKafka` in `kraft` mode.\nUntil these issues are resolved, the `kraft` default on `EmbeddedKafka` will remain as `false`.\n\n[[x32-kafka-streams-iqs-support]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Testing Support Changes", "heading_level": 3, "file_order": 0, "section_index": 29, "content_hash": "299cf1c8ebc0b88a00b28fc446fa2b7ea1ef355cb8b0a63c0ac7ddcaf4b43396", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:b9926d584ad84b98b4b223e71bbb62f365a2b061b17663c06cb67b37348054bf", "content": "A new API `KafkaStreamsInteractiveQuerySupport` for accessing queryable stores used in Kafka Streams interactive queries.\nSee xref:streams.adoc#kafka-streams-iq-support[Kafka Streams Interactive Support] for more details.\n\n[[x32-tiss]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Streams Interactive Query Support", "heading_level": 3, "file_order": 0, "section_index": 30, "content_hash": "b9926d584ad84b98b4b223e71bbb62f365a2b061b17663c06cb67b37348054bf", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:6d2f2c54b4d3455b17afa70d1eb7f6b3a4d68e3d44cc8ca8929658ce50cb4406", "content": "A new `TransactionIdSuffixStrategy` interface was introduced to manage `transactional.id` suffix.\nThe default implementation is `DefaultTransactionIdSuffixStrategy` when setting `maxCache` greater than zero can reuse `transactional.id` within a specific range, otherwise suffixes will be generated on the fly by incrementing a counter.\nSee xref:kafka/transactions.adoc#transaction-id-suffix-fixed[Fixed TransactionIdSuffix] for more information.\n\n[[x32-async-return]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "TransactionIdSuffixStrategy", "heading_level": 3, "file_order": 0, "section_index": 31, "content_hash": "6d2f2c54b4d3455b17afa70d1eb7f6b3a4d68e3d44cc8ca8929658ce50cb4406", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:d2e4ce3197de09ffadbe6db50ca27597fe09b78dc6cd7e8c30374a38227fe3d1", "content": "`@KafkaListener` (and `@KafkaHandler`) methods can now return asynchronous return types include `CompletableFuture<?>`, `Mono<?>` and Kotlin `suspend` functions.\nSee xref:kafka/receiving-messages/async-returns.adoc[Async Returns] for more information.\n\n[[x32-customizable-dlt-routing]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Async @KafkaListener Return", "heading_level": 3, "file_order": 0, "section_index": 32, "content_hash": "d2e4ce3197de09ffadbe6db50ca27597fe09b78dc6cd7e8c30374a38227fe3d1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:0392df274e8d7b8baee3b3ba382835f2d1f8fdc245067df02cbc1fc169571da8", "content": "It's now possible to redirect messages to the custom DLTs based on the type of the exception, which has been thrown during the message processing.\nRules for the redirection are set either via the `RetryableTopic.exceptionBasedDltRouting` or the `RetryTopicConfigurationBuilder.dltRoutingRules`.\nCustom DLTs are created automatically as well as other retry and dead-letter topics.\nSee xref:retrytopic/features.adoc#exc-based-custom-dlt-routing[Routing of messages to custom DLTs based on thrown exceptions] for more information.\n\n[[x32-cp-ptm]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Routing of messages to custom DLTs based on thrown exceptions", "heading_level": 3, "file_order": 0, "section_index": 33, "content_hash": "0392df274e8d7b8baee3b3ba382835f2d1f8fdc245067df02cbc1fc169571da8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:676089b20bc2840243f876c48a3ba948dd716a739631b026aba87bd9d20acf02", "content": "Deprecating the `transactionManager` property in `ContainerProperties` in favor of `KafkaAwareTransactionManager`, a narrower type compared to the general `PlatformTransactionManager`. See xref:kafka/container-props.adoc#kafkaAwareTransactionManager[ContainerProperties] and xref:kafka/transactions.adoc#transaction-synchronization[Transaction Synchronization].\n\n[[x32-after-rollback-processing]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Deprecating ContainerProperties transactionManager property", "heading_level": 3, "file_order": 0, "section_index": 34, "content_hash": "676089b20bc2840243f876c48a3ba948dd716a739631b026aba87bd9d20acf02", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:3c9480b1237861ef585b6d6d835fc45662122af24dfc6db7fcc4aff1e7607e02", "content": "A new `AfterRollbackProcessor` API `processBatch` is provided.\nSee xref:kafka/annotation-error-handling.adoc#after-rollback[After-rollback Processor] for more information.\n\n[[x32-retry-topic]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "After Rollback Processing", "heading_level": 3, "file_order": 0, "section_index": 35, "content_hash": "3c9480b1237861ef585b6d6d835fc45662122af24dfc6db7fcc4aff1e7607e02", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:19145dd44aeb4b24f78aa15ea3a0dd5d6ff7fcda5d320116ffd2867b27bad3b7", "content": "Change `@RetryableTopic` property `SameIntervalTopicReuseStrategy` default value to `SINGLE_TOPIC`.\nSee xref:retrytopic/topic-naming.adoc#single-topic-maxinterval-delay[Single Topic for maxInterval Exponential Delay].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Change @RetryableTopic SameIntervalTopicReuseStrategy default value", "heading_level": 3, "file_order": 0, "section_index": 36, "content_hash": "19145dd44aeb4b24f78aa15ea3a0dd5d6ff7fcda5d320116ffd2867b27bad3b7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:02aefc2974ec71351487ed6a2ca49e998c4258169479277eae5fa233017a79ce", "content": "Non-blocking retries support xref:kafka/receiving-messages/class-level-kafkalistener.adoc[@KafkaListener on a Class].\nSee xref:retrytopic.adoc[Non-Blocking Retries].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Non-blocking retries support class level @KafkaListener", "heading_level": 3, "file_order": 0, "section_index": 37, "content_hash": "02aefc2974ec71351487ed6a2ca49e998c4258169479277eae5fa233017a79ce", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:c8122f6e9e95c3005c856e34319d683b3e2fca059388c1eb605bb98065c36cb0", "content": "Provides a new public API to find `RetryTopicConfiguration`.\nSee xref:retrytopic/retry-config.adoc#find-retry-topic-config[Find RetryTopicConfiguration]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Support process @RetryableTopic on a class in RetryTopicConfigurationProvider.", "heading_level": 3, "file_order": 0, "section_index": 38, "content_hash": "c8122f6e9e95c3005c856e34319d683b3e2fca059388c1eb605bb98065c36cb0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:3891d2f96c86d53f2e8949e918c25099864c18893264cb43ee54557ce136cd93", "content": "The `RetryTopicConfigurer` support process and register `MultiMethodKafkaListenerEndpoint`.\nThe `MultiMethodKafkaListenerEndpoint` provides `getter/setter` for properties `defaultMethod` and `methods`.\nModify the `EndpointCustomizer` that strictly for `MethodKafkaListenerEndpoint` types.\nThe `EndpointHandlerMethod` add new constructors construct an instance for the provided bean.\nProvides new class `EndpointHandlerMultiMethod` to handler multi method for retrying endpoints.\n\n[[x32-seek-offset-compute-fn]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "RetryTopicConfigurer support process MultiMethodKafkaListenerEndpoint.", "heading_level": 3, "file_order": 0, "section_index": 39, "content_hash": "3891d2f96c86d53f2e8949e918c25099864c18893264cb43ee54557ce136cd93", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:e8afaed1aa9ca2f03e9f0eae647045ec7567420992a6811920c542e1441db50f", "content": "`ConsumerCallback` provides a new API to seek to an offset based on a user-defined function, which takes the current offset in the consumer as an argument.\nSee xref:kafka/seek.adoc#seek[Seek API Docs] for more details.\n\n[[x32-annotation-partition-offset-seek-position]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "New API method to seek to an offset based on a user provided function", "heading_level": 3, "file_order": 0, "section_index": 40, "content_hash": "e8afaed1aa9ca2f03e9f0eae647045ec7567420992a6811920c542e1441db50f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ea811e86eb6e2c4392777bab6ec518d90b08d2eda14aa82be6f6d574733221b5", "content": "Adding `seekPosition` property to `@PartitionOffset` support for `TopicPartitionOffset.SeekPosition`.\nSee xref:kafka/receiving-messages/listener-annotation.adoc#manual-assignment[manual-assignment] for more details.\n\n[[x32-topic-partition-offset-constructor]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "@PartitionOffset support for SeekPosition", "heading_level": 3, "file_order": 0, "section_index": 41, "content_hash": "ea811e86eb6e2c4392777bab6ec518d90b08d2eda14aa82be6f6d574733221b5", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:e65b9db089b1c41ebe72fc2dbbb9c80b0934c846277865080c0ab6c36b8f07c7", "content": "`TopicPartitionOffset` has a new constructor that takes a user-provided function to compute the offset to seek to.\nWhen this constructor is used, the framework calls the function with the input argument of the current consumer offset position.\nSee xref:kafka/seek.adoc#seek[Seek API Docs] for more details.\n\n[[x32-default-clientid-prefix]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "New constructor in TopicPartitionOffset that accepts a function to compute the offset to seek to", "heading_level": 3, "file_order": 0, "section_index": 42, "content_hash": "e65b9db089b1c41ebe72fc2dbbb9c80b0934c846277865080c0ab6c36b8f07c7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:15cb631fd7199d234bc08fee2d58b72fcd8d94846f1062bb65708e0bfc018dbb", "content": "For Spring Boot applications which define an application name, this name is now used\nas a default prefix for auto-generated client IDs for certain client types.\nSee xref:kafka/connecting.adoc#default-client-id-prefixes[Default client ID prefixes] for more details.\n\n[[get-listener-containers-matching]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Spring Boot application name as default client ID prefix", "heading_level": 3, "file_order": 0, "section_index": 43, "content_hash": "15cb631fd7199d234bc08fee2d58b72fcd8d94846f1062bb65708e0bfc018dbb", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:d75e95215cdbd65cc431a4d2d1c90c8ca1441f9c66f4d0a43d496caed85deda8", "content": "`ListenerContainerRegistry` provides two new API's dynamically find and filter `MessageListenerContainer` instances.\n`getListenerContainersMatching(Predicate<String> idMatcher)` to filter by ID and the other is\n`getListenerContainersMatching(BiPredicate<String, MessageListenerContainer> matcher)` to filter by ID and container properties.\n\nSee xref:kafka/receiving-messages/kafkalistener-lifecycle.adoc#retrieving-message-listener-containers[`@KafkaListener` Lifecycle Management's API Docs] for more information.\n\n[[x32-observation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Enhanced Retrieval of MessageListenerContainers", "heading_level": 2, "file_order": 0, "section_index": 44, "content_hash": "d75e95215cdbd65cc431a4d2d1c90c8ca1441f9c66f4d0a43d496caed85deda8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:edc82d3adcdb5a9b78485dcbb4fe0d4fd1c91aae4d2745d4fa12579c22b26eaa", "content": "`KafkaTemplateObservation` provides more tracing tags(low cardinality).\n`KafkaListenerObservation` provides a new API to find high cardinality key names and more tracing tags(high or low cardinality).\nSee xref:kafka/micrometer.adoc#observation[Micrometer Observation]\n\n[[what-s-new-in-3-1-since-3-0]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Enhanced observation by providing more tracing tags", "heading_level": 2, "file_order": 0, "section_index": 45, "content_hash": "edc82d3adcdb5a9b78485dcbb4fe0d4fd1c91aae4d2745d4fa12579c22b26eaa", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:36612c1582e122eea57ccb3538922be2a2d327a79d4633d89715cc8d92ea99e1", "content": "This section covers the changes made from version 3.0 to version 3.1.\nFor changes in earlier version, see xref:appendix/change-history.adoc[Change History].\n\n[[x31-kafka-client]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "What's New in 3.1 Since 3.0", "heading_level": 2, "file_order": 0, "section_index": 46, "content_hash": "36612c1582e122eea57ccb3538922be2a2d327a79d4633d89715cc8d92ea99e1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:1bc69d3e8022f0695ebdd3d7dd7f5189031ba9fb08b4ef79b13eb77c16c5680c", "content": "This version requires the 3.6.0 `kafka-clients`.\n\n[[x31-ekb]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 47, "content_hash": "1bc69d3e8022f0695ebdd3d7dd7f5189031ba9fb08b4ef79b13eb77c16c5680c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:88de755ff8debea2687c7b1a27e19fc2f985acb5e16aaabe87f2f33cbc85d5fd", "content": "An additional implementation is now provided to use `Kraft` instead of Zookeeper.\nSee xref:testing.adoc#ekb[Embedded Kafka Broker] for more information.\n\n[[x31-jd]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "EmbeddedKafkaBroker", "heading_level": 3, "file_order": 0, "section_index": 48, "content_hash": "88de755ff8debea2687c7b1a27e19fc2f985acb5e16aaabe87f2f33cbc85d5fd", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:4ea1e3307b10b90d859fc84df5bf9c334020261192289fc18185df32436ed402", "content": "When a deserialization exception occurs, the `SerializationException` message no longer contains the data with the form `Can't deserialize data [[123, 34, 98, 97, 122, ...`; an array of numerical values for each data byte is not useful and can be verbose for large data.\nWhen used with an `ErrorHandlingDeserializer`, the `DeserializationException` sent to the error handler contains the `data` property which contains the raw data that could not be deserialized.\nWhen not used with an `ErrorHandlingDeserializer`, the `KafkaConsumer` will continually emit exceptions for the same record showing the topic/partition/offset and the cause thrown by Jackson.\n\n[[x31-cpp]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "JsonDeserializer", "heading_level": 3, "file_order": 0, "section_index": 49, "content_hash": "4ea1e3307b10b90d859fc84df5bf9c334020261192289fc18185df32436ed402", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7498a3a725f45e15c268ff68c2d4393aef6d8844fd5332bd686cc38f1e77a87d", "content": "Post-processing can be applied on a listener container by specifying the bean name of a `ContainerPostProcessor` on the `@KafkaListener` annotation.\nThis occurs after the container has been created and after any configured `ContainerCustomizer` configured on the container factory.\nSee xref:kafka/container-factory.adoc[Container Factory] for more information.\n\n[[x31-ehd]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ContainerPostProcessor", "heading_level": 3, "file_order": 0, "section_index": 50, "content_hash": "7498a3a725f45e15c268ff68c2d4393aef6d8844fd5332bd686cc38f1e77a87d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:1e1bfa3b304c7a4c057bebf84d5d819fdaa54fd86e2ed283a3f52e6ce8b5e0f0", "content": "You can now add a `Validator` to this deserializer; if the delegate `Deserializer` successfully deserializes the object, but that object fails validation, an exception is thrown similar to a deserialization exception occurring.\nThis allows the original raw data to be passed to the error handler.\nSee xref:kafka/serdes.adoc#error-handling-deserializer[Using `ErrorHandlingDeserializer`] for more information.\n\n[[x31-retryable]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ErrorHandlingDeserializer", "heading_level": 3, "file_order": 0, "section_index": 51, "content_hash": "1e1bfa3b304c7a4c057bebf84d5d819fdaa54fd86e2ed283a3f52e6ce8b5e0f0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:3a772a5be194ece95e8071aced8a7f844825b3d294e9ea2ccb80a9ce62857bd9", "content": "Change suffix `-retry-5000` to `-retry` when `@RetryableTopic(backOff = @BackOff(delay = 5000), attempts = \"2\", fixedDelayTopicStrategy = FixedDelayStrategy.SINGLE_TOPIC)`.\nIf you want to keep suffix `-retry-5000`, use `@RetryableTopic(backOff = @BackOff(delay = 5000), attempts = \"2\")`.\nSee xref:retrytopic/topic-naming.adoc[Topic Naming] for more information.\n\n[[x31-c]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Retryable Topics", "heading_level": 3, "file_order": 0, "section_index": 52, "content_hash": "3a772a5be194ece95e8071aced8a7f844825b3d294e9ea2ccb80a9ce62857bd9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:147450aeebd731da28b09df0e567f2f6f806e53e4d625f2da43615a866d0efd7", "content": "When manually assigning partitions, with a `null` consumer `group.id`, the `AckMode` is now automatically coerced to `MANUAL`.\nSee xref:tips.adoc#tip-assign-all-parts[Manually Assigning All Partitions] for more information.\n\n[[what-s-new-in-3-0-since-2-9]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container Changes", "heading_level": 3, "file_order": 0, "section_index": 53, "content_hash": "147450aeebd731da28b09df0e567f2f6f806e53e4d625f2da43615a866d0efd7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:2275c07d183336ac1461054d5b23e86f25fd048e00781568fcd7498678c81bd3", "content": "[[x30-kafka-client]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "What's New in 3.0 Since 2.9", "heading_level": 2, "file_order": 0, "section_index": 54, "content_hash": "2275c07d183336ac1461054d5b23e86f25fd048e00781568fcd7498678c81bd3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ee8fff4e82770a07d1f57a5e5223d17ca66fe4915ea61c7b871f1543d525d5f0", "content": "This version requires the 3.3.1 `kafka-clients`.\n\n[[x30-eos]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 55, "content_hash": "ee8fff4e82770a07d1f57a5e5223d17ca66fe4915ea61c7b871f1543d525d5f0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:e5f1ce7ff9fd3e359a07abe171a2538c421c197e9cb3e577e59357c6eff06b52", "content": "`EOSMode.V1` (aka `ALPHA`) is no longer supported.\n\nIMPORTANT: When using transactions, the minimum broker version is 2.5.\n\nSee xref:kafka/exactly-once.adoc[Exactly Once Semantics] and https://cwiki.apache.org/confluence/display/KAFKA/KIP-447%3A+Producer+scalability+for+exactly+once+semantics[KIP-447] for more information.\n\n[[x30-obs]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Exactly Once Semantics", "heading_level": 3, "file_order": 0, "section_index": 56, "content_hash": "e5f1ce7ff9fd3e359a07abe171a2538c421c197e9cb3e577e59357c6eff06b52", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:4a63b11669a91b9b3c25e9de3dcf77b7bb08135c79cfc4d2a999b34e392f32fa", "content": "Enabling observation for timers and tracing using Micrometer is now supported.\nSee xref:appendix/change-history.adoc#x30-obs[Observation] for more information.\n\n[[x30-Native]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Observation", "heading_level": 3, "file_order": 0, "section_index": 57, "content_hash": "4a63b11669a91b9b3c25e9de3dcf77b7bb08135c79cfc4d2a999b34e392f32fa", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:b3066d3fad14199900ca2c2cc15adf6a7c8bbc8e4d0cc28e1259d73e82002098", "content": "Support for creating native images is provided.\nSee xref:appendix/change-history.adoc#x30-Native[Native Images] for more information.\n\n[[x30-global-embedded-kafka]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Native Images", "heading_level": 3, "file_order": 0, "section_index": 58, "content_hash": "b3066d3fad14199900ca2c2cc15adf6a7c8bbc8e4d0cc28e1259d73e82002098", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:a3ab95f57d417f28e90dde3ab30283f25de75604e4bb7a1c328820d1d0a99c82", "content": "The embedded Kafka (`EmbeddedKafkaBroker`) can now be start as a single global instance for the whole test plan.\nSee xref:testing.adoc#same-broker-multiple-tests[Using the Same Broker(s) for Multiple Test Classes] for more information.\n\n[[x30-retryable]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Global Single Embedded Kafka", "heading_level": 3, "file_order": 0, "section_index": 59, "content_hash": "a3ab95f57d417f28e90dde3ab30283f25de75604e4bb7a1c328820d1d0a99c82", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:a142230f746be9cd71fbe80695e249e7d16cf74e686734dbb54bca04a50ccdac", "content": "This feature is no longer considered experimental (as far as its API is concerned), the feature itself has been supported since 2.7, but with a greater than normal possibility of breaking API changes.\n\nThe bootstrapping of xref:retrytopic.adoc[Non-Blocking Retries] infrastructure beans has changed in this release to avoid some timing problems that occurred in some application regarding application initialization.\n\nYou can now set a different `concurrency` for the retry containers; by default, the concurrency is the same as the main container.\n\n`@RetryableTopic` can now be used as a meta-annotation on custom annotations, including support for `@AliasFor` properties.\n\nSee xref:retrytopic/retry-config.adoc[Configuration] for more information.\n\nThe default replication factor for the retry topics is now `-1` (use broker default).\nIf your broker is earlier that version 2.4, you will now need to explicitly set the property.\n\nYou can now configure multiple `@RetryableTopic` listeners on the same topic in the same application context.\nPreviously, this was not possible.\nSee xref:retrytopic/multi-retry.adoc[Multiple Listeners, Same Topic(s)] for more information.\n\nThere are breaking API changes in `RetryTopicConfigurationSupport`; specifically, if you override the bean definition methods for `destinationTopicResolver`, `kafkaConsumerBackoffManager` and/or `retryTopicConfigurer`;\nthese methods now require an `ObjectProvider<RetryTopicComponentFactory>` parameter.\n\n[[x30-lc-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Retryable Topics Changes", "heading_level": 3, "file_order": 0, "section_index": 60, "content_hash": "a142230f746be9cd71fbe80695e249e7d16cf74e686734dbb54bca04a50ccdac", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7371c93b393ad483f35ed2ba0afe0dd5bc02fb03041b3caff13f13809c60b314", "content": "Events related to consumer authentication and authorization failures are now published by the container.\nSee xref:kafka/events.adoc[Application Events] for more information.\n\nYou can now customize the thread names used by consumer threads.\nSee xref:kafka/receiving-messages/container-thread-naming.adoc[Container Thread Naming] for more information.\n\nThe container property `restartAfterAuthException` has been added.\nSee xref:kafka/container-props.adoc[Listener Container Properties] for more information.\n\n[[x30-template-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container Changes", "heading_level": 3, "file_order": 0, "section_index": 61, "content_hash": "7371c93b393ad483f35ed2ba0afe0dd5bc02fb03041b3caff13f13809c60b314", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:d675a308c70926e515fc350440d54905556803468551154d54bc964ab363572d", "content": "The futures returned by this class are now ``CompletableFuture``s instead of ``ListenableFuture``s.\nSee xref:kafka/sending-messages.adoc#kafka-template[Using `KafkaTemplate`].\n\n[[x30-rkt-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`KafkaTemplate` Changes", "heading_level": 3, "file_order": 0, "section_index": 62, "content_hash": "d675a308c70926e515fc350440d54905556803468551154d54bc964ab363572d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:fa1e37e254292dbf1728c63546d4c85b027c214eaf35dc08c87a47750fae327a", "content": "The futures returned by this class are now ``CompletableFuture``s instead of ``ListenableFuture``s.\nSee xref:kafka/sending-messages.adoc#replying-template[Using `ReplyingKafkaTemplate`] and xref:kafka/sending-messages.adoc#exchanging-messages[Request/Reply with ``Message<?>``s].\n\n[[x30-listener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`ReplyingKafkaTemplate` Changes", "heading_level": 3, "file_order": 0, "section_index": 63, "content_hash": "fa1e37e254292dbf1728c63546d4c85b027c214eaf35dc08c87a47750fae327a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:8a5319a49a37ada893c8d52c50795d0edfb37cfaf56d5e3fa2d2ad31f7d1d272", "content": "You can now use a custom correlation header which will be echoed in any reply message.\nSee the note at the end of xref:kafka/sending-messages.adoc#replying-template[Using `ReplyingKafkaTemplate`] for more information.\n\nYou can now manually commit parts of a batch before the entire batch is processed.\nSee xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[Committing Offsets] for more information.\n\n[[x30-headers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`@KafkaListener` Changes", "heading_level": 3, "file_order": 0, "section_index": 64, "content_hash": "8a5319a49a37ada893c8d52c50795d0edfb37cfaf56d5e3fa2d2ad31f7d1d272", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:058c73fe120736b31f54f3df08135a89b45a374ce71e3713a188a9a1fdfc3873", "content": "Four constants in `KafkaHeaders` that were deprecated in 2.9.x have now been removed.\n\n* Instead of `MESSAGE_KEY`, use `KEY`.\n\n* Instead of `PARTITION_ID`, use `PARTITION`\n\nSimilarly, `RECEIVED_MESSAGE_KEY` is replaced by `RECEIVED_KEY` and `RECEIVED_PARTITION_ID` is replaced by `RECEIVED_PARTITION`.\n\n[[x30-testing]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`KafkaHeaders` Changes", "heading_level": 3, "file_order": 0, "section_index": 65, "content_hash": "058c73fe120736b31f54f3df08135a89b45a374ce71e3713a188a9a1fdfc3873", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:d91bdf4bd9c4f9121ac1ccbbc03e2542a3b928e8c7d858af1b125aeddf21cacc", "content": "Version 3.0.7 introduced a `MockConsumerFactory` and `MockProducerFactory`.\nSee xref:testing.adoc#mock-cons-prod[Mock Consumer and Producer] for more information.\n\nStarting with version 3.0.10, the embedded Kafka broker, by default, sets the Spring Boot property `spring.kafka.bootstrap-servers` to the address(es) of the embedded broker(s).\n\n[[what-s-new-in-2-9-since-2-8]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Testing Changes", "heading_level": 3, "file_order": 0, "section_index": 66, "content_hash": "d91bdf4bd9c4f9121ac1ccbbc03e2542a3b928e8c7d858af1b125aeddf21cacc", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:6d6dbfd9e012595a3e461f890df1e0cc6598463eed72959423b569df39ac075c", "content": "[[x29-kafka-client]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "What's New in 2.9 since 2.8", "heading_level": 2, "file_order": 0, "section_index": 67, "content_hash": "6d6dbfd9e012595a3e461f890df1e0cc6598463eed72959423b569df39ac075c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:2917d68dc2ab2a4f57c3d92dc911d6fd20c4f144c633fe50d565a1b6ad762b24", "content": "This version requires the 3.2.0 `kafka-clients`.\n\n[[x29-eh-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 68, "content_hash": "2917d68dc2ab2a4f57c3d92dc911d6fd20c4f144c633fe50d565a1b6ad762b24", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7c39340aedd251c264e58af686b1efd9749c2c669f2a978dd16aab3e987dae8b", "content": "The `DefaultErrorHandler` can now be configured to pause the container for one poll and use the remaining results from the previous poll, instead of seeking to the offsets of the remaining records.\nSee xref:kafka/annotation-error-handling.adoc#default-eh[DefaultErrorHandler] for more information.\n\nThe `DefaultErrorHandler` now has a `BackOffHandler` property.\nSee xref:kafka/annotation-error-handling.adoc#backoff-handlers[Back Off Handlers] for more information.\n\n[[x29-lc-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Error Handler Changes", "heading_level": 3, "file_order": 0, "section_index": 69, "content_hash": "7c39340aedd251c264e58af686b1efd9749c2c669f2a978dd16aab3e987dae8b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7062babf791cd3a063428e863baa03c21051f7ff8b889c25aa7159c9ab5ca960", "content": "`interceptBeforeTx` now works with all transaction managers (previously it was only applied when a `KafkaAwareTransactionManager` was used).\nSee <<interceptBeforeTx>>.\n\nA new container property `pauseImmediate` is provided which allows the container to pause the consumer after the current record is processed, instead of after all the records from the previous poll have been processed.\nSee <<pauseImmediate>>.\n\nEvents related to consumer authentication and authorization\n\n[[x29-hm-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container Changes", "heading_level": 3, "file_order": 0, "section_index": 70, "content_hash": "7062babf791cd3a063428e863baa03c21051f7ff8b889c25aa7159c9ab5ca960", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:59567afd328f4293cd6adae9d0ccc5f391c1e6c1cade82910d918fc0d3d31c4e", "content": "You can now configure which inbound headers should be mapped.\nAlso available in version 2.8.8 or later.\nSee xref:kafka/headers.adoc[Message Headers] for more information.\n\n[[x29-template-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Header Mapper Changes", "heading_level": 3, "file_order": 0, "section_index": 71, "content_hash": "59567afd328f4293cd6adae9d0ccc5f391c1e6c1cade82910d918fc0d3d31c4e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:1e65f43b1629a780074d11dc817616137f1126d16a73dbab62ce33cc5de74d5c", "content": "In 3.0, the futures returned by this class will be ``CompletableFuture``s instead of ``ListenableFuture``s.\nSee xref:kafka/sending-messages.adoc#kafka-template[Using `KafkaTemplate`] for assistance in transitioning when using this release.\n\n[[x29-rkt-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`KafkaTemplate` Changes", "heading_level": 3, "file_order": 0, "section_index": 72, "content_hash": "1e65f43b1629a780074d11dc817616137f1126d16a73dbab62ce33cc5de74d5c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ab17d2738318c8ee99b5069025a77a5802b06562e01be5859a4dc5bbc297b196", "content": "The template now provides a method to wait for assignment on the reply container, to avoid a race when sending a request before the reply container is initialized.\nAlso available in version 2.8.8 or later.\nSee xref:kafka/sending-messages.adoc#replying-template[Using `ReplyingKafkaTemplate`].\n\nIn 3.0, the futures returned by this class will be ``CompletableFuture``s instead of ``ListenableFuture``s.\nSee xref:kafka/sending-messages.adoc#replying-template[Using `ReplyingKafkaTemplate`] and xref:kafka/sending-messages.adoc#exchanging-messages[Request/Reply with ``Message<?>``s] for assistance in transitioning when using this release.\n\n[[what-s-new-in-2-8-since-2-7]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`ReplyingKafkaTemplate` Changes", "heading_level": 3, "file_order": 0, "section_index": 73, "content_hash": "ab17d2738318c8ee99b5069025a77a5802b06562e01be5859a4dc5bbc297b196", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:6b0c027e664337ae8ca0e9d10064985417dde6aa727a6d5cd4849023bbf3b9cc", "content": "This section covers the changes made from version 2.7 to version 2.8.\nFor changes in earlier version, see xref:appendix.adoc#history[Change History].\n\n[[x28-kafka-client]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "What's New in 2.8 Since 2.7", "heading_level": 2, "file_order": 0, "section_index": 74, "content_hash": "6b0c027e664337ae8ca0e9d10064985417dde6aa727a6d5cd4849023bbf3b9cc", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:3c07a5f662232ed48e62b1ab66bcef0f6c3f78046965709e10b576375a687d5e", "content": "This version requires the 3.0.0 `kafka-clients`\n\n[[x28-packages]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 75, "content_hash": "3c07a5f662232ed48e62b1ab66bcef0f6c3f78046965709e10b576375a687d5e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:29beed8495497a262a530d875302868ca7c3d29fcdc13c7803b0b0abb32f65d7", "content": "Classes and interfaces related to type mapping have been moved from `...support.converter` to `...support.mapping`.\n\n* `AbstractJavaTypeMapper`\n* `ClassMapper`\n* `DefaultJackson2JavaTypeMapper`\n* `Jackson2JavaTypeMapper`\n\n[[x28-ooo-commits]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Package Changes", "heading_level": 3, "file_order": 0, "section_index": 76, "content_hash": "29beed8495497a262a530d875302868ca7c3d29fcdc13c7803b0b0abb32f65d7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:c0490224439e41334633a0afacd12d9883bdd6854dd1d2777c850b642d2b27e4", "content": "The listener container can now be configured to accept manual offset commits out of order (usually asynchronously).\nThe container will defer the commit until the missing offset is acknowledged.\nSee xref:kafka/receiving-messages/ooo-commits.adoc[Manually Committing Offsets] for more information.\n\n[[x28-batch-overrude]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Out of Order Manual Commits", "heading_level": 3, "file_order": 0, "section_index": 77, "content_hash": "c0490224439e41334633a0afacd12d9883bdd6854dd1d2777c850b642d2b27e4", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:761bad4652c47871add90eb20fa0e30ce7a52c6ab458acc1ec46cd07b417c0ae", "content": "It is now possible to specify whether the listener method is a batch listener on the method itself.\nThis allows the same container factory to be used for both record and batch listeners.\n\nSee <<batch-listeners>> for more information.\n\nBatch listeners can now handle conversion exceptions.\n\nSee xref:kafka/annotation-error-handling.adoc#batch-listener-conv-errors[Conversion Errors with Batch Error Handlers] for more information.\n\n`RecordFilterStrategy`, when used with batch listeners, can now filter the entire batch in one call.\nSee the note at the end of <<batch-listeners>> for more information.\n\nThe `@KafkaListener` annotation now has the `filter` attribute, to override the container factory's `RecordFilterStrategy` for just this listener.\n\nThe `@KafkaListener` annotation now has the `info` attribute; this is used to populate the new listener container property `listenerInfo`.\nThis is then used to populate a `KafkaHeaders.LISTENER_INFO` header in each record which can be used in `RecordInterceptor`, `RecordFilterStrategy`, or the listener itself.\nSee xref:kafka/annotation-error-handling.adoc#li-header[Listener Info Header] and xref:kafka/container-props.adoc#amlc-props[AbstractMessageListenerContainer Properties] for more information.\n\n[[x28-template]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`@KafkaListener` Changes", "heading_level": 3, "file_order": 0, "section_index": 78, "content_hash": "761bad4652c47871add90eb20fa0e30ce7a52c6ab458acc1ec46cd07b417c0ae", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:d5ed402697dee6af066b734f6024554b7e9dc6e7f9a15d917e6bd81813dbcf88", "content": "You can now receive a single record, given the topic, partition and offset.\nSee xref:kafka/receiving-messages/template-receive.adoc[Using `KafkaTemplate` to Receive] for more information.\n\n[[x28-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`KafkaTemplate` Changes", "heading_level": 3, "file_order": 0, "section_index": 79, "content_hash": "d5ed402697dee6af066b734f6024554b7e9dc6e7f9a15d917e6bd81813dbcf88", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:278bef70312e3310b0ada778aec6489ad2e699cddd477f983016dc4b042abb8b", "content": "The legacy `GenericErrorHandler` and its sub-interface hierarchies for record an batch listeners have been replaced by a new single interface `CommonErrorHandler` with implementations corresponding to most legacy implementations of `GenericErrorHandler`.\nSee xref:kafka/annotation-error-handling.adoc#error-handlers[Container Error Handlers] and xref:kafka/annotation-error-handling.adoc#migrating-legacy-eh[Migrating Custom Legacy Error Handler Implementations to `CommonErrorHandler`] for more information.\n\n[[x28-lcc]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`CommonErrorHandler` Added", "heading_level": 3, "file_order": 0, "section_index": 80, "content_hash": "278bef70312e3310b0ada778aec6489ad2e699cddd477f983016dc4b042abb8b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:33bf480e639adca39bcd272884dcd369dd027ccc9e9d9243539ba1f813a7ca6e", "content": "The `interceptBeforeTx` container property is now `true` by default.\n\nThe `authorizationExceptionRetryInterval` property has been renamed to `authExceptionRetryInterval` and now applies to ``AuthenticationException``s in addition to ``AuthorizationException``s previously.\nBoth exceptions are considered fatal and the container will stop by default, unless this property is set.\n\nSee xref:kafka/receiving-messages/message-listener-container.adoc#kafka-container[Using `KafkaMessageListenerContainer`] and xref:kafka/container-props.adoc[Listener Container Properties] for more information.\n\n[[x28-serializers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container Changes", "heading_level": 3, "file_order": 0, "section_index": 81, "content_hash": "33bf480e639adca39bcd272884dcd369dd027ccc9e9d9243539ba1f813a7ca6e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:1ca85db3efe537b1c79d1d4e2d77b50eb63644a0f278cab87ace643a70a582e7", "content": "The `DelegatingByTopicSerializer` and `DelegatingByTopicDeserializer` are now provided.\nSee xref:kafka/serdes.adoc#delegating-serialization[Delegating Serializer and Deserializer] for more information.\n\n[[x28-dlpr]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Serializer/Deserializer Changes", "heading_level": 3, "file_order": 0, "section_index": 82, "content_hash": "1ca85db3efe537b1c79d1d4e2d77b50eb63644a0f278cab87ace643a70a582e7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:9449bcb84b745d040de28bac3f612743449db6078b830089eec640383748bb5d", "content": "The property `stripPreviousExceptionHeaders` is now `true` by default.\n\nThere are now several techniques to customize which headers are added to the output record.\n\nSee xref:kafka/annotation-error-handling.adoc#dlpr-headers[Managing Dead Letter Record Headers] for more information.\n\n[[x28-retryable-topics-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`DeadLetterPublishingRecover` Changes", "heading_level": 3, "file_order": 0, "section_index": 83, "content_hash": "9449bcb84b745d040de28bac3f612743449db6078b830089eec640383748bb5d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:6aac0efde67b60ce44a490f8c8f0c29897d146c01136a18c74e4f1db3bf24982", "content": "Now you can use the same factory for retryable and non-retryable topics.\nSee xref:retrytopic/retry-topic-lcf.adoc[Specifying a ListenerContainerFactory] for more information.\n\nThere's now a manageable global list of fatal exceptions that will make the failed record go straight to the DLT.\nRefer to xref:retrytopic/features.adoc#retry-topic-ex-classifier[Exception Classifier] to see how to manage it.\n\nYou can now use blocking and non-blocking retries in conjunction.\nSee xref:retrytopic/retry-topic-combine-blocking.adoc[Combining Blocking and Non-Blocking Retries] for more information.\n\nThe KafkaBackOffException thrown when using the retryable topics feature is now logged at DEBUG level.\nSee xref:retrytopic/change-kboe-logging-level.adoc[Changing KafkaBackOffException Logging Level] if you need to change the logging level back to WARN or set it to any other level.\n\n[[changes-between-2-6-and-2-7]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Retryable Topics Changes", "heading_level": 3, "file_order": 0, "section_index": 84, "content_hash": "6aac0efde67b60ce44a490f8c8f0c29897d146c01136a18c74e4f1db3bf24982", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7435ef2c1afe8e1de515c64489fedecdf3cf2007341c262a143313c3c00f7f88", "content": "[[x27-kafka-client]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes between 2.6 and 2.7", "heading_level": 2, "file_order": 0, "section_index": 85, "content_hash": "7435ef2c1afe8e1de515c64489fedecdf3cf2007341c262a143313c3c00f7f88", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:49bf21e9cc61a6c43858b9a151104bf94b86d0a249e0b33105d1b44caa0bfc8c", "content": "This version requires the 2.7.0 `kafka-clients`.\nIt is also compatible with the 2.8.0 clients, since version 2.7.1; see xref:appendix.adoc[Override Spring Boot Dependencies].\n\n[[x-27-nonblock-retry]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 86, "content_hash": "49bf21e9cc61a6c43858b9a151104bf94b86d0a249e0b33105d1b44caa0bfc8c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:56225b50ff445e05d477f74e78b169801e44c0dc5ccea9da896fdb7719d83947", "content": "This significant new feature is added in this release.\nWhen strict ordering is not important, failed deliveries can be sent to another topic to be consumed later.\nA series of such retry topics can be configured, with increasing delays.\nSee xref:retrytopic.adoc[Non-Blocking Retries] for more information.\n\n[[x27-container]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Non-Blocking Delayed Retries Using Topics", "heading_level": 3, "file_order": 0, "section_index": 87, "content_hash": "56225b50ff445e05d477f74e78b169801e44c0dc5ccea9da896fdb7719d83947", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ed006634628dd0c1f420b4a6689fca3ab95bef00fa2c2f4b58d1d7e828f57354", "content": "The `onlyLogRecordMetadata` container property is now `true` by default.\n\nA new container property `stopImmediate` is now available.\n\nSee xref:kafka/container-props.adoc[Listener Container Properties] for more information.\n\nError handlers that use a `BackOff` between delivery attempts (e.g. `SeekToCurrentErrorHandler` and `DefaultAfterRollbackProcessor`) will now exit the back off interval soon after the container is stopped, rather than delaying the stop.\n\nError handlers and after rollback processors that extend `FailedRecordProcessor` can now be configured with one or more ``RetryListener``s to receive information about retry and recovery progress.\n\nThe `RecordInterceptor` now has additional methods called after the listener returns (normally, or by throwing an exception).\nIt also has a sub-interface `ConsumerAwareRecordInterceptor`.\nIn addition, there is now a `BatchInterceptor` for batch listeners.\nSee xref:kafka/receiving-messages/message-listener-container.adoc[Message Listener Containers] for more information.\n\n[[x27-listener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container Changes", "heading_level": 3, "file_order": 0, "section_index": 88, "content_hash": "ed006634628dd0c1f420b4a6689fca3ab95bef00fa2c2f4b58d1d7e828f57354", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:97db4d916a283c5dc697f28881e076b6606196e5edb87e39925517f8fff05419", "content": "You can now validate the payload parameter of `@KafkaHandler` methods (class-level listeners).\nSee xref:kafka/receiving-messages/validation.adoc[`@KafkaListener` `@Payload` Validation] for more information.\n\nYou can now set the `rawRecordHeader` property on the `MessagingMessageConverter` and `BatchMessagingMessageConverter` which causes the raw `ConsumerRecord` to be added to the converted `Message<?>`.\nThis is useful, for example, if you wish to use a `DeadLetterPublishingRecoverer` in a listener error handler.\nSee xref:kafka/annotation-error-handling.adoc#listener-error-handlers[Listener Error Handlers] for more information.\n\nYou can now modify `@KafkaListener` annotations during application initialization.\nSee xref:kafka/receiving-messages/kafkalistener-attrs.adoc[`@KafkaListener` Attribute Modification] for more information.\n\n[[x27-dlt]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`@KafkaListener` Changes", "heading_level": 3, "file_order": 0, "section_index": 89, "content_hash": "97db4d916a283c5dc697f28881e076b6606196e5edb87e39925517f8fff05419", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:4fd247dcfe6a4b0fc010c9e7b1eaff5b69bae28b2b67da34272f27f4fd2bc108", "content": "Now, if both the key and value fail deserialization, the original values are published to the DLT.\nPreviously, the value was populated but the key `DeserializationException` remained in the headers.\nThere is a breaking API change, if you subclassed the recoverer and overrode the `createProducerRecord` method.\n\nIn addition, the recoverer verifies that the partition selected by the destination resolver actually exists before publishing to it.\n\nSee xref:kafka/annotation-error-handling.adoc#dead-letters[Publishing Dead-letter Records] for more information.\n\n[[x27-CKTM]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`DeadLetterPublishingRecover` Changes", "heading_level": 3, "file_order": 0, "section_index": 90, "content_hash": "4fd247dcfe6a4b0fc010c9e7b1eaff5b69bae28b2b67da34272f27f4fd2bc108", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7b6f69a79d7a2dc66bbb72671fc50200d2114ac459857317d09afc367b7a85fd", "content": "See xref:kafka/transactions.adoc[Transactions] for more information.\n\n[[x27-RKT]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`ChainedKafkaTransactionManager` is Deprecated", "heading_level": 3, "file_order": 0, "section_index": 91, "content_hash": "7b6f69a79d7a2dc66bbb72671fc50200d2114ac459857317d09afc367b7a85fd", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:8fa78b10b7f8d29206fbbd91b74e18d54f7a5d48ab4eb7aef66b1734a91d3a5d", "content": "There is now a mechanism to examine a reply and fail the future exceptionally if some condition exists.\n\nSupport for sending and receiving `spring-messaging` ``Message<?>``s has been added.\n\nSee xref:kafka/sending-messages.adoc#replying-template[Using `ReplyingKafkaTemplate`] for more information.\n\n[[x27-streams]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`ReplyingKafkaTemplate` Changes", "heading_level": 3, "file_order": 0, "section_index": 92, "content_hash": "8fa78b10b7f8d29206fbbd91b74e18d54f7a5d48ab4eb7aef66b1734a91d3a5d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:e637ab6f110d593b4384a8884270a29e908ac7bf5c1728ad3994bcb2e7c50332", "content": "By default, the `StreamsBuilderFactoryBean` is now configured to not clean up local state.\nSee xref:streams.adoc#streams-config[Configuration] for more information.\n\n[[x27-admin]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Streams Changes", "heading_level": 3, "file_order": 0, "section_index": 93, "content_hash": "e637ab6f110d593b4384a8884270a29e908ac7bf5c1728ad3994bcb2e7c50332", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:b92d395d46cf3c36de508cac17606c2b4bce2671594c11cf46821098988fb5be", "content": "New methods `createOrModifyTopics` and `describeTopics` have been added.\n`KafkaAdmin.NewTopics` has been added to facilitate configuring multiple topics in a single bean.\nSee <<configuring-topics>> for more information.\n\n[[x27-conv]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`KafkaAdmin` Changes", "heading_level": 3, "file_order": 0, "section_index": 94, "content_hash": "b92d395d46cf3c36de508cac17606c2b4bce2671594c11cf46821098988fb5be", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7ef4f1ed9e0120c3870ed379de8b9eaa9f0cc5ba7a8ba258f2a8f5c194ba5a53", "content": "It is now possible to add a `spring-messaging` `SmartMessageConverter` to the `MessagingMessageConverter`, allowing content negotiation based on the `contentType` header.\nSee xref:kafka/serdes.adoc#messaging-message-conversion[Spring Messaging Message Conversion] for more information.\n\n[[x27-sequencing]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`MessageConverter` Changes", "heading_level": 3, "file_order": 0, "section_index": 95, "content_hash": "7ef4f1ed9e0120c3870ed379de8b9eaa9f0cc5ba7a8ba258f2a8f5c194ba5a53", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:cefa188bccb495081ffdd11b7bbe6e4095fb01745f434c5344dc3aee77dfb255", "content": "See xref:kafka/receiving-messages/sequencing.adoc[Starting ``@KafkaListener``s in Sequence] for more information.\n\n[[x27-exp-backoff]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Sequencing ``@KafkaListener``s", "heading_level": 3, "file_order": 0, "section_index": 96, "content_hash": "cefa188bccb495081ffdd11b7bbe6e4095fb01745f434c5344dc3aee77dfb255", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:50c1fb4d658891ce389f4538f15ddbf57135b3815049a8c368d4d9aaef50ba68", "content": "A new `BackOff` implementation is provided, making it more convenient to configure the max retries.\nSee xref:kafka/annotation-error-handling.adoc#exp-backoff[`ExponentialBackOffWithMaxRetries` Implementation] for more information.\n\n[[x27-delegating-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`ExponentialBackOffWithMaxRetries`", "heading_level": 3, "file_order": 0, "section_index": 97, "content_hash": "50c1fb4d658891ce389f4538f15ddbf57135b3815049a8c368d4d9aaef50ba68", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7c32652770c15484011e361ff65208d1b086b41c521f8e03e6d00485cbbe5653", "content": "These new error handlers can be configured to delegate to different error handlers, depending on the exception type.\nSee xref:kafka/annotation-error-handling.adoc#cond-eh[Delegating Error Handler] for more information.\n\n[[changes-between-2-5-and-2-6]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Conditional Delegating Error Handlers", "heading_level": 3, "file_order": 0, "section_index": 98, "content_hash": "7c32652770c15484011e361ff65208d1b086b41c521f8e03e6d00485cbbe5653", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:3bee1de6409922938979ce715a510662f09158c634e8384a015d8ee4afeaa703", "content": "[[x26-kafka-client]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes between 2.5 and 2.6", "heading_level": 2, "file_order": 0, "section_index": 99, "content_hash": "3bee1de6409922938979ce715a510662f09158c634e8384a015d8ee4afeaa703", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:a0f1cc2a36612a0adf0f6cdfdacb360301bb69dfb59ab0cfcd473e32bb99e1ab", "content": "This version requires the 2.6.0 `kafka-clients`.\n\n[[listener-container-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 100, "content_hash": "a0f1cc2a36612a0adf0f6cdfdacb360301bb69dfb59ab0cfcd473e32bb99e1ab", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:92557b7d3c2a03eaa62e56475c6c5837e578d2668e9a8116cb2fb8128a4d5e6f", "content": "The default `EOSMode` is now `BETA`.\nSee xref:kafka/exactly-once.adoc[Exactly Once Semantics] for more information.\n\nVarious error handlers (that extend `FailedRecordProcessor`) and the `DefaultAfterRollbackProcessor` now reset the `BackOff` if recovery fails.\nIn addition, you can now select the `BackOff` to use based on the failed record and/or exception.\n\nYou can now configure an `adviceChain` in the container properties.\nSee xref:kafka/container-props.adoc[Listener Container Properties] for more information.\n\nWhen the container is configured to publish ``ListenerContainerIdleEvent``s, it now publishes a `ListenerContainerNoLongerIdleEvent` when a record is received after publishing an idle event.\nSee xref:kafka/events.adoc[Application Events] and xref:kafka/events.adoc#idle-containers[Detecting Idle and Non-Responsive Consumers] for more information.\n\n[[kafkalistener-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container Changes", "heading_level": 3, "file_order": 0, "section_index": 101, "content_hash": "92557b7d3c2a03eaa62e56475c6c5837e578d2668e9a8116cb2fb8128a4d5e6f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:a8956b0187dc158df92b196bcf2fc59be8cc479f16a4ec223c3c9a9142f54c7f", "content": "When using manual partition assignment, you can now specify a wildcard for determining which partitions should be reset to the initial offset.\nIn addition, if the listener implements `ConsumerSeekAware`, `onPartitionsAssigned()` is called after the manual assignment.\n(Also added in version 2.5.5).\nSee xref:kafka/receiving-messages/listener-annotation.adoc#manual-assignment[Explicit Partition Assignment] for more information.\n\nConvenience methods have been added to `AbstractConsumerSeekAware` to make seeking easier.\nSee <<seek>> for more information.\n\n[[errorhandler-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "@KafkaListener Changes", "heading_level": 3, "file_order": 0, "section_index": 102, "content_hash": "a8956b0187dc158df92b196bcf2fc59be8cc479f16a4ec223c3c9a9142f54c7f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:aa5799635dbcd42a8462de7115c353277c8460a41a0b0970d3fe3f6613cd7515", "content": "Subclasses of `FailedRecordProcessor` (e.g. `SeekToCurrentErrorHandler`, `DefaultAfterRollbackProcessor`, `RecoveringBatchErrorHandler`) can now be configured to reset the retry state if the exception is a different type to that which occurred previously with this record.\n\n[[producer-factory-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ErrorHandler Changes", "heading_level": 3, "file_order": 0, "section_index": 103, "content_hash": "aa5799635dbcd42a8462de7115c353277c8460a41a0b0970d3fe3f6613cd7515", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:5c29c240a05d2136d2dcd01cd711b58302d7c46b72f816d0157619f962e517f1", "content": "You can now set a maximum age for producers after which they will be closed and recreated.\nSee xref:kafka/transactions.adoc[Transactions] for more information.\n\nYou can now update the configuration map after the `DefaultKafkaProducerFactory` has been created.\nThis might be useful, for example, if you have to update SSL key/trust store locations after a credentials change.\nSee xref:kafka/sending-messages.adoc#producer-factory[Using `DefaultKafkaProducerFactory`] for more information.\n\n[[changes-between-2-4-and-2-5]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Producer Factory Changes", "heading_level": 3, "file_order": 0, "section_index": 104, "content_hash": "5c29c240a05d2136d2dcd01cd711b58302d7c46b72f816d0157619f962e517f1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:868f13a13040761b6098bca6e0cc97e2c1158c0d1cedeba7e547d97ad88b7a28", "content": "This section covers the changes made from version 2.4 to version 2.5.\nFor changes in earlier version, see xref:appendix.adoc#history[Change History].\n\n[[x25-factory-listeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes between 2.4 and 2.5", "heading_level": 2, "file_order": 0, "section_index": 105, "content_hash": "868f13a13040761b6098bca6e0cc97e2c1158c0d1cedeba7e547d97ad88b7a28", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:b54382c92028886a1a662ae822538f7ceef4a7413678877105b27c7055b33dfe", "content": "The default consumer and producer factories can now invoke a callback whenever a consumer or producer is created or closed.\nImplementations for native Micrometer metrics are provided.\nSee xref:kafka/connecting.adoc#factory-listeners[Factory Listeners] for more information.\n\nYou can now change bootstrap server properties at runtime, enabling failover to another Kafka cluster.\nSee xref:kafka/connecting.adoc[Connecting to Kafka] for more information.\n\n[[x25-streams-listeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Consumer/Producer Factory Changes", "heading_level": 3, "file_order": 0, "section_index": 106, "content_hash": "b54382c92028886a1a662ae822538f7ceef4a7413678877105b27c7055b33dfe", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:f9a7c301ec327d5d15d5586610362425f93b29d44d9aba90ebeb565e4bc538a1", "content": "The factory bean can now invoke a callback whenever a `KafkaStreams` created or destroyed.\nAn Implementation for native Micrometer metrics is provided.\nSee xref:streams.adoc#streams-micrometer[KafkaStreams Micrometer Support] for more information.\n\n[[x25-kafka-client]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`StreamsBuilderFactoryBean` Changes", "heading_level": 3, "file_order": 0, "section_index": 107, "content_hash": "f9a7c301ec327d5d15d5586610362425f93b29d44d9aba90ebeb565e4bc538a1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:f7d2fb0d2502c918db9b5c2ced2521315cdc38041b81239d4b1e7d44d12d36f8", "content": "This version requires the 2.5.0 `kafka-clients`.\n\n[[class-package-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 108, "content_hash": "f7d2fb0d2502c918db9b5c2ced2521315cdc38041b81239d4b1e7d44d12d36f8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:bec72b6b015c80592a66e183fd2db4227ad4d8a9846e0ae9cd17e02b6de1f438", "content": "`SeekUtils` has been moved from the `o.s.k.support` package to `o.s.k.listener`.\n\n[[x25-delivery]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Class/Package Changes", "heading_level": 3, "file_order": 0, "section_index": 109, "content_hash": "bec72b6b015c80592a66e183fd2db4227ad4d8a9846e0ae9cd17e02b6de1f438", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:d7052d41d71541a60c38bf310334ea8584c73ba25b819a441dd66c8611e4be76", "content": "There is now an option to to add a header which tracks delivery attempts when using certain error handlers and after rollback processors.\nSee xref:kafka/annotation-error-handling.adoc#delivery-header[Delivery Attempts Header] for more information.\n\n[[x25-message-return]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Delivery Attempts Header", "heading_level": 3, "file_order": 0, "section_index": 110, "content_hash": "d7052d41d71541a60c38bf310334ea8584c73ba25b819a441dd66c8611e4be76", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:fef62e919dfdd7e6b798428a580e82917b7e7e932fc40e7a89688c523b9e993d", "content": "Default reply headers will now be populated automatically if needed when a `@KafkaListener` return type is `Message<?>`.\nSee xref:kafka/sending-messages.adoc#reply-message[Reply Type Message<?>] for more information.\n\nThe `KafkaHeaders.RECEIVED_MESSAGE_KEY` is no longer populated with a `null` value when the incoming record has a `null` key; the header is omitted altogether.\n\n`@KafkaListener` methods can now specify a `ConsumerRecordMetadata` parameter instead of using discrete headers for metadata such as topic, partition, etc.\nSee xref:kafka/receiving-messages/listener-annotation.adoc#consumer-record-metadata[Consumer Record Metadata] for more information.\n\n[[x25-container]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "@KafkaListener Changes", "heading_level": 3, "file_order": 0, "section_index": 111, "content_hash": "fef62e919dfdd7e6b798428a580e82917b7e7e932fc40e7a89688c523b9e993d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:007223550321aa05fa96165c768a6798ad293e1add15f7bb4b5c13fdc5dc4a9d", "content": "The `assignmentCommitOption` container property is now `LATEST_ONLY_NO_TX` by default.\nSee xref:kafka/container-props.adoc[Listener Container Properties] for more information.\n\nThe `subBatchPerPartition` container property is now `true` by default when using transactions.\nSee xref:kafka/transactions.adoc[Transactions] for more information.\n\nA new `RecoveringBatchErrorHandler` is now provided.\n\nStatic group membership is now supported.\nSee xref:kafka/receiving-messages/message-listener-container.adoc[Message Listener Containers] for more information.\n\nWhen incremental/cooperative rebalancing is configured, if offsets fail to commit with a non-fatal `RebalanceInProgressException`, the container will attempt to re-commit the offsets for the partitions that remain assigned to this instance after the rebalance is completed.\n\nThe default error handler is now the `SeekToCurrentErrorHandler` for record listeners and `RecoveringBatchErrorHandler` for batch listeners.\nSee xref:kafka/annotation-error-handling.adoc#error-handlers[Container Error Handlers] for more information.\n\nYou can now control the level at which exceptions intentionally thrown by standard error handlers are logged.\nSee xref:kafka/annotation-error-handling.adoc#error-handlers[Container Error Handlers] for more information.\n\nThe `getAssignmentsByClientId()` method has been added, making it easier to determine which consumers in a concurrent container are assigned which partition(s).\nSee xref:kafka/container-props.adoc[Listener Container Properties] for more information.\n\nYou can now suppress logging entire ``ConsumerRecord``s in error, debug logs etc.\nSee `onlyLogRecordMetadata` in xref:kafka/container-props.adoc[Listener Container Properties].\n\n[[x25-template]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container Changes", "heading_level": 3, "file_order": 0, "section_index": 112, "content_hash": "007223550321aa05fa96165c768a6798ad293e1add15f7bb4b5c13fdc5dc4a9d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:05e4cd731e68778cb7f59384223b452180a109e5ed0ea41b7c63326108795e11", "content": "The `KafkaTemplate` can now maintain micrometer timers.\nSee xref:kafka/micrometer.adoc[Monitoring] for more information.\n\nThe `KafkaTemplate` can now be configured with `ProducerConfig` properties to override those in the producer factory.\nSee xref:kafka/sending-messages.adoc#kafka-template[Using `KafkaTemplate`] for more information.\n\nA `RoutingKafkaTemplate` has now been provided.\nSee xref:kafka/sending-messages.adoc#routing-template[Using `RoutingKafkaTemplate`] for more information.\n\nYou can now use `KafkaSendCallback` instead of `ListenerFutureCallback` to get a narrower exception, making it easier to extract the failed `ProducerRecord`.\nSee xref:kafka/sending-messages.adoc#kafka-template[Using `KafkaTemplate`] for more information.\n\n[[x25-string-serializer]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "KafkaTemplate Changes", "heading_level": 3, "file_order": 0, "section_index": 113, "content_hash": "05e4cd731e68778cb7f59384223b452180a109e5ed0ea41b7c63326108795e11", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:00f4df0524ba510a89d3fa9803571734f52313a4aa777355a4e0466255b97778", "content": "New `ToStringSerializer`/``StringDeserializer``s as well as an associated `SerDe` are now provided.\nSee xref:kafka/serdes.adoc#string-serde[String serialization] for more information.\n\n[[x25-json-deser]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka String Serializer/Deserializer", "heading_level": 3, "file_order": 0, "section_index": 114, "content_hash": "00f4df0524ba510a89d3fa9803571734f52313a4aa777355a4e0466255b97778", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:6537e50e2b6078b89d36bcdffed035e81fddb70bac4d2361b58b7b7c74074ba0", "content": "The `JsonDeserializer` now has more flexibility to determine the deserialization type.\nSee xref:kafka/serdes.adoc#serdes-type-methods[Using Methods to Determine Types] for more information.\n\n[[x25-delegate-serde]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "JsonDeserializer", "heading_level": 3, "file_order": 0, "section_index": 115, "content_hash": "6537e50e2b6078b89d36bcdffed035e81fddb70bac4d2361b58b7b7c74074ba0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:0ef91b52de535ab37b47502318b4f63c489657bc0ca7574c29d863bd36393007", "content": "The `DelegatingSerializer` can now handle \"standard\" types, when the outbound record has no header.\nSee xref:kafka/serdes.adoc#delegating-serialization[Delegating Serializer and Deserializer] for more information.\n\n[[x25-testing]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Delegating Serializer/Deserializer", "heading_level": 3, "file_order": 0, "section_index": 116, "content_hash": "0ef91b52de535ab37b47502318b4f63c489657bc0ca7574c29d863bd36393007", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:290742a20c2b7f8ab3e0bec60d207bfdb0e4a0231193b22b884fc3fb8cfe8f7f", "content": "The `KafkaTestUtils.consumerProps()` helper record now sets `ConsumerConfig.AUTO_OFFSET_RESET_CONFIG` to `earliest` by default.\nSee xref:testing.adoc#junit[JUnit] for more information.\n\n[[changes-between-2-3-and-2-4]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Testing Changes", "heading_level": 3, "file_order": 0, "section_index": 117, "content_hash": "290742a20c2b7f8ab3e0bec60d207bfdb0e4a0231193b22b884fc3fb8cfe8f7f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:fb35bb6bd9d3f3cb8008e20fb05b6f88f9c24c73c6e9f055e0164e44a282fff1", "content": "[[kafka-client-2.4]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes between 2.3 and 2.4", "heading_level": 2, "file_order": 0, "section_index": 118, "content_hash": "fb35bb6bd9d3f3cb8008e20fb05b6f88f9c24c73c6e9f055e0164e44a282fff1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:76d69526f0b04d29e96c877de6c4f7368c69287191a9627be06534bb003e3630", "content": "This version requires the 2.4.0 `kafka-clients` or higher and supports the new incremental rebalancing feature.\n\n[[x24-carl]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 119, "content_hash": "76d69526f0b04d29e96c877de6c4f7368c69287191a9627be06534bb003e3630", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:68aaece3f47e660c61eeb893d6d9589ec4d7c59e0d0121b789bece4a713977aa", "content": "Like `ConsumerRebalanceListener`, this interface now has an additional method `onPartitionsLost`.\nRefer to the Apache Kafka documentation for more information.\n\nUnlike the `ConsumerRebalanceListener`, The default implementation does **not** call `onPartitionsRevoked`.\nInstead, the listener container will call that method after it has called `onPartitionsLost`; you should not, therefore, do the same when implementing `ConsumerAwareRebalanceListener`.\n\nSee the IMPORTANT note at the end of xref:kafka/receiving-messages/rebalance-listeners.adoc[Rebalancing Listeners] for more information.\n\n[[x24-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ConsumerAwareRebalanceListener", "heading_level": 3, "file_order": 0, "section_index": 120, "content_hash": "68aaece3f47e660c61eeb893d6d9589ec4d7c59e0d0121b789bece4a713977aa", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:2810b6903c217dd4f38b15cbb968c195970b7d08ed9e36cf6765bff477a61190", "content": "The `isAckAfterHandle()` default implementation now returns true by default.\n\n[[x24-template]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "GenericErrorHandler", "heading_level": 3, "file_order": 0, "section_index": 121, "content_hash": "2810b6903c217dd4f38b15cbb968c195970b7d08ed9e36cf6765bff477a61190", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:35f8e79b339768039bb6390836289e2397f5c0e9e0e84f72cc29d57aa0907c77", "content": "The `KafkaTemplate` now supports non-transactional publishing alongside transactional.\nSee xref:kafka/transactions.adoc#tx-template-mixed[`KafkaTemplate` Transactional and non-Transactional Publishing] for more information.\n\n[[x24-agg]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "KafkaTemplate", "heading_level": 3, "file_order": 0, "section_index": 122, "content_hash": "35f8e79b339768039bb6390836289e2397f5c0e9e0e84f72cc29d57aa0907c77", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:bc2278d5a5864746b68895fce62222cf5962d378a6e44032c3301116669a3fbc", "content": "The `releaseStrategy` is now a `BiConsumer`.\nIt is now called after a timeout (as well as when records arrive); the second parameter is `true` in the case of a call after a timeout.\n\nSee xref:kafka/sending-messages.adoc#aggregating-request-reply[Aggregating Multiple Replies] for more information.\n\n[[listener-container]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "AggregatingReplyingKafkaTemplate", "heading_level": 3, "file_order": 0, "section_index": 123, "content_hash": "bc2278d5a5864746b68895fce62222cf5962d378a6e44032c3301116669a3fbc", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:88d07bea717b826ace40334027efb2e29e9001b15aff32c462f0e69b5ac44754", "content": "The `ContainerProperties` provides an `authorizationExceptionRetryInterval` option to let the listener container to retry after any `AuthorizationException` is thrown by the `KafkaConsumer`.\nSee its JavaDocs and xref:kafka/receiving-messages/message-listener-container.adoc#kafka-container[Using `KafkaMessageListenerContainer`] for more information.\n\n[[kafkalistener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container", "heading_level": 3, "file_order": 0, "section_index": 124, "content_hash": "88d07bea717b826ace40334027efb2e29e9001b15aff32c462f0e69b5ac44754", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:1ddf49c16a2ed0b27c45e8274b523c9ac0397da543f3caa861e30acb8170f858", "content": "The `@KafkaListener` annotation has a new property `splitIterables`; default true.\nWhen a replying listener returns an `Iterable` this property controls whether the return result is sent as a single record or a record for each element is sent.\nSee xref:kafka/receiving-messages/annotation-send-to.adoc[Forwarding Listener Results using `@SendTo`] for more information\n\nBatch listeners can now be configured with a `BatchToRecordAdapter`; this allows, for example, the batch to be processed in a transaction while the listener gets one record at a time.\nWith the default implementation, a `ConsumerRecordRecoverer` can be used to handle errors within the batch, without stopping the processing of the entire batch - this might be useful when using transactions.\nSee xref:kafka/transactions.adoc#transactions-batch[Transactions with Batch Listeners] for more information.\n\n[[kafka-streams]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "@KafkaListener", "heading_level": 3, "file_order": 0, "section_index": 125, "content_hash": "1ddf49c16a2ed0b27c45e8274b523c9ac0397da543f3caa861e30acb8170f858", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:c09a727abc831b09c117aeb099e6d3073647eabf161a9800ec8307fa1359f845", "content": "The `StreamsBuilderFactoryBean` accepts a new property `KafkaStreamsInfrastructureCustomizer`.\nThis allows configuration of the builder and/or topology before the stream is created.\nSee xref:streams.adoc#streams-spring[Spring Management] for more information.\n\n[[changes-between-2-2-and-2-3]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Streams", "heading_level": 3, "file_order": 0, "section_index": 126, "content_hash": "c09a727abc831b09c117aeb099e6d3073647eabf161a9800ec8307fa1359f845", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:104acb1e186b53783259a0284095e2106d33bd797f3a46269e84a71e1da30027", "content": "This section covers the changes made from version 2.2 to version 2.3.\n\n[[cb-2-2-and-2-3-tips-tricks-and-examples]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes Between 2.2 and 2.3", "heading_level": 2, "file_order": 0, "section_index": 127, "content_hash": "104acb1e186b53783259a0284095e2106d33bd797f3a46269e84a71e1da30027", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:f6a07aaa3999b693d4b73863e5dfe3443ee26ad989d9b2985dc107f31098ffb5", "content": "A new chapter xref:index.adoc#tips-n-tricks[Tips, Tricks and Examples] has been added.\nPlease submit GitHub issues and/or pull requests for additional entries in that chapter.\n\n[[cb-2-2-and-2-3-kafka-client-2.2]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Tips, Tricks and Examples", "heading_level": 3, "file_order": 0, "section_index": 128, "content_hash": "f6a07aaa3999b693d4b73863e5dfe3443ee26ad989d9b2985dc107f31098ffb5", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:32ec7089bb2785a3ca6510de111adbbaaee962237bba3277ddfc032a619bea67", "content": "This version requires the 2.3.0 `kafka-clients` or higher.\n\n[[cb-2-2-and-2-3-class-package-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 129, "content_hash": "32ec7089bb2785a3ca6510de111adbbaaee962237bba3277ddfc032a619bea67", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:d290f3e3b253f3f5ef68cc4bd4349a2f374e7757611542ded11309a909f58a8b", "content": "`TopicPartitionInitialOffset` is deprecated in favor of `TopicPartitionOffset`.\n\n[[cb-2-2-and-2-3-configuration-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Class/Package Changes", "heading_level": 3, "file_order": 0, "section_index": 130, "content_hash": "d290f3e3b253f3f5ef68cc4bd4349a2f374e7757611542ded11309a909f58a8b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:18498432ee4ec960847ba28d8f431b00bcb8b8c920aefcc7eb3dbd6f2a4750dd", "content": "Starting with version 2.3.4, the `missingTopicsFatal` container property is false by default.\nWhen this is true, the application fails to start if the broker is down; many users were affected by this change; given that Kafka is a high-availability platform, we did not anticipate that starting an application with no active brokers would be a common use case.\n\n[[cb-2-2-and-2-3-producer-and-consumer-factory-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Configuration Changes", "heading_level": 3, "file_order": 0, "section_index": 131, "content_hash": "18498432ee4ec960847ba28d8f431b00bcb8b8c920aefcc7eb3dbd6f2a4750dd", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:83680269c2468fab0d552360fb6b2c0241b8ceba7835519cea1157a060f4f203", "content": "The `DefaultKafkaProducerFactory` can now be configured to create a producer per thread.\nYou can also provide `Supplier<Serializer>` instances in the constructor as an alternative to either configured classes (which require no-arg constructors), or constructing with `Serializer` instances, which are then shared between all Producers.\nSee xref:kafka/sending-messages.adoc#producer-factory[Using `DefaultKafkaProducerFactory`] for more information.\n\nThe same option is available with `Supplier<Deserializer>` instances in `DefaultKafkaConsumerFactory`.\nSee xref:kafka/receiving-messages/message-listener-container.adoc#kafka-container[Using `KafkaMessageListenerContainer`] for more information.\n\n[[cb-2-2-and-2-3-listener-container-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Producer and Consumer Factory Changes", "heading_level": 3, "file_order": 0, "section_index": 132, "content_hash": "83680269c2468fab0d552360fb6b2c0241b8ceba7835519cea1157a060f4f203", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:4020472a178418e1a58f844eabe4a0a6c699610944ab7fe8ec8047c3784a14b5", "content": "Previously, error handlers received `ListenerExecutionFailedException` (with the actual listener exception as the `cause`) when the listener was invoked using a listener adapter (such as ``@KafkaListener``s).\nExceptions thrown by native ``GenericMessageListener``s were passed to the error handler unchanged.\nNow a `ListenerExecutionFailedException` is always the argument (with the actual listener exception as the `cause`), which provides access to the container's `group.id` property.\n\nBecause the listener container has it's own mechanism for committing offsets, it prefers the Kafka `ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG` to be `false`.\nIt now sets it to false automatically unless specifically set in the consumer factory or the container's consumer property overrides.\n\nThe `ackOnError` property is now `false` by default.\n\nIt is now possible to obtain the consumer's `group.id` property in the listener method.\nSee xref:kafka/receiving-messages/listener-group-id.adoc[Obtaining the Consumer `group.id`] for more information.\n\nThe container has a new property `recordInterceptor` allowing records to be inspected or modified before invoking the listener.\nA `CompositeRecordInterceptor` is also provided in case you need to invoke multiple interceptors.\nSee xref:kafka/receiving-messages/message-listener-container.adoc[Message Listener Containers] for more information.\n\nThe `ConsumerSeekAware` has new methods allowing you to perform seeks relative to the beginning, end, or current position and to seek to the first offset greater than or equal to a time stamp.\nSee <<seek>> for more information.\n\nA convenience class `AbstractConsumerSeekAware` is now provided to simplify seeking.\nSee <<seek>> for more information.\n\nThe `ContainerProperties` provides an `idleBetweenPolls` option to let the main loop in the listener container to sleep between `KafkaConsumer.poll()` calls.\nSee its JavaDocs and xref:kafka/receiving-messages/message-listener-container.adoc#kafka-container[Using `KafkaMessageListenerContainer`] for more information.\n\nWhen using `AckMode.MANUAL` (or `MANUAL_IMMEDIATE`) you can now cause a redelivery by calling `nack` on the `Acknowledgment`.\nSee xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[Committing Offsets] for more information.\n\nListener performance can now be monitored using Micrometer ``Timer``s.\nSee xref:kafka/micrometer.adoc[Monitoring] for more information.\n\nThe containers now publish additional consumer lifecycle events relating to startup.\nSee xref:kafka/events.adoc[Application Events] for more information.\n\nTransactional batch listeners can now support zombie fencing.\nSee xref:kafka/transactions.adoc[Transactions] for more information.\n\nThe listener container factory can now be configured with a `ContainerCustomizer` to further configure each container after it has been created and configured.\nSee xref:kafka/container-factory.adoc[Container factory] for more information.\n\n[[cb-2-2-and-2-3-errorhandler-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container Changes", "heading_level": 3, "file_order": 0, "section_index": 133, "content_hash": "4020472a178418e1a58f844eabe4a0a6c699610944ab7fe8ec8047c3784a14b5", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:47510d7063e1c5c178d989d54d19434cb6856e34e01df0bb8e5fdc318718c514", "content": "The `SeekToCurrentErrorHandler` now treats certain exceptions as fatal and disables retry for those, invoking the recoverer on first failure.\n\nThe `SeekToCurrentErrorHandler` and `SeekToCurrentBatchErrorHandler` can now be configured to apply a `BackOff` (thread sleep) between delivery attempts.\n\nStarting with version 2.3.2, recovered records' offsets will be committed when the error handler returns after recovering a failed record.\n\nThe `DeadLetterPublishingRecoverer`, when used in conjunction with an `ErrorHandlingDeserializer`, now sets the payload of the message sent to the dead-letter topic, to the original value that could not be deserialized.\nPreviously, it was `null` and user code needed to extract the `DeserializationException` from the message headers.\nSee xref:kafka/annotation-error-handling.adoc#dead-letters[Publishing Dead-letter Records] for more information.\n\n[[cb-2-2-and-2-3-topicbuilder]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ErrorHandler Changes", "heading_level": 3, "file_order": 0, "section_index": 134, "content_hash": "47510d7063e1c5c178d989d54d19434cb6856e34e01df0bb8e5fdc318718c514", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:7050a61fccb23b61cccc376bd8a64b17359512c9c99e5f3c4cff9b2ab9e4b8de", "content": "A new class `TopicBuilder` is provided for more convenient creation of `NewTopic` ``@Bean``s for automatic topic provisioning.\nSee <<configuring-topics>> for more information.\n\n[[cb-2-2-and-2-3-kafka-streams-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "TopicBuilder", "heading_level": 3, "file_order": 0, "section_index": 135, "content_hash": "7050a61fccb23b61cccc376bd8a64b17359512c9c99e5f3c4cff9b2ab9e4b8de", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:4e2ed2a31778ad4a7d963b6cb47124dc1f0521b6b5544c3b1c658f2eb84eb1f8", "content": "You can now perform additional configuration of the `StreamsBuilderFactoryBean` created by `@EnableKafkaStreams`.\nSee xref:streams.adoc#streams-config[Streams Configuration] for more information.\n\nA `RecoveringDeserializationExceptionHandler` is now provided which allows records with deserialization errors to be recovered.\nIt can be used in conjunction with a `DeadLetterPublishingRecoverer` to send these records to a dead-letter topic.\nSee xref:streams.adoc#streams-deser-recovery[Recovery from Deserialization Exceptions] for more information.\n\nThe `HeaderEnricher` transformer has been provided, using SpEL to generate the header values.\nSee xref:streams.adoc#streams-header-enricher[Header Enricher] for more information.\n\nThe `MessagingTransformer` has been provided.\nThis allows a Kafka streams topology to interact with a spring-messaging component, such as a Spring Integration flow.\nSee xref:streams.adoc#streams-messaging[`MessagingProcessor`] and See {spring-integration-url}/kafka.html#streams-integration[Calling a Spring Integration Flow from a `KStream`] for more information.\n\n[[cb-2-2-and-2-3-json-component-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Streams Changes", "heading_level": 3, "file_order": 0, "section_index": 136, "content_hash": "4e2ed2a31778ad4a7d963b6cb47124dc1f0521b6b5544c3b1c658f2eb84eb1f8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ab6bae30fac555e9a30f57f2fc3ad89030ab7a1b56429e554a0fae73cd08050b", "content": "Now all the JSON-aware components are configured by default with a Jackson `ObjectMapper` produced by the `JacksonUtils.enhancedObjectMapper()`.\nThe `JsonDeserializer` now provides `TypeReference`-based constructors for better handling of target generic container types.\nAlso a `JacksonMimeTypeModule` has been introduced for serialization of `org.springframework.util.MimeType` to plain string.\nSee its JavaDocs and xref:kafka/serdes.adoc[Serialization, Deserialization, and Message Conversion] for more information.\n\nA `ByteArrayJsonMessageConverter` has been provided as well as a new super class for all Json converters, `JsonMessageConverter`.\nAlso, a `StringOrBytesSerializer` is now available; it can serialize `byte[]`, `Bytes` and `String` values in ``ProducerRecord``s.\nSee xref:kafka/serdes.adoc#messaging-message-conversion[Spring Messaging Message Conversion] for more information.\n\nThe `JsonSerializer`, `JsonDeserializer` and `JsonSerde` now have fluent APIs to make programmatic configuration simpler.\nSee the javadocs, xref:kafka/serdes.adoc[Serialization, Deserialization, and Message Conversion], and xref:streams.adoc#serde[Streams JSON Serialization and Deserialization] for more information.\n\n[[cb-2-2-and-2-3-replyingkafkatemplate]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "JSON Component Changes", "heading_level": 3, "file_order": 0, "section_index": 137, "content_hash": "ab6bae30fac555e9a30f57f2fc3ad89030ab7a1b56429e554a0fae73cd08050b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:0cf8c4166ade9383892ed6794ad2f050b088ac6484a3e3b941166619939e70bc", "content": "When a reply times out, the future is completed exceptionally with a `KafkaReplyTimeoutException` instead of a `KafkaException`.\n\nAlso, an overloaded `sendAndReceive` method is now provided that allows specifying the reply timeout on a per message basis.\n\n[[aggregatingreplyingkafkatemplate]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ReplyingKafkaTemplate", "heading_level": 3, "file_order": 0, "section_index": 138, "content_hash": "0cf8c4166ade9383892ed6794ad2f050b088ac6484a3e3b941166619939e70bc", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:fc5cff8abf0e7081526ea2cb9eb97ae692126b445b9767c9fdc4a3c7ea2bde70", "content": "Extends the `ReplyingKafkaTemplate` by aggregating replies from multiple receivers.\nSee xref:kafka/sending-messages.adoc#aggregating-request-reply[Aggregating Multiple Replies] for more information.\n\n[[cb-2-2-and-2-3-transaction-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "AggregatingReplyingKafkaTemplate", "heading_level": 3, "file_order": 0, "section_index": 139, "content_hash": "fc5cff8abf0e7081526ea2cb9eb97ae692126b445b9767c9fdc4a3c7ea2bde70", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:2aedc96db280479f9acb075c691d09a7dab9479ee04e1649a1862ad6527673ab", "content": "You can now override the producer factory's `transactionIdPrefix` on the `KafkaTemplate` and `KafkaTransactionManager`.\nSee xref:kafka/transactions.adoc#transaction-id-prefix[`transactionIdPrefix`] for more information.\n\n[[cb-2-2-and-2-3-new-delegating-serializerdeserializer]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Transaction Changes", "heading_level": 3, "file_order": 0, "section_index": 140, "content_hash": "2aedc96db280479f9acb075c691d09a7dab9479ee04e1649a1862ad6527673ab", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:58494ddd916856fff8873713e8f10cc26d331194afefbdbaa3e7dac855c962c2", "content": "The framework now provides a delegating `Serializer` and `Deserializer`, utilizing a header to enable producing and consuming records with multiple key/value types.\nSee xref:kafka/serdes.adoc#delegating-serialization[Delegating Serializer and Deserializer] for more information.\n\n[[cb-2-2-and-2-3-new-retrying-deserializer]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "New Delegating Serializer/Deserializer", "heading_level": 3, "file_order": 0, "section_index": 141, "content_hash": "58494ddd916856fff8873713e8f10cc26d331194afefbdbaa3e7dac855c962c2", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:69a0a77b45a13e64173c5d2b28ecdb19b34543a833419e7892442362aeba57c5", "content": "The framework now provides a delegating `RetryingDeserializer`, to retry serialization when transient errors such as network problems might occur.\nSee xref:kafka/serdes.adoc#retrying-deserialization[Retrying Deserializer] for more information.\n\n[[changes-between-2-1-and-2-2]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "New Retrying Deserializer", "heading_level": 3, "file_order": 0, "section_index": 142, "content_hash": "69a0a77b45a13e64173c5d2b28ecdb19b34543a833419e7892442362aeba57c5", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:68f899a4680ad70f5ebd1a2e791c1d7d2e82f954a5f8be9899bc3e17c6c66bea", "content": "[[cb-2-1-and-2-2-kafka-client-2.0]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes Between 2.1 and 2.2", "heading_level": 2, "file_order": 0, "section_index": 143, "content_hash": "68f899a4680ad70f5ebd1a2e791c1d7d2e82f954a5f8be9899bc3e17c6c66bea", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:583fc9862e3f16d3d6abf742cbe8346672c756ff8a03d94dca523796cca42a8c", "content": "This version requires the 2.0.0 `kafka-clients` or higher.\n\n[[cb-2-1-and-2-2-class-and-package-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 144, "content_hash": "583fc9862e3f16d3d6abf742cbe8346672c756ff8a03d94dca523796cca42a8c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:fd6e4b223f40024ef3a89fca595266525dddcc5f25701e4c10f1faa7e6b08c71", "content": "The `ContainerProperties` class has been moved from `org.springframework.kafka.listener.config` to `org.springframework.kafka.listener`.\n\nThe `AckMode` enum has been moved from `AbstractMessageListenerContainer` to `ContainerProperties`.\n\nThe `setBatchErrorHandler()` and `setErrorHandler()` methods have been moved from `ContainerProperties` to both `AbstractMessageListenerContainer` and `AbstractKafkaListenerContainerFactory`.\n\n[[cb-2-1-and-2-2-after-rollback-processing]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Class and Package Changes", "heading_level": 3, "file_order": 0, "section_index": 145, "content_hash": "fd6e4b223f40024ef3a89fca595266525dddcc5f25701e4c10f1faa7e6b08c71", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ded3ee05ca82c5a6555861e3957df4755f8e9b3449601fd271947f46bc92b71e", "content": "A new `AfterRollbackProcessor` strategy is provided.\nSee xref:kafka/annotation-error-handling.adoc#after-rollback[After-rollback Processor] for more information.\n\n[[cb-2-1-and-2-2-concurrentkafkalistenercontainerfactory-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "After Rollback Processing", "heading_level": 3, "file_order": 0, "section_index": 146, "content_hash": "ded3ee05ca82c5a6555861e3957df4755f8e9b3449601fd271947f46bc92b71e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:aa27b0a70624b84b6176b6471d82c37216efe9a380ee71624607750b282e487a", "content": "You can now use the `ConcurrentKafkaListenerContainerFactory` to create and configure any `ConcurrentMessageListenerContainer`, not only those for `@KafkaListener` annotations.\nSee xref:kafka/container-factory.adoc[Container factory] for more information.\n\n[[cb-2-1-and-2-2-listener-container-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`ConcurrentKafkaListenerContainerFactory` Changes", "heading_level": 3, "file_order": 0, "section_index": 147, "content_hash": "aa27b0a70624b84b6176b6471d82c37216efe9a380ee71624607750b282e487a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:92f4de2731c20223a626ca701ec77dcaffbe41f75b8bce39988177b896678ca7", "content": "A new container property (`missingTopicsFatal`) has been added.\nSee xref:kafka/receiving-messages/message-listener-container.adoc#kafka-container[Using `KafkaMessageListenerContainer`] for more information.\n\nA `ConsumerStoppedEvent` is now emitted when a consumer stops.\nSee xref:kafka/thread-safety.adoc[Thread Safety] for more information.\n\nBatch listeners can optionally receive the complete `ConsumerRecords<?, ?>` object instead of a `List<ConsumerRecord<?, ?>`.\nSee <<batch-listeners>> for more information.\n\nThe `DefaultAfterRollbackProcessor` and `SeekToCurrentErrorHandler` can now recover (skip) records that keep failing, and, by default, does so after 10 failures.\nThey can be configured to publish failed records to a dead-letter topic.\n\nStarting with version 2.2.4, the consumer's group ID can be used while selecting the dead letter topic name.\n\nThe `ConsumerStoppingEvent` has been added.\nSee xref:kafka/events.adoc[Application Events] for more information.\n\nThe `SeekToCurrentErrorHandler` can now be configured to commit the offset of a recovered record when the container is configured with `AckMode.MANUAL_IMMEDIATE` (since 2.2.4).\n\n[[cb-2-1-and-2-2-kafkalistener-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Listener Container Changes", "heading_level": 3, "file_order": 0, "section_index": 148, "content_hash": "92f4de2731c20223a626ca701ec77dcaffbe41f75b8bce39988177b896678ca7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:e28e8d5b8502805b1947ed423fcb7e557fa8a5b3a7648e8bd0cf7460a0bf698e", "content": "You can now override the `concurrency` and `autoStartup` properties of the listener container factory by setting properties on the annotation.\nYou can now add configuration to determine which headers (if any) are copied to a reply message.\nSee xref:kafka/receiving-messages/listener-annotation.adoc[`@KafkaListener` Annotation] for more information.\n\nYou can now use `@KafkaListener` as a meta-annotation on your own annotations.\nSee xref:kafka/receiving-messages/listener-meta.adoc[`@KafkaListener` as a Meta Annotation] for more information.\n\nIt is now easier to configure a `Validator` for `@Payload` validation.\nSee xref:kafka/receiving-messages/validation.adoc[`@KafkaListener` `@Payload` Validation] for more information.\n\nYou can now specify kafka consumer properties directly on the annotation; these will override any properties with the same name defined in the consumer factory (since version 2.2.4).\nSee xref:kafka/receiving-messages/listener-annotation.adoc#annotation-properties[Annotation Properties] for more information.\n\n[[cb-2-1-and-2-2-header-mapping-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "@KafkaListener Changes", "heading_level": 3, "file_order": 0, "section_index": 149, "content_hash": "e28e8d5b8502805b1947ed423fcb7e557fa8a5b3a7648e8bd0cf7460a0bf698e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ef5c2b4ba5bceeb710988dafd1301a563edd8963c47f41a99287f53bed70c050", "content": "Headers of type `MimeType` and `MediaType` are now mapped as simple strings in the `RecordHeader` value.\nPreviously, they were mapped as JSON and only `MimeType` was decoded.\n`MediaType` could not be decoded.\nThey are now simple strings for interoperability.\n\nAlso, the `JsonKafkaHeaderMapper` has a new `addToStringClasses` method, allowing the specification of types that should be mapped by using `toString()` instead of JSON.\nSee xref:kafka/headers.adoc[Message Headers] for more information.\n\n[[cb-2-1-and-2-2-embedded-kafka-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Header Mapping Changes", "heading_level": 3, "file_order": 0, "section_index": 150, "content_hash": "ef5c2b4ba5bceeb710988dafd1301a563edd8963c47f41a99287f53bed70c050", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:1103a966b095d1d6574e3e2edd57e943d32a0e62323cce03df1a61ab73d029e4", "content": "The `KafkaEmbedded` class and its `KafkaRule` interface have been deprecated in favor of the `EmbeddedKafkaBroker` and its JUnit 4 `EmbeddedKafkaRule` wrapper.\nThe `@EmbeddedKafka` annotation now populates an `EmbeddedKafkaBroker` bean instead of the deprecated `KafkaEmbedded`.\nThis change allows the use of `@EmbeddedKafka` in JUnit 5 tests.\nThe `@EmbeddedKafka` annotation now has the attribute `ports` to specify the port that populates the `EmbeddedKafkaBroker`.\nSee xref:testing.adoc[Testing Applications] for more information.\n\n[[cb-2-1-and-2-2-jsonserializer-deserializer-enhancements]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Embedded Kafka Changes", "heading_level": 3, "file_order": 0, "section_index": 151, "content_hash": "1103a966b095d1d6574e3e2edd57e943d32a0e62323cce03df1a61ab73d029e4", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:44600f4804e6b73c86aaa3bf71791d57e288ceffa8c1dbac424a2e401d86a599", "content": "You can now provide type mapping information by using producer and consumer properties.\n\nNew constructors are available on the deserializer to allow overriding the type header information with the supplied target type.\n\nThe `JsonDeserializer` now removes any type information headers by default.\n\nYou can now configure the `JsonDeserializer` to ignore type information headers by using a Kafka property (since 2.2.3).\n\nSee xref:kafka/serdes.adoc[Serialization, Deserialization, and Message Conversion] for more information.\n\n[[cb-2-1-and-2-2-kafka-streams-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "JsonSerializer/Deserializer Enhancements", "heading_level": 3, "file_order": 0, "section_index": 152, "content_hash": "44600f4804e6b73c86aaa3bf71791d57e288ceffa8c1dbac424a2e401d86a599", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:c9fa6a278b96fde314ed2566ac1413245aaf4ab03c9ef15f3fdfe549dcace61b", "content": "The streams configuration bean must now be a `KafkaStreamsConfiguration` object instead of a `StreamsConfig` object.\n\nThe `StreamsBuilderFactoryBean` has been moved from package `...core` to `...config`.\n\nThe `KafkaStreamBrancher` has been introduced for better end-user experience when conditional branches are built on top of `KStream` instance.\n\nSee xref:streams.adoc[Apache Kafka Streams Support] and xref:streams.adoc#streams-config[Configuration] for more information.\n\n[[cb-2-1-and-2-2-transactional-id]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Streams Changes", "heading_level": 3, "file_order": 0, "section_index": 153, "content_hash": "c9fa6a278b96fde314ed2566ac1413245aaf4ab03c9ef15f3fdfe549dcace61b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:c411efc1010d0104bd3d76017c61dc89466d97a52efdd05abbfbc2d4809668b4", "content": "When a transaction is started by the listener container, the `transactional.id` is now the `transactionIdPrefix` appended with `<group.id>.<topic>.<partition>`.\nThis change allows proper fencing of zombies, https://www.confluent.io/blog/transactions-apache-kafka/[as described here].\n\n[[changes-between-2-0-and-2-1]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Transactional ID", "heading_level": 3, "file_order": 0, "section_index": 154, "content_hash": "c411efc1010d0104bd3d76017c61dc89466d97a52efdd05abbfbc2d4809668b4", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:648b0b73c3e884eaf48cd6bd34979da032edc088b0e09833df51f25839ba0220", "content": "[[cb-2-0-and-2-1-kafka-client-1.0]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes Between 2.0 and 2.1", "heading_level": 2, "file_order": 0, "section_index": 155, "content_hash": "648b0b73c3e884eaf48cd6bd34979da032edc088b0e09833df51f25839ba0220", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:0374003ad53dfcb98fd9714eb6aa215c571d3a53e5ca972096f615fdb5763120", "content": "This version requires the 1.0.0 `kafka-clients` or higher.\n\nThe 1.1.x client is supported natively in version 2.2.\n\n[[cb-2-0-and-2-1-json-improvements]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client Version", "heading_level": 3, "file_order": 0, "section_index": 156, "content_hash": "0374003ad53dfcb98fd9714eb6aa215c571d3a53e5ca972096f615fdb5763120", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:27f71da7aea789a328974b020dc32bd9f3733c9d4a499d86627e6563014b999c", "content": "The `StringJsonMessageConverter` and `JsonSerializer` now add type information in `Headers`, letting the converter and `JsonDeserializer` create specific types on reception, based on the message itself rather than a fixed configured type.\nSee xref:kafka/serdes.adoc[Serialization, Deserialization, and Message Conversion] for more information.\n\n[[cb-2-0-and-2-1-container-stopping-error-handlers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "JSON Improvements", "heading_level": 3, "file_order": 0, "section_index": 157, "content_hash": "27f71da7aea789a328974b020dc32bd9f3733c9d4a499d86627e6563014b999c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:bed0d175bdef558e31dfa4b5070eb6a88f6963adf4a822f102a0bc25a3706113", "content": "Container error handlers are now provided for both record and batch listeners that treat any exceptions thrown by the listener as fatal/\nThey stop the container.\nSee xref:kafka/annotation-error-handling.adoc[Handling Exceptions] for more information.\n\n[[cb-2-0-and-2-1-pausing-and-resuming-containers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Container Stopping Error Handlers", "heading_level": 3, "file_order": 0, "section_index": 158, "content_hash": "bed0d175bdef558e31dfa4b5070eb6a88f6963adf4a822f102a0bc25a3706113", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:4172782536785cfbe1e3df9ac67ddfb344fbfd60965c9e0a3fde593068901d05", "content": "The listener containers now have `pause()` and `resume()` methods (since version 2.1.3).\nSee xref:kafka/pause-resume.adoc[Pausing and Resuming Listener Containers] for more information.\n\n[[cb-2-0-and-2-1-stateful-retry]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Pausing and Resuming Containers", "heading_level": 3, "file_order": 0, "section_index": 159, "content_hash": "4172782536785cfbe1e3df9ac67ddfb344fbfd60965c9e0a3fde593068901d05", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:a1f0cfd7d00dbc745a9b34ef5d19a916ce223a26ea10f014d0af819bf56b4b29", "content": "Starting with version 2.1.3, you can configure stateful retry.\nSee xref:appendix/change-history.adoc#cb-2-0-and-2-1-stateful-retry[Stateful Retry] for more information.\n\n[[cb-2-0-and-2-1-client-id]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Stateful Retry", "heading_level": 3, "file_order": 0, "section_index": 160, "content_hash": "a1f0cfd7d00dbc745a9b34ef5d19a916ce223a26ea10f014d0af819bf56b4b29", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:0d1013350db628c7443b5dcf52a90885c6674cec3b1628e0922fea3bea301890", "content": "Starting with version 2.1.1, you can now set the `client.id` prefix on `@KafkaListener`.\nPreviously, to customize the client ID, you needed a separate consumer factory (and container factory) per listener.\nThe prefix is suffixed with `-n` to provide unique client IDs when you use concurrency.\n\n[[cb-2-0-and-2-1-logging-offset-commits]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Client ID", "heading_level": 3, "file_order": 0, "section_index": 161, "content_hash": "0d1013350db628c7443b5dcf52a90885c6674cec3b1628e0922fea3bea301890", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:5c3d27d6b2579ab006d6f21abfc75b96b433391e55584bd1d03e82e48c95beae", "content": "By default, logging of topic offset commits is performed with the `DEBUG` logging level.\nStarting with version 2.1.2, a new property in `ContainerProperties` called `commitLogLevel` lets you specify the log level for these messages.\nSee xref:kafka/receiving-messages/message-listener-container.adoc#kafka-container[Using `KafkaMessageListenerContainer`] for more information.\n\n[[cb-2-0-and-2-1-default-kafkahandler]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Logging Offset Commits", "heading_level": 3, "file_order": 0, "section_index": 162, "content_hash": "5c3d27d6b2579ab006d6f21abfc75b96b433391e55584bd1d03e82e48c95beae", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:5b92016f47ec1129661bf0bb7800fa33123509d201934a7d7d0de03015e11fd9", "content": "Starting with version 2.1.3, you can designate one of the `@KafkaHandler` annotations on a class-level `@KafkaListener` as the default.\nSee xref:kafka/receiving-messages/class-level-kafkalistener.adoc[`@KafkaListener` on a Class] for more information.\n\n[[cb-2-0-and-2-1-replyingkafkatemplate]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Default @KafkaHandler", "heading_level": 3, "file_order": 0, "section_index": 163, "content_hash": "5b92016f47ec1129661bf0bb7800fa33123509d201934a7d7d0de03015e11fd9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:a32db74a8e70d6f109798bd20fec1cc75b89a7492c9fd6f21d0ee5bbff381277", "content": "Starting with version 2.1.3, a subclass of `KafkaTemplate` is provided to support request/reply semantics.\nSee xref:kafka/sending-messages.adoc#replying-template[Using `ReplyingKafkaTemplate`] for more information.\n\n[[cb-2-0-and-2-1-chainedkafkatransactionmanager]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ReplyingKafkaTemplate", "heading_level": 3, "file_order": 0, "section_index": 164, "content_hash": "a32db74a8e70d6f109798bd20fec1cc75b89a7492c9fd6f21d0ee5bbff381277", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:ff7ad36b72ff91a53fa073cc65ebd32f45c355e342e72d3bed5ba808c161207c", "content": "Version 2.1.3 introduced the `ChainedKafkaTransactionManager`.\n(It is now deprecated).\n\n[[cb-2-0-and-2-1-migration-guide-from-2-0]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "ChainedKafkaTransactionManager", "heading_level": 3, "file_order": 0, "section_index": 165, "content_hash": "ff7ad36b72ff91a53fa073cc65ebd32f45c355e342e72d3bed5ba808c161207c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:826ca2d8b9819ca760e6cfee2d7b20cd98bb4ef4c47f86e7d571243278c4e6bd", "content": "See the https://github.com/spring-projects/spring-kafka/wiki/Spring-for-Apache-Kafka-2.0-to-2.1-Migration-Guide[2.0 to 2.1 Migration] guide.\n\n[[changes-between-1-3-and-2-0]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Migration Guide from 2.0", "heading_level": 3, "file_order": 0, "section_index": 166, "content_hash": "826ca2d8b9819ca760e6cfee2d7b20cd98bb4ef4c47f86e7d571243278c4e6bd", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:1d384906165e18e61b26b71c33f4c51310b182b398b4f208af0d551deae2b140", "content": "[[cb-1-3-and-2-0-spring-framework-and-java-versions]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes Between 1.3 and 2.0", "heading_level": 2, "file_order": 0, "section_index": 167, "content_hash": "1d384906165e18e61b26b71c33f4c51310b182b398b4f208af0d551deae2b140", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:f60414ba94219bb178a6e6998601243e51d94da5b3039ed1450355fe3622de20", "content": "The Spring for Apache Kafka project now requires Spring Framework 5.0 and Java 8.\n\n[[cb-1-3-and-2-0-kafkalistener-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Spring Framework and Java Versions", "heading_level": 3, "file_order": 0, "section_index": 168, "content_hash": "f60414ba94219bb178a6e6998601243e51d94da5b3039ed1450355fe3622de20", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:eda34d64c75a439fa9aa5beb85ce984d1cbb12466a272dec40fa2ee53647a544", "content": "You can now annotate `@KafkaListener` methods (and classes and `@KafkaHandler` methods) with `@SendTo`.\nIf the method returns a result, it is forwarded to the specified topic.\nSee xref:kafka/receiving-messages/annotation-send-to.adoc[Forwarding Listener Results using `@SendTo`] for more information.\n\n[[cb-1-3-and-2-0-message-listeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`@KafkaListener` Changes", "heading_level": 3, "file_order": 0, "section_index": 169, "content_hash": "eda34d64c75a439fa9aa5beb85ce984d1cbb12466a272dec40fa2ee53647a544", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:2e324fca95cdb4a50dadcd7ff36f20d9b7294897e6c950c5b9880132c34e71c9", "content": "Message listeners can now be aware of the `Consumer` object.\nSee <<message-listeners>> for more information.\n\n[[cb-1-3-and-2-0-using-consumerawarerebalancelistener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Message Listeners", "heading_level": 3, "file_order": 0, "section_index": 170, "content_hash": "2e324fca95cdb4a50dadcd7ff36f20d9b7294897e6c950c5b9880132c34e71c9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:f49516cce11576b134137e97958e6539c3b83109c9e61a1d6ffe4648bd241701", "content": "Rebalance listeners can now access the `Consumer` object during rebalance notifications.\nSee xref:kafka/receiving-messages/rebalance-listeners.adoc[Rebalancing Listeners] for more information.\n\n[[changes-between-1-2-and-1-3]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Using `ConsumerAwareRebalanceListener`", "heading_level": 3, "file_order": 0, "section_index": 171, "content_hash": "f49516cce11576b134137e97958e6539c3b83109c9e61a1d6ffe4648bd241701", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:9b0ccdd29a75a44de8bfbfdd76e3db9c89ea605dbecfc44ccb78a66064fcac97", "content": "[[cb-1-2-and-1-3-support-for-transactions]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes Between 1.2 and 1.3", "heading_level": 2, "file_order": 0, "section_index": 172, "content_hash": "9b0ccdd29a75a44de8bfbfdd76e3db9c89ea605dbecfc44ccb78a66064fcac97", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:c9baa5d827d22b49b9e7e888c240ead414cba30e0182e8b2e90109c04aa552c8", "content": "The 0.11.0.0 client library added support for transactions.\nThe `KafkaTransactionManager` and other support for transactions have been added.\nSee xref:kafka/transactions.adoc[Transactions] for more information.\n\n[[cb-1-2-and-1-3-support-for-headers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Support for Transactions", "heading_level": 3, "file_order": 0, "section_index": 173, "content_hash": "c9baa5d827d22b49b9e7e888c240ead414cba30e0182e8b2e90109c04aa552c8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:26c8e073fc442f56679a73023040cbb57424c358dd0001416628841e133d3ae5", "content": "The 0.11.0.0 client library added support for message headers.\nThese can now be mapped to and from `spring-messaging` `MessageHeaders`.\nSee xref:kafka/headers.adoc[Message Headers] for more information.\n\n[[cb-1-2-and-1-3-creating-topics]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Support for Headers", "heading_level": 3, "file_order": 0, "section_index": 174, "content_hash": "26c8e073fc442f56679a73023040cbb57424c358dd0001416628841e133d3ae5", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:2538c14f0855fea2d91b2c32b33baf10637ea1264b6b65a0014677b9ba125366", "content": "The 0.11.0.0 client library provides an `AdminClient`, which you can use to create topics.\nThe `KafkaAdmin` uses this client to automatically add topics defined as `@Bean` instances.\n\n[[cb-1-2-and-1-3-support-for-kafka-timestamps]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Creating Topics", "heading_level": 3, "file_order": 0, "section_index": 175, "content_hash": "2538c14f0855fea2d91b2c32b33baf10637ea1264b6b65a0014677b9ba125366", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:042f97a0b3011749594c3204c1e51c1efe69cd4ccc35e5534195ed67c34bb660", "content": "`KafkaTemplate` now supports an API to add records with timestamps.\nNew `KafkaHeaders` have been introduced regarding `timestamp` support.\nAlso, new `KafkaConditions.timestamp()` and `KafkaMatchers.hasTimestamp()` testing utilities have been added.\nSee xref:kafka/sending-messages.adoc#kafka-template[Using `KafkaTemplate`], xref:kafka/receiving-messages/listener-annotation.adoc[`@KafkaListener` Annotation], and xref:testing.adoc[Testing Applications] for more details.\n\n[[cb-1-2-and-1-3-kafkalistener-changes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Support for Kafka Timestamps", "heading_level": 3, "file_order": 0, "section_index": 176, "content_hash": "042f97a0b3011749594c3204c1e51c1efe69cd4ccc35e5534195ed67c34bb660", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:e563cc4000482905083593b7e553200a5d612a159ec7527ef9f2d420d2a63253", "content": "You can now configure a `KafkaListenerErrorHandler` to handle exceptions.\nSee xref:kafka/annotation-error-handling.adoc[Handling Exceptions] for more information.\n\nBy default, the `@KafkaListener` `id` property is now used as the `group.id` property, overriding the property configured in the consumer factory (if present).\nFurther, you can explicitly configure the `groupId` on the annotation.\nPreviously, you would have needed a separate container factory (and consumer factory) to use different `group.id` values for listeners.\nTo restore the previous behavior of using the factory configured `group.id`, set the `idIsGroup` property on the annotation to `false`.\n\n[[cb-1-2-and-1-3-embeddedkafka-annotation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`@KafkaListener` Changes", "heading_level": 3, "file_order": 0, "section_index": 177, "content_hash": "e563cc4000482905083593b7e553200a5d612a159ec7527ef9f2d420d2a63253", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:b0b4b3a6106d262bc23d634c4800ac6e4aa35cb9f0125cb8b24175ad1e892d6c", "content": "For convenience, a test class-level `@EmbeddedKafka` annotation is provided, to register `KafkaEmbedded` as a bean.\nSee xref:testing.adoc[Testing Applications] for more information.\n\n[[cb-1-2-and-1-3-kerberos-configuration]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "`@EmbeddedKafka` Annotation", "heading_level": 3, "file_order": 0, "section_index": 178, "content_hash": "b0b4b3a6106d262bc23d634c4800ac6e4aa35cb9f0125cb8b24175ad1e892d6c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:e8a9e7b1daf527f7c7feaf17e84345699b4462f0c03bbaf9803c9b86af397e27", "content": "Support for configuring Kerberos is now provided.\nSee xref:kafka/kerberos.adoc[JAAS and Kerberos] for more information.\n\n[[changes-between-1-1-and-1-2]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kerberos Configuration", "heading_level": 3, "file_order": 0, "section_index": 179, "content_hash": "e8a9e7b1daf527f7c7feaf17e84345699b4462f0c03bbaf9803c9b86af397e27", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:5f8feede2b26a7ae8e5d31bf3e58f7760561cd0e1841f156a50008b356a9b636", "content": "This version uses the 0.10.2.x client.\n\n[[cb-1-1-and-1-2-changes-between-1-0-and-1-1]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes Between 1.1 and 1.2", "heading_level": 2, "file_order": 0, "section_index": 180, "content_hash": "5f8feede2b26a7ae8e5d31bf3e58f7760561cd0e1841f156a50008b356a9b636", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:73159a43f307249deacb064fba1ec3459c060e61eb4570c9d0945ae40c371e37", "content": "[[cb-1-1-and-1-2-kafka-client]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Changes Between 1.0 and 1.1", "heading_level": 2, "file_order": 0, "section_index": 181, "content_hash": "73159a43f307249deacb064fba1ec3459c060e61eb4570c9d0945ae40c371e37", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:59c61a402717f14cc0ad1f62fcd9087b2e5d374259f5c43285aeccd12830262d", "content": "This version uses the Apache Kafka 0.10.x.x client.\n\n[[cb-1-1-and-1-2-batch-listeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Kafka Client", "heading_level": 3, "file_order": 0, "section_index": 182, "content_hash": "59c61a402717f14cc0ad1f62fcd9087b2e5d374259f5c43285aeccd12830262d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:78bc13c77d88c19b16c245939e47b053145505899e53dafca18003de5e945ddd", "content": "Listeners can be configured to receive the entire batch of messages returned by the `consumer.poll()` operation, rather than one at a time.\n\n[[cb-1-1-and-1-2-null-payloads]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Batch Listeners", "heading_level": 3, "file_order": 0, "section_index": 183, "content_hash": "78bc13c77d88c19b16c245939e47b053145505899e53dafca18003de5e945ddd", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:28ad86b6b0f5af83dc1a7a2e015e6884890ccc9d8852ce0ae6a5c7d130a97695", "content": "Null payloads are used to \"`delete`\" keys when you use log compaction.\n\n[[cb-1-1-and-1-2-initial-offset]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Null Payloads", "heading_level": 3, "file_order": 0, "section_index": 184, "content_hash": "28ad86b6b0f5af83dc1a7a2e015e6884890ccc9d8852ce0ae6a5c7d130a97695", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:6c95ae1ce0f2adb92ba7b8c7b8a4c7f040d7e8d9a1fd2d0f9515e145d4891b36", "content": "When explicitly assigning partitions, you can now configure the initial offset relative to the current position for the consumer group, rather than absolute or relative to the current end.\n\n[[cb-1-1-and-1-2-seek]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Initial Offset", "heading_level": 3, "file_order": 0, "section_index": 185, "content_hash": "6c95ae1ce0f2adb92ba7b8c7b8a4c7f040d7e8d9a1fd2d0f9515e145d4891b36", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:2e7dd4ebe8470be88cb0ccf2f75cf6576a53543afe6b50e50558786e4b0e7ef3", "content": "You can now seek the position of each topic or partition.\nYou can use this to set the initial position during initialization when group management is in use and Kafka assigns the partitions.\nYou can also seek when an idle container is detected or at any arbitrary point in your application's execution.\nSee <<seek>> for more information.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/change-history.adoc", "title": "change-history", "heading": "Seek", "heading_level": 3, "file_order": 0, "section_index": 186, "content_hash": "2e7dd4ebe8470be88cb0ccf2f75cf6576a53543afe6b50e50558786e4b0e7ef3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/change-history.adoc"}}
{"id": "sha256:6696c1fb71949dffb477cb1c29a99c67604e83d32149ef9df56cd41682aac844", "content": "[[observation-gen]]\n\ninclude::partial$metrics.adoc[leveloffset=-1]\n\ninclude::partial$spans.adoc[leveloffset=-1]\n\ninclude::partial$conventions.adoc[leveloffset=-1]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/micrometer.adoc", "title": "micrometer", "heading": "micrometer", "heading_level": 1, "file_order": 1, "section_index": 0, "content_hash": "6696c1fb71949dffb477cb1c29a99c67604e83d32149ef9df56cd41682aac844", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/micrometer.adoc"}}
{"id": "sha256:57a08ca003f082f82b404542652e6b470c1c99413002724546b0f723c2e7591a", "content": "[[native-images]]\n\n{spring-framework-reference-url}/core/aot.html[Spring AOT] native hints are provided to assist in developing native images for Spring applications that use Spring for Apache Kafka, including hints for AVRO generated classes used in ``@KafkaListener``s.\n\nIMPORTANT: `spring-kafka-test` (and, specifically, its `EmbeddedKafkaBroker`) is not supported in native images.\n\nSome examples can be seen in the https://github.com/spring-projects/spring-aot-smoke-tests/tree/main/integration[`spring-aot-smoke-tests` GitHub repository].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/native-images.adoc", "title": "native-images", "heading": "native-images", "heading_level": 1, "file_order": 2, "section_index": 0, "content_hash": "57a08ca003f082f82b404542652e6b470c1c99413002724546b0f723c2e7591a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/native-images.adoc"}}
{"id": "sha256:d2d1765fa3970e8c0d3b8624d0137c437f39ce603846ea016c9668dc2ec9d57e", "content": "[[update-deps]]\n\nWhen using Spring for Apache Kafka in a Spring Boot application, the Apache Kafka dependency versions are determined by Spring Boot's dependency management.\nIf you wish to use a different version of `kafka-clients` or `kafka-streams`, and use the embedded kafka broker for testing, you need to override their version used by Spring Boot dependency management; set the `kafka.version` property.\n\nNOTE: Both Spring Boot 3.5.x and 3.4.x use the `kafka-clients` version 3.8.x and if users need to use 3.9.x client, they have to manually upgrade it using the method below.\n\nOr, to use a different Spring for Apache Kafka version with a supported Spring Boot version, set the `spring-kafka.version` property.\n\n[tabs]\n======\nMaven::\n+\n[source, xml, subs=\"+attributes\", role=\"primary\"]\n----\n<properties>\n <kafka.version>4.0.0</kafka.version>\n <spring-kafka.version>{project-version}</spring-kafka.version>\n</properties>\n\n<dependency>\n <groupId>org.springframework.kafka</groupId>\n <artifactId>spring-kafka</artifactId>\n</dependency>\n<!-- optional - only needed when using kafka-streams -->\n<dependency>\n <groupId>org.apache.kafka</groupId>\n <artifactId>kafka-streams</artifactId>\n</dependency>\n\n<dependency>\n <groupId>org.springframework.kafka</groupId>\n <artifactId>spring-kafka-test</artifactId>\n <scope>test</scope>\n</dependency>\n----\n\nGradle::\n+\n[source, groovy, subs=\"+attributes\", role=\"secondary\"]\n----\next['kafka.version'] = '3.5.0'\next['spring-kafka.version'] = '{project-version}'\n\ndependencies {\n implementation 'org.springframework.kafka:spring-kafka'\n implementation 'org.apache.kafka:kafka-streams' // optional - only needed when using kafka-streams\n testImplementation 'org.springframework.kafka:spring-kafka-test'\n}\n----\n======\n\nThe test scope dependencies are only needed if you are using the embedded Kafka broker in tests.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix/override-boot-dependencies.adoc", "title": "override-boot-dependencies", "heading": "override-boot-dependencies", "heading_level": 1, "file_order": 3, "section_index": 0, "content_hash": "d2d1765fa3970e8c0d3b8624d0137c437f39ce603846ea016c9668dc2ec9d57e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix/override-boot-dependencies.adoc"}}
{"id": "sha256:b99cb61f0d2c27c89928662d77ca547740c0fea07f8c1fe067d31c1edebe4ea9", "content": "[[annotation-send-to]]\n\nStarting with version 2.0, if you also annotate a `@KafkaListener` with a `@SendTo` annotation and the method invocation returns a result, the result is forwarded to the topic specified by the `@SendTo`.\n\nThe `@SendTo` value can have several forms:\n\n* `@SendTo(\"someTopic\")` routes to the literal topic.\n* `+@SendTo(\"#{someExpression}\")+` routes to the topic determined by evaluating the expression once during application context initialization.\n* `+@SendTo(\"!{someExpression}\")+` routes to the topic determined by evaluating the expression at runtime.\nThe `#root` object for the evaluation has three properties:\n** `request`: The inbound `ConsumerRecord` (or `ConsumerRecords` object for a batch listener).\n** `source`: The `org.springframework.messaging.Message<?>` converted from the `request`.\n** `result`: The method return result.\n* `@SendTo` (no properties): This is treated as `!{source.headers['kafka_replyTopic']}` (since version 2.1.3).\n\nStarting with versions 2.1.11 and 2.2.1, property placeholders are resolved within `@SendTo` values.\n\nThe result of the expression evaluation must be a `String` that represents the topic name.\nThe following examples show the various ways to use `@SendTo`:\n\n[source, java]\n----\n@KafkaListener(topics = \"annotated21\")\n@SendTo(\"!{request.value()}\") // runtime SpEL\npublic String replyingListener(String in) {\n ...\n}\n\n@KafkaListener(topics = \"${some.property:annotated22}\")\n@SendTo(\"#{myBean.replyTopic}\") // config time SpEL\npublic Collection<String> replyingBatchListener(List<String> in) {\n ...\n}\n\n@KafkaListener(topics = \"annotated23\", errorHandler = \"replyErrorHandler\")\n@SendTo(\"annotated23reply\") // static reply topic definition\npublic String replyingListenerWithErrorHandler(String in) {\n ...\n}\n...\n@KafkaListener(topics = \"annotated25\")\n@SendTo(\"annotated25reply1\")\npublic class MultiListenerSendTo {\n\n @KafkaHandler\n public String foo(String in) {\n ...\n }\n\n @KafkaHandler\n @SendTo(\"!{'annotated25reply2'}\")\n public String bar(@Payload(required = false) KafkaNull nul,\n @Header(KafkaHeaders.RECEIVED_KEY) int key) {\n ...\n }\n\n}\n----\n\nIMPORTANT: In order to support `@SendTo`, the listener container factory must be provided with a `KafkaTemplate` (in its `replyTemplate` property), which is used to send the reply.\nThis should be a `KafkaTemplate` and not a `ReplyingKafkaTemplate` which is used on the client-side for request/reply processing.\nWhen using Spring Boot, it will auto-configure the template into the factory; when configuring your own factory, it must be set as shown in the examples below.\n\nStarting with version 2.2, you can add a `ReplyHeadersConfigurer` to the listener container factory.\nThis is consulted to determine which headers you want to set in the reply message.\nThe following example shows how to add a `ReplyHeadersConfigurer`:\n\n[source, java]\n----\n@Bean\npublic ConcurrentKafkaListenerContainerFactory<Integer, String> kafkaListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory<Integer, String> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n factory.setConsumerFactory(cf());\n factory.setReplyTemplate(template());\n factory.setReplyHeadersConfigurer((k, v) -> k.equals(\"cat\"));\n return factory;\n}\n----\n\nYou can also add more headers if you wish.\nThe following example shows how to do so:\n\n[source, java]\n----\n@Bean\npublic ConcurrentKafkaListenerContainerFactory<Integer, String> kafkaListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory<Integer, String> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n factory.setConsumerFactory(cf());\n factory.setReplyTemplate(template());\n factory.setReplyHeadersConfigurer(new ReplyHeadersConfigurer() {\n\n @Override\n public boolean shouldCopy(String headerName, Object headerValue) {\n return false;\n }\n\n @Override\n public Map<String, Object> additionalHeaders() {\n return Collections.singletonMap(\"qux\", \"fiz\");\n }\n\n });\n return factory;\n}\n----\n\nWhen you use `@SendTo`, you must configure the `ConcurrentKafkaListenerContainerFactory` with a `KafkaTemplate` in its `replyTemplate` property to perform the send.\nSpring Boot will automatically wire in its auto-configured template (or any if a single instance is present).\n\nNOTE: Unless you use xref:kafka/sending-messages.adoc#replying-template[request/reply semantics], only the simple `send(topic, value)` method is used, so you may wish to create a subclass to generate the partition or key.\nThe following example shows how to do so:\n\n[source, java]\n----\n@Bean\npublic KafkaTemplate<String, String> myReplyingTemplate() {\n return new KafkaTemplate<String, String>(producerFactory()) {\n\n @Override\n public CompletableFuture<SendResult<String, String>> send(String topic, String data) {\n return super.send(topic, partitionForData(data), keyForData(data), data);\n }\n\n ...\n\n };\n}\n----\n\n[IMPORTANT]\n====\nIf the listener method returns `Message<?>` or `Collection<Message<?>>`, the listener method is responsible for setting up the message headers for the reply.\nFor example, when handling a request from a `ReplyingKafkaTemplate`, you might do the following:\n\n=====\n[source, java]\n----\n@KafkaListener(id = \"messageReturned\", topics = \"someTopic\")\npublic Message<?> listen(String in, @Header(KafkaHeaders.REPLY_TOPIC) byte[] replyTo,\n @Header(KafkaHeaders.CORRELATION_ID) byte[] correlation) {\n return MessageBuilder.withPayload(in.toUpperCase())\n .setHeader(KafkaHeaders.TOPIC, replyTo)\n .setHeader(KafkaHeaders.KEY, 42)\n .setHeader(KafkaHeaders.CORRELATION_ID, correlation)\n .setHeader(\"someOtherHeader\", \"someValue\")\n .build();\n}\n----\n=====\n====\n\nWhen using request/reply semantics, the target partition can be requested by the sender.\n\n[NOTE]\n====\nYou can annotate a `@KafkaListener` method with `@SendTo` even if no result is returned.\nThis is to allow the configuration of an `errorHandler` that can forward information about a failed message delivery to some topic.\nThe following example shows how to do so:\n\n=====\n[source, java]\n----\n@KafkaListener(id = \"voidListenerWithReplyingErrorHandler\", topics = \"someTopic\",\n errorHandler = \"voidSendToErrorHandler\")\n@SendTo(\"failures\")\npublic void voidListenerWithReplyingErrorHandler(String in) {\n throw new RuntimeException(\"fail\");\n}\n\n@Bean\npublic KafkaListenerErrorHandler voidSendToErrorHandler() {\n return (m, e) -> {\n return ... // some information about the failure and input data\n };\n}\n----\n=====\n\nSee xref:kafka/annotation-error-handling.adoc[Handling Exceptions] for more information.\n====\n\nNOTE: If a listener method returns an `Iterable`, by default a record for each element as the value is sent.\nStarting with version 2.3.5, set the `splitIterables` property on `@KafkaListener` to `false` and the entire result will be sent as the value of a single `ProducerRecord`.\nThis requires a suitable serializer in the reply template's producer configuration.\nHowever, if the reply is `Iterable<Message<?>>` the property is ignored and each message is sent separately.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/annotation-send-to.adoc", "title": "annotation-send-to", "heading": "annotation-send-to", "heading_level": 1, "file_order": 4, "section_index": 0, "content_hash": "b99cb61f0d2c27c89928662d77ca547740c0fea07f8c1fe067d31c1edebe4ea9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/annotation-send-to.adoc"}}
{"id": "sha256:bbc8184e883d5350ed909d7ea719da04c7b02eb141ad92ac226089cc91815bb1", "content": "[[async-returns]]\n\nStarting with version 3.2, `@KafkaListener` (and `@KafkaHandler`) methods can be specified with asynchronous return types, letting the reply be sent asynchronously.\nreturn types include `CompletableFuture<?>`, `Mono<?>` and Kotlin `suspend` functions.\n\n[source, java]\n----\n@KafkaListener(id = \"myListener\", topics = \"myTopic\")\npublic CompletableFuture<String> listen(String data) {\n ...\n CompletableFuture<String> future = new CompletableFuture<>();\n future.complete(\"done\");\n return future;\n}\n----\n\n[source, java]\n----\n@KafkaListener(id = \"myListener\", topics = \"myTopic\")\npublic Mono<Void> listen(String data) {\n ...\n return Mono.empty();\n}\n----\n\nIMPORTANT: The `AckMode` will be automatically set the `MANUAL` and enable out-of-order commits when async return types are detected; instead, the asynchronous completion will ack when the async operation completes.\nWhen the async result is completed with an error, whether the message is recover or not depends on the container error handler.\nIf some exception occurs within the listener method that prevents creation of the async result object, you MUST catch that exception and return an appropriate return object that will cause the message to be ack or recover.\n\nIf a `KafkaListenerErrorHandler` is configured on a listener with an async return type (including Kotlin suspend functions), the error handler is invoked after a failure.\nSee xref:kafka/annotation-error-handling.adoc[Handling Exceptions] for more information about this error handler and its purpose.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/async-returns.adoc", "title": "async-returns", "heading": "async-returns", "heading_level": 1, "file_order": 5, "section_index": 0, "content_hash": "bbc8184e883d5350ed909d7ea719da04c7b02eb141ad92ac226089cc91815bb1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/async-returns.adoc"}}
{"id": "sha256:2662b0042d9874a050437acb56a0bb2f0d6d58caf0c9f78b828ac2bcdd2a69ec", "content": "[[class-level-kafkalistener]]\n\nWhen you use `@KafkaListener` at the class-level, you must specify `@KafkaHandler` at the method level.\nIf no `@KafkaHandler` on any methods of this class or its sub-classes, the framework will reject such a configuration.\nThe `@KafkaHandler` annotation is required for explicit and concise purpose of the method.\nOtherwise it is hard to make a decision about this or other method without extra restrictions.\n\nWhen messages are delivered, the converted message payload type is used to determine which method to call.\nThe following example shows how to do so:\n\n[source, java]\n----\n@KafkaListener(id = \"multi\", topics = \"myTopic\")\nstatic class MultiListenerBean {\n\n @KafkaHandler\n public void listen(String foo) {\n ...\n }\n\n @KafkaHandler\n public void listen(Integer bar) {\n ...\n }\n\n @KafkaHandler(isDefault = true)\n public void listenDefault(Object object) {\n ...\n }\n\n}\n----\n\nStarting with version 2.1.3, you can designate a `@KafkaHandler` method as the default method that is invoked if there is no match on other methods.\nAt most, one method can be so designated.\nWhen using `@KafkaHandler` methods, the payload must have already been converted to the domain object (so the match can be performed).\nUse a custom deserializer, the `JacksonJsonDeserializer`, or the `JacksonJsonMessageConverter` with its `TypePrecedence` set to `TYPE_ID`.\nSee xref:kafka/serdes.adoc[Serialization, Deserialization, and Message Conversion] for more information.\n\nIMPORTANT: Due to some limitations in the way Spring resolves method arguments, a default `@KafkaHandler` cannot receive discrete headers; it must use the `ConsumerRecordMetadata` as discussed in xref:kafka/receiving-messages/listener-annotation.adoc#consumer-record-metadata[Consumer Record Metadata].\n\nFor example:\n\n[source, java]\n----\n@KafkaHandler(isDefault = true)\npublic void listenDefault(Object object, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) {\n ...\n}\n----\n\nThis won't work if the object is a `String`; the `topic` parameter will also get a reference to `object`.\n\nIf you need metadata about the record in a default method, use this:\n\n[source, java]\n----\n@KafkaHandler(isDefault = true)\nvoid listen(Object in, @Header(KafkaHeaders.RECORD_METADATA) ConsumerRecordMetadata meta) {\n String topic = meta.topic();\n ...\n}\n----\n\nAlso, this won't work as well.\nThe `topic` is resolved to the `payload`.\n\n[source, java]\n----\n@KafkaHandler(isDefault = true)\npublic void listenDefault(String payload, @Header(KafkaHeaders.RECEIVED_TOPIC) String topic) {\n // payload.equals(topic) is True.\n ...\n}\n----\n\nIf there are use cases in which discrete custom headers are required in a default method, use this:\n[source, java]\n----\n@KafkaHandler(isDefault = true)\nvoid listenDefault(String payload, @Headers Map<String, Object> headers) {\n Object myValue = headers.get(\"MyCustomHeader\");\n ...\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/class-level-kafkalistener.adoc", "title": "class-level-kafkalistener", "heading": "class-level-kafkalistener", "heading_level": 1, "file_order": 6, "section_index": 0, "content_hash": "2662b0042d9874a050437acb56a0bb2f0d6d58caf0c9f78b828ac2bcdd2a69ec", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/class-level-kafkalistener.adoc"}}
{"id": "sha256:897a246e1f88f7cffb8eeec1d12292656bbb01cf0f65f2d467be1352cedeba8f", "content": "[[container-thread-naming]]\n\nA `TaskExecutor` is used to invoke the consumer and the listener.\nYou can provide a custom executor by setting the `consumerExecutor` property of the container's `ContainerProperties`.\nWhen using pooled executors, be sure that enough threads are available to handle the concurrency across all the containers in which they are used.\nWhen using the `ConcurrentMessageListenerContainer`, a thread from the executor is used for each consumer (`concurrency`).\n\nIf you do not provide a consumer executor, a `SimpleAsyncTaskExecutor` is used for each container.\nThis executor creates threads with names similar to `<beanName>-C-<n>`.\nFor the `ConcurrentMessageListenerContainer`, the `<beanName>` part of the thread name becomes `<beanName>-m`, where `m` represents the consumer instance.\n`n` increments each time the container is started.\nSo, with a bean name of `container`, threads in this container will be named `container-0-C-1`, `container-1-C-1` etc., after the container is started the first time; `container-0-C-2`, `container-1-C-2` etc., after a stop and subsequent start.\n\nStarting with version `3.0.1`, you can now change the name of the thread, regardless of which executor is used.\nSet the `AbstractMessageListenerContainer.changeConsumerThreadName` property to `true` and the `AbstractMessageListenerContainer.threadNameSupplier` will be invoked to obtain the thread name.\nThis is a `Function<MessageListenerContainer, String>`, with the default implementation returning `container.getListenerId()`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/container-thread-naming.adoc", "title": "container-thread-naming", "heading": "container-thread-naming", "heading_level": 1, "file_order": 7, "section_index": 0, "content_hash": "897a246e1f88f7cffb8eeec1d12292656bbb01cf0f65f2d467be1352cedeba8f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/container-thread-naming.adoc"}}
{"id": "sha256:5d01f2e6e5093b568a627db9a3b8ecd845c13f9a53f733fc1ddc98a078dcd415", "content": "[[enforced-rebalance]]\n\nKafka clients now support an option to trigger an https://cwiki.apache.org/confluence/display/KAFKA/KIP-568%3A+Explicit+rebalance+triggering+on+the+Consumer[enforced rebalance].\nStarting with version `3.1.2`, Spring for Apache Kafka provides an option to invoke this API on the Kafka consumer via the message listener container.\nWhen calling this API, it is simply alerting the Kafka consumer to trigger an enforced rebalance; the actual rebalance will only occur as part of the next `poll()` operation.\nIf there is already a rebalance in progress, calling an enforced rebalance is a NO-OP.\nThe caller must wait for the current rebalance to complete before invoking another one.\nSee the javadocs for `enforceRebalance` for more details.\n\nThe following code snippet shows the essence of enforcing a rebalance using the message listener container.\n\n[source, java]\n----\n@KafkaListener(id = \"my.id\", topics = \"my-topic\")\nvoid listen(ConsumerRecord<String, String> in) {\n System.out.println(\"From KafkaListener: \" + in);\n}\n\n@Bean\npublic ApplicationRunner runner(KafkaTemplate<String, Object> template, KafkaListenerEndpointRegistry registry) {\n return args -> {\n final MessageListenerContainer listenerContainer = registry.getListenerContainer(\"my.id\");\n System.out.println(\"Enforcing a rebalance\");\n Thread.sleep(5_000);\n listenerContainer.enforceRebalance();\n Thread.sleep(5_000);\n };\n}\n----\n\nAs the code above shows, the application uses the `KafkaListenerEndpointRegistry` to gain access to the message listener container and then calling the `enforceRebalance` API on it.\nWhen calling the `enforceRebalance` on the listener container, it delegates the call to the underlying Kafka consumer.\nThe Kafka consumer will trigger a rebalance as part of the next `poll()` operation.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/enforced-rebalance.adoc", "title": "enforced-rebalance", "heading": "enforced-rebalance", "heading_level": 1, "file_order": 8, "section_index": 0, "content_hash": "5d01f2e6e5093b568a627db9a3b8ecd845c13f9a53f733fc1ddc98a078dcd415", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/enforced-rebalance.adoc"}}
{"id": "sha256:fd80d6397538086bf37cef01a9c7227b793dd91cca8a716d6343fe878181f36d", "content": "[[filtering-messages]]\n\nIn certain scenarios, such as rebalancing, a message that has already been processed may be redelivered.\nThe framework cannot know whether such a message has been processed or not.\nThat is an application-level function.\nThis is known as the https://www.enterpriseintegrationpatterns.com/patterns/messaging/IdempotentReceiver.html[Idempotent Receiver] pattern and Spring Integration provides an {spring-integration-url}/handler-advice/idempotent-receiver.html[implementation] of it.\n\nThe Spring for Apache Kafka project also provides some assistance by means of the `FilteringMessageListenerAdapter` class, which can wrap your `MessageListener`.\nThis class takes an implementation of `RecordFilterStrategy` in which you implement the `filter` method to signal that a message is a duplicate and should be discarded.\nThis has an additional property called `ackDiscarded`, which indicates whether the adapter should acknowledge the discarded record.\nIt is `false` by default.\n\nWhen you use `@KafkaListener`, set the `RecordFilterStrategy` (and optionally `ackDiscarded`) on the container factory so that the listener is wrapped in the appropriate filtering adapter.\n\nIn addition, a `FilteringBatchMessageListenerAdapter` is provided, for when you use a batch xref:kafka/receiving-messages/message-listeners.adoc[message listener].\n\nIMPORTANT: The `FilteringBatchMessageListenerAdapter` is ignored if your `@KafkaListener` receives a `ConsumerRecords<?, ?>` instead of `List<ConsumerRecord<?, ?>>`, because `ConsumerRecords` is immutable.\n\nStarting with version 2.8.4, you can override the listener container factory's default `RecordFilterStrategy` by using the `filter` property on the listener annotations.\n\n[source, java]\n----\n@KafkaListener(id = \"filtered\", topics = \"topic\", filter = \"differentFilter\")\npublic void listen(Thing thing) {\n ...\n}\n----\n\nStarting with version 3.3, Ignoring empty batches that result from filtering by `RecordFilterStrategy` is supported.\nWhen implementing `RecordFilterStrategy`, it can be configured through `ignoreEmptyBatch()`.\nThe default setting is `false`, indicating `KafkaListener` will be invoked even if all ``ConsumerRecord``s are filtered out.\n\nIf `true` is returned, the `KafkaListener` [underline]#will not be invoked# when all `ConsumerRecord` are filtered out.\nHowever, commit to broker, will still be executed.\n\nIf `false` is returned, the `KafkaListener` [underline]#will be invoked# when all `ConsumerRecord` are filtered out.\n\nHere are some examples.\n\n[source,java]\n----\npublic class IgnoreEmptyBatchRecordFilterStrategy implements RecordFilterStrategy {\n ...\n @Override\n public List<ConsumerRecord<String, String>> filterBatch(\n List<ConsumerRecord<String, String>> consumerRecords) {\n return List.of();\n }\n\n @Override\n public boolean ignoreEmptyBatch() {\n return true;\n }\n}\n\n@KafkaListener(id = \"filtered\", topics = \"topic\", filter = \"ignoreEmptyBatchRecordFilterStrategy\")\npublic void listen(List<Thing> things) {\n ...\n}\n----\nIn this case, `IgnoreEmptyBatchRecordFilterStrategy` always returns empty list and return `true` as result of `ignoreEmptyBatch()`.\nThus `KafkaListener#listen(...)` never will be invoked at all.\n\n[source,java]\n----\npublic class NotIgnoreEmptyBatchRecordFilterStrategy implements RecordFilterStrategy {\n ...\n @Override\n public List<ConsumerRecord<String, String>> filterBatch(\n List<ConsumerRecord<String, String>> consumerRecords) {\n return List.of();\n }\n\n @Override\n public boolean ignoreEmptyBatch() {\n return false;\n }\n}\n\n@KafkaListener(id = \"filtered\", topics = \"topic\", filter = \"notIgnoreEmptyBatchRecordFilterStrategy\")\npublic void listen(List<Thing> things) {\n ...\n}\n----\nHowever, in this case, `NotIgnoreEmptyBatchRecordFilterStrategy` always returns empty list and return `false` as result of `ignoreEmptyBatch()`.\nThus `KafkaListener#listen(...)` always will be invoked.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/filtering.adoc", "title": "filtering", "heading": "filtering", "heading_level": 1, "file_order": 9, "section_index": 0, "content_hash": "fd80d6397538086bf37cef01a9c7227b793dd91cca8a716d6343fe878181f36d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/filtering.adoc"}}
{"id": "sha256:5c59e84ed39a2f2e7d4092ae9da24295d078e10c330117cf8def1072edc7a368", "content": "[[kafkalistener-attrs]]\n\nStarting with version 2.7.2, you can now programmatically modify annotation attributes before the container is created.\nTo do so, add one or more `KafkaListenerAnnotationBeanPostProcessor.AnnotationEnhancer` to the application context.\n`AnnotationEnhancer` is a `BiFunction<Map<String, Object>, AnnotatedElement, Map<String, Object>` and must return a map of attributes.\nThe attribute values can contain SpEL and/or property placeholders; the enhancer is called before any resolution is performed.\nIf more than one enhancer is present, and they implement `Ordered`, they will be invoked in order.\n\nIMPORTANT: `AnnotationEnhancer` bean definitions must be declared `static` because they are required very early in the application context's lifecycle.\n\nAn example follows:\n\n[source, java]\n----\n@Bean\npublic static AnnotationEnhancer groupIdEnhancer() {\n return (attrs, element) -> {\n attrs.put(\"groupId\", attrs.get(\"id\") + \".\" + (element instanceof Class\n ? ((Class<?>) element).getSimpleName()\n : ((Method) element).getDeclaringClass().getSimpleName()\n + \".\" + ((Method) element).getName()));\n return attrs;\n };\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/kafkalistener-attrs.adoc", "title": "kafkalistener-attrs", "heading": "kafkalistener-attrs", "heading_level": 1, "file_order": 10, "section_index": 0, "content_hash": "5c59e84ed39a2f2e7d4092ae9da24295d078e10c330117cf8def1072edc7a368", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/kafkalistener-attrs.adoc"}}
{"id": "sha256:0495665ed9308557e334bd59e9aead32a8f4f54737b177fad99c716b95a4b016", "content": "[[kafkalistener-lifecycle]]\n\nThe listener containers created for `@KafkaListener` annotations are not beans in the application context.\nInstead, they are registered with an infrastructure bean of type `KafkaListenerEndpointRegistry`.\nThis bean is automatically declared by the framework and manages the containers' lifecycles; it will auto-start any containers that have `autoStartup` set to `true`.\nAll containers created by all container factories must be in the same `phase`.\nSee xref:kafka/receiving-messages/message-listener-container.adoc#container-auto-startup[Listener Container Auto Startup] for more information.\nYou can manage the lifecycle programmatically by using the registry.\nStarting or stopping the registry will start or stop all the registered containers.\nAlternatively, you can get a reference to an individual container by using its `id` attribute.\nYou can set `autoStartup` on the annotation, which overrides the default setting configured into the container factory.\nYou can get a reference to the bean from the application context, such as auto-wiring, to manage its registered containers.\nThe following examples show how to do so:\n\n[source, java]\n----\n@KafkaListener(id = \"myContainer\", topics = \"myTopic\", autoStartup = \"false\")\npublic void listen(...) { ... }\n\n----\n\n[source, java]\n----\n@Autowired\nprivate KafkaListenerEndpointRegistry registry;\n\n...\n\n this.registry.getListenerContainer(\"myContainer\").start();\n\n...\n----\n\nThe registry only maintains the life cycle of containers it manages; containers declared as beans are not managed by the registry and can be obtained from the application context.\nA collection of managed containers can be obtained by calling the registry's `getListenerContainers()` method.\nVersion 2.2.5 added a convenience method `getAllListenerContainers()`, which returns a collection of all containers, including those managed by the registry and those declared as beans.\nThe collection returned will include any prototype beans that have been initialized, but it will not initialize any lazy bean declarations.\n\nIMPORTANT: Endpoints registered after the application context has been refreshed will start immediately, regardless of their `autoStartup` property, to comply with the `SmartLifecycle` contract, where `autoStartup` is only considered during application context initialization.\nAn example of late registration is a bean with a `@KafkaListener` in prototype scope where an instance is created after the context is initialized.\nStarting with version 2.8.7, you can set the registry's `alwaysStartAfterRefresh` property to `false` and then the container's `autoStartup` property will define whether or not the container is started.\n\n[[retrieving-message-listener-containers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/kafkalistener-lifecycle.adoc", "title": "kafkalistener-lifecycle", "heading": "kafkalistener-lifecycle", "heading_level": 1, "file_order": 11, "section_index": 0, "content_hash": "0495665ed9308557e334bd59e9aead32a8f4f54737b177fad99c716b95a4b016", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/kafkalistener-lifecycle.adoc"}}
{"id": "sha256:7cef906155f9beeb10a9b336b2afb0992a463f87606d3674b4cd5792477e43b3", "content": "The `KafkaListenerEndpointRegistry` provides methods for retrieving `MessageListenerContainer` instances to accommodate a range of management scenarios:\n\n**All Containers**: For operations that cover all listener containers, use `getListenerContainers()` to retrieve a comprehensive collection.\n\n[source, java]\n----\nCollection<MessageListenerContainer> allContainers = registry.getListenerContainers();\n----\n\n**Specific Container by ID**: To manage an individual container, `getListenerContainer(String id)` enables retrieval by its id.\n\n[source, java]\n----\nMessageListenerContainer specificContainer = registry.getListenerContainer(\"myContainerId\");\n----\n\n**Dynamic Container Filtering**: Introduced in version 3.2, two overloaded `getListenerContainersMatching` methods enable refined selection of containers.\nOne method takes a `Predicate<String>` for ID-based filtering as a parameter, while the other takes a `BiPredicate<String, MessageListenerContainer>`\nfor more advanced criteria that may include container properties or state as a parameter.\n\n[source, java]\n----\nCollection<MessageListenerContainer> filteredContainers =\n registry.getListenerContainersMatching(id -> id.startsWith(\"productListener-retry-\"));\n\nCollection<MessageListenerContainer> regexFilteredContainers =\n registry.getListenerContainersMatching(myPattern::matches);\n\nCollection<MessageListenerContainer> setFilteredContainers =\n registry.getListenerContainersMatching(myIdSet::contains);\n\nCollection<MessageListenerContainer> advancedFilteredContainers =\n registry.getListenerContainersMatching(\n (id, container) -> id.startsWith(\"specificPrefix-\") && container.isRunning()\n );\n----\n\nUtilize these methods to efficiently manage and query `MessageListenerContainer` instances within your application.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/kafkalistener-lifecycle.adoc", "title": "kafkalistener-lifecycle", "heading": "Retrieving MessageListenerContainers from KafkaListenerEndpointRegistry", "heading_level": 2, "file_order": 11, "section_index": 1, "content_hash": "7cef906155f9beeb10a9b336b2afb0992a463f87606d3674b4cd5792477e43b3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/kafkalistener-lifecycle.adoc"}}
{"id": "sha256:d25dc76a0d08fce18149a83dec8a7497f54faf4ef6f57aef2d3173625fa1fa53", "content": "[[kafka-listener-annotation]]\n\nThe `@KafkaListener` annotation is used to designate a bean method as a listener for a listener container.\nThe bean is wrapped in a `MessagingMessageListenerAdapter` configured with various features, such as converters to convert the data, if necessary, to match the method parameters.\n\nYou can configure most attributes on the annotation with SpEL by using `#{...}` or property placeholders (`${...}`).\nSee the javadoc:org.springframework.kafka.annotation.KafkaListener[Javadoc] for more information.\n\n[[record-listener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc", "title": "listener-annotation", "heading": "listener-annotation", "heading_level": 1, "file_order": 12, "section_index": 0, "content_hash": "d25dc76a0d08fce18149a83dec8a7497f54faf4ef6f57aef2d3173625fa1fa53", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc"}}
{"id": "sha256:914d4692f882ce05543c273196712777e6b2f221780746e53d716f628826009e", "content": "The `@KafkaListener` annotation provides a mechanism for simple POJO listeners.\nThe following example shows how to use it:\n\n[source, java]\n----\npublic class Listener {\n\n @KafkaListener(id = \"foo\", topics = \"myTopic\", clientIdPrefix = \"myClientId\")\n public void listen(String data) {\n ...\n }\n\n}\n----\n\nThis mechanism requires an `@EnableKafka` annotation on one of your `@Configuration` classes and a listener container factory, which is used to configure the underlying `ConcurrentMessageListenerContainer`.\nBy default, a bean with name `kafkaListenerContainerFactory` is expected.\nThe following example shows how to use `ConcurrentMessageListenerContainer`:\n\n[source, java]\n----\n@Configuration\n@EnableKafka\npublic class KafkaConfig {\n\n @Bean\n KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<Integer, String>>\n kafkaListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory<Integer, String> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n factory.setConsumerFactory(consumerFactory());\n factory.setConcurrency(3);\n factory.getContainerProperties().setPollTimeout(3000);\n return factory;\n }\n\n @Bean\n public ConsumerFactory<Integer, String> consumerFactory() {\n return new DefaultKafkaConsumerFactory<>(consumerConfigs());\n }\n\n @Bean\n public Map<String, Object> consumerConfigs() {\n Map<String, Object> props = new HashMap<>();\n props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n ...\n return props;\n }\n}\n----\n\nNotice that, to set container properties, you must use the `getContainerProperties()` method on the factory.\nIt is used as a template for the actual properties injected into the container.\n\nStarting with version 2.1.1, you can now set the `client.id` property for consumers created by the annotation.\nThe `clientIdPrefix` is suffixed with `-n`, where `n` is an integer representing the container number when using concurrency.\n\nStarting with version 2.2, you can now override the container factory's `concurrency` and `autoStartup` properties by using properties on the annotation itself.\nThe properties can be simple values, property placeholders, or SpEL expressions.\nThe following example shows how to do so:\n\n[source, java]\n----\n@KafkaListener(id = \"myListener\", topics = \"myTopic\",\n autoStartup = \"${listen.auto.start:true}\", concurrency = \"${listen.concurrency:3}\")\npublic void listen(String data) {\n ...\n}\n----\n\n[[topic-partition-assignment]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc", "title": "listener-annotation", "heading": "Record Listeners", "heading_level": 2, "file_order": 12, "section_index": 1, "content_hash": "914d4692f882ce05543c273196712777e6b2f221780746e53d716f628826009e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc"}}
{"id": "sha256:f1eedc524dd3c08ad264792b173eb51053d3795dc71fd6c1dd39ad2fb3642c26", "content": "You can configure the topic for `@KafkaListener` in three ways.\nYou must configure the topic in one of these ways.\n[source, java]\n----\n@KafkaListener(id = \"myListener\", topics = \"myTopic\")\npublic void listen(String data) {\n ...\n}\n\n@KafkaListener(id = \"myListener\", topicPattern = \"my.*\")\npublic void listen(String data) {\n ...\n}\n\n@KafkaListener(id = \"myListener\", topicPartitions = { @TopicPartition(topic = \"myTopic\", partitions = { \"0\", \"1\" })})\npublic void listen(String data) {\n ...\n}\n----\n\nYou can simply configure the topic directly by name.\nIn this case, you can also configure the multiple topics like `topics = {\"myTopic1\", myTopic2\"}`.\n\nYou can also configure topics using topicPattern, which enables topic subscription based on a regular expression.\n\nWhen you configure topics using either of these ways (topic or topic pattern), Kafka automatically assigns partitions according to the consumer group.\nAlternatively, you can configure POJO listeners with explicit topics and partitions (and, optionally, their initial offsets).\nThe following example shows how to do so:\n\n[source, java]\n----\n@KafkaListener(id = \"thing2\", topicPartitions =\n { @TopicPartition(topic = \"topic1\", partitions = { \"0\", \"1\" }),\n @TopicPartition(topic = \"topic2\", partitions = \"0\",\n partitionOffsets = @PartitionOffset(partition = \"1\", initialOffset = \"100\"))\n })\npublic void listen(ConsumerRecord<?, ?> record) {\n ...\n}\n----\n\nYou can specify each partition in the `partitions` or `partitionOffsets` attribute but not both.\n\nAs with most annotation properties, you can use SpEL expressions; for an example of how to generate a large list of partitions, see xref:tips.adoc[Manually Assigning All Partitions].\n\nStarting with version 2.5.5, you can apply an initial offset to all assigned partitions:\n\n[source, java]\n----\n@KafkaListener(id = \"thing3\", topicPartitions =\n { @TopicPartition(topic = \"topic1\", partitions = { \"0\", \"1\" },\n partitionOffsets = @PartitionOffset(partition = \"*\", initialOffset = \"0\"))\n })\npublic void listen(ConsumerRecord<?, ?> record) {\n ...\n}\n----\n\nThe `*` wildcard represents all partitions in the `partitions` attribute.\nThere must only be one `@PartitionOffset` with the wildcard in each `@TopicPartition`.\n\nIn addition, when the listener implements `ConsumerSeekAware`, `onPartitionsAssigned` is now called, even when using manual assignment.\nThis allows, for example, any arbitrary seek operations at that time.\n\nStarting with version 2.6.4, you can specify a comma-delimited list of partitions, or partition ranges:\n\n[source, java]\n----\n@KafkaListener(id = \"pp\", autoStartup = \"false\",\n topicPartitions = @TopicPartition(topic = \"topic1\",\n partitions = \"0-5, 7, 10-15\"))\npublic void process(String in) {\n ...\n}\n----\n\nThe range is inclusive; the example above will assign partitions `0, 1, 2, 3, 4, 5, 7, 10, 11, 12, 13, 14, 15`.\n\nThe same technique can be used when specifying initial offsets:\n\n[source, java]\n----\n@KafkaListener(id = \"thing3\", topicPartitions =\n { @TopicPartition(topic = \"topic1\",\n partitionOffsets = @PartitionOffset(partition = \"0-5\", initialOffset = \"0\"))\n })\npublic void listen(ConsumerRecord<?, ?> record) {\n ...\n}\n----\n\nThe initial offset will be applied to all 6 partitions.\n\nSince 3.2, `@PartitionOffset` support `SeekPosition.END`, `SeekPosition.BEGINNING`, `SeekPosition.TIMESTAMP`, `seekPosition` match `SeekPosition` enum name:\n\n[source, java]\n----\n@KafkaListener(id = \"seekPositionTime\", topicPartitions = {\n @TopicPartition(topic = TOPIC_SEEK_POSITION, partitionOffsets = {\n @PartitionOffset(partition = \"0\", initialOffset = \"723916800000\", seekPosition = \"TIMESTAMP\"),\n @PartitionOffset(partition = \"1\", initialOffset = \"0\", seekPosition = \"BEGINNING\"),\n @PartitionOffset(partition = \"2\", initialOffset = \"0\", seekPosition = \"END\")\n })\n})\npublic void listen(ConsumerRecord<?, ?> record) {\n ...\n}\n----\n\nIf seekPosition set `END` or `BEGINNING` will ignore `initialOffset` and `relativeToCurrent`.\nIf seekPosition set `TIMESTAMP`, `initialOffset` means timestamp.\n\n[[manual-acknowledgment]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc", "title": "listener-annotation", "heading": "Topic Partition Assignment", "heading_level": 2, "file_order": 12, "section_index": 2, "content_hash": "f1eedc524dd3c08ad264792b173eb51053d3795dc71fd6c1dd39ad2fb3642c26", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc"}}
{"id": "sha256:f844f46bf531b0452e9c6b6356ef3cd34b35e3e5ca65e000920bac7e34570d72", "content": "When using manual `AckMode`, you can also provide the listener with the `Acknowledgment`.\nTo activate the manual `AckMode`, you need to set the ack-mode in `ContainerProperties` to the appropriate manual mode.\nThe following example also shows how to use a different container factory.\nThis custom container factory must set the `AckMode` to a manual type by calling the `getContainerProperties()` and then calling `setAckMode` on it.\nOtherwise, the `Acknowledgment` object will be null.\n\n[source, java]\n----\n@KafkaListener(id = \"cat\", topics = \"myTopic\",\n containerFactory = \"kafkaManualAckListenerContainerFactory\")\npublic void listen(String data, Acknowledgment ack) {\n ...\n ack.acknowledge();\n}\n----\n\nStarting with version 4.1, you can override the container factory's default `AckMode` directly on the `@KafkaListener` annotation using the `ackMode` attribute:\n\n[source, java]\n----\n@KafkaListener(id = \"manual\", topics = \"myTopic\", ackMode = \"MANUAL\")\npublic void listen(String data, Acknowledgment ack) {\n ...\n ack.acknowledge();\n}\n----\n\nThe `ackMode` attribute accepts string values corresponding to `ContainerProperties.AckMode` enum values.\nThis eliminates the need to create separate container factories solely for different acknowledgment modes.\nThe attribute can also be configured as SpEL expression (`#{...}`) or property placeholders (`${...}`).\n\n[[consumer-record-metadata]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc", "title": "listener-annotation", "heading": "Manual Acknowledgment", "heading_level": 2, "file_order": 12, "section_index": 3, "content_hash": "f844f46bf531b0452e9c6b6356ef3cd34b35e3e5ca65e000920bac7e34570d72", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc"}}
{"id": "sha256:766c135fe4d0f078b279d066737e1d166571ac574c95c91e7e4c00a147a8a737", "content": "Finally, metadata about the record is available from message headers.\nYou can use the following header names to retrieve the headers of the message:\n\n* `KafkaHeaders.OFFSET`\n* `KafkaHeaders.RECEIVED_KEY`\n* `KafkaHeaders.RECEIVED_TOPIC`\n* `KafkaHeaders.RECEIVED_PARTITION`\n* `KafkaHeaders.RECEIVED_TIMESTAMP`\n* `KafkaHeaders.TIMESTAMP_TYPE`\n\nStarting with version 2.5 the `RECEIVED_KEY` is not present if the incoming record has a `null` key; previously the header was populated with a `null` value.\nThis change is to make the framework consistent with `spring-messaging` conventions where `null` valued headers are not present.\n\nThe following example shows how to use the headers:\n\n[source, java]\n----\n@KafkaListener(id = \"qux\", topics = \"myTopic1\")\npublic void listen(@Payload String foo,\n @Header(name = KafkaHeaders.RECEIVED_KEY, required = false) Integer key,\n @Header(KafkaHeaders.RECEIVED_PARTITION) int partition,\n @Header(KafkaHeaders.RECEIVED_TOPIC) String topic,\n @Header(KafkaHeaders.RECEIVED_TIMESTAMP) long ts\n ) {\n ...\n}\n----\n\nIMPORTANT: Parameter annotations (`@Payload`, `@Header`) must be specified on the concrete implementation of the listener method; they will not be detected if they are defined on an interface.\n\nStarting with version 2.5, instead of using discrete headers, you can receive record metadata in a `ConsumerRecordMetadata` parameter.\n\n[source, java]\n----\n@KafkaListener(...)\npublic void listen(String str, ConsumerRecordMetadata meta) {\n ...\n}\n----\n\nThis contains all the data from the `ConsumerRecord` except the key and value.\n\n[[batch-listeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc", "title": "listener-annotation", "heading": "Consumer Record Metadata", "heading_level": 2, "file_order": 12, "section_index": 4, "content_hash": "766c135fe4d0f078b279d066737e1d166571ac574c95c91e7e4c00a147a8a737", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc"}}
{"id": "sha256:54d04925be63db8295f69a492819a00400d5edfc300cb1cbc53e3352cc74d309", "content": "Starting with version 1.1, you can configure `@KafkaListener` methods to receive the entire batch of consumer records received from the consumer poll.\n\nIMPORTANT: xref:retrytopic.adoc[Non-Blocking Retries] are not supported with batch listeners.\n\nTo configure the listener container factory to create batch listeners, you can set the `batchListener` property.\nThe following example shows how to do so:\n\n[source, java]\n----\n@Bean\npublic KafkaListenerContainerFactory<?> batchFactory() {\n ConcurrentKafkaListenerContainerFactory<Integer, String> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n factory.setConsumerFactory(consumerFactory());\n factory.setBatchListener(true);\n return factory;\n}\n----\n\nNOTE: Starting with version 2.8, you can override the factory's `batchListener` property using the `batch` property on the `@KafkaListener` annotation.\nThis, together with the changes to xref:kafka/annotation-error-handling.adoc#error-handlers[Container Error Handlers] allows the same factory to be used for both record and batch listeners.\n\nNOTE: Starting with version 2.9.6, the container factory has separate setters for the `recordMessageConverter` and `batchMessageConverter` properties.\nPreviously, there was only one property `messageConverter` which applied to both record and batch listeners.\n\nThe following example shows how to receive a list of payloads:\n\n[source, java]\n----\n@KafkaListener(id = \"list\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void listen(List<String> list) {\n ...\n}\n----\n\nThe topic, partition, offset, and so on are available in headers that parallel the payloads.\nThe following example shows how to use the headers:\n\n[source, java]\n----\n@KafkaListener(id = \"list\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void listen(List<String> list,\n @Header(KafkaHeaders.RECEIVED_KEY) List<Integer> keys,\n @Header(KafkaHeaders.RECEIVED_PARTITION) List<Integer> partitions,\n @Header(KafkaHeaders.RECEIVED_TOPIC) List<String> topics,\n @Header(KafkaHeaders.OFFSET) List<Long> offsets) {\n ...\n}\n----\n\nAlternatively, you can receive a `List` of `Message<?>` objects with each offset and other details in each message, but it must be the only parameter (aside from optional `Acknowledgment`, when using manual commits, and/or `Consumer<?, ?>` parameters) defined on the method.\nThe following example shows how to do so:\n\n[source, java]\n----\n@KafkaListener(id = \"listMsg\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void listen1(List<Message<?>> list) {\n ...\n}\n\n@KafkaListener(id = \"listMsgAck\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void listen2(List<Message<?>> list, Acknowledgment ack) {\n ...\n}\n\n@KafkaListener(id = \"listMsgAckConsumer\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void listen3(List<Message<?>> list, Acknowledgment ack, Consumer<?, ?> consumer) {\n ...\n}\n----\n\nNo conversion is performed on the payloads in this case.\n\nIf the `BatchMessagingMessageConverter` is configured with a `RecordMessageConverter`, you can also add a generic type to the `Message` parameter and the payloads are converted.\nSee xref:kafka/serdes.adoc#payload-conversion-with-batch[Payload Conversion with Batch Listeners] for more information.\n\nYou can also receive a list of `ConsumerRecord<?, ?>` objects, but it must be the only parameter (aside from optional `Acknowledgment`, when using manual commits and `Consumer<?, ?>` parameters) defined on the method.\nThe following example shows how to do so:\n\n[source, java]\n----\n@KafkaListener(id = \"listCRs\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void listen(List<ConsumerRecord<Integer, String>> list) {\n ...\n}\n\n@KafkaListener(id = \"listCRsAck\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void listen(List<ConsumerRecord<Integer, String>> list, Acknowledgment ack) {\n ...\n}\n----\n\nStarting with version 2.2, the listener can receive the complete `ConsumerRecords<?, ?>` object returned by the `poll()` method, letting the listener access additional methods, such as `partitions()` (which returns the `TopicPartition` instances in the list) and `records(TopicPartition)` (which gets selective records).\nAgain, this must be the only parameter (aside from optional `Acknowledgment`, when using manual commits or `Consumer<?, ?>` parameters) on the method.\nThe following example shows how to do so:\n\n[source, java]\n----\n@KafkaListener(id = \"pollResults\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void pollResults(ConsumerRecords<?, ?> records) {\n ...\n}\n----\n\nIMPORTANT: If the container factory has a `RecordFilterStrategy` configured, it is ignored for `ConsumerRecords<?, ?>` listeners, with a `WARN` log message emitted.\nRecords can only be filtered with a batch listener if the `List<?>` form of listener is used.\nBy default, records are filtered one-at-a-time; starting with version 2.8, you can override `filterBatch` to filter the entire batch in one call.\n\n[[annotation-properties]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc", "title": "listener-annotation", "heading": "Batch Listeners", "heading_level": 2, "file_order": 12, "section_index": 5, "content_hash": "54d04925be63db8295f69a492819a00400d5edfc300cb1cbc53e3352cc74d309", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc"}}
{"id": "sha256:a06872863938beafb110d3d9ee1a25dcaf09a7473f4a2ee7b76476594ffb85e9", "content": "Starting with version 2.0, the `id` property (if present) is used as the Kafka consumer `group.id` property, overriding the configured property in the consumer factory, if present.\nYou can also set `groupId` explicitly or set `idIsGroup` to false to restore the previous behavior of using the consumer factory `group.id`.\n\nYou can use property placeholders or SpEL expressions within most annotation properties, as the following example shows:\n\n[source, java]\n----\n@KafkaListener(topics = \"${some.property}\")\n\n@KafkaListener(topics = \"#{someBean.someProperty}\",\n groupId = \"#{someBean.someProperty}.group\")\n----\n\nStarting with version 2.1.2, the SpEL expressions support a special token: `__listener`.\nIt is a pseudo bean name that represents the current bean instance within which this annotation exists.\n\nConsider the following example:\n\n[source, java]\n----\n@Bean\npublic Listener listener1() {\n return new Listener(\"topic1\");\n}\n\n@Bean\npublic Listener listener2() {\n return new Listener(\"topic2\");\n}\n----\n\nGiven the beans in the previous example, we can then use the following:\n\n[source, java]\n----\npublic class Listener {\n\n private final String topic;\n\n public Listener(String topic) {\n this.topic = topic;\n }\n\n @KafkaListener(topics = \"#{__listener.topic}\",\n groupId = \"#{__listener.topic}.group\")\n public void listen(...) {\n ...\n }\n\n public String getTopic() {\n return this.topic;\n }\n\n}\n----\n\nIf, in the unlikely event that you have an actual bean called `__listener`, you can change the expression token by using the `beanRef` attribute.\nThe following example shows how to do so:\n\n[source, java]\n----\n@KafkaListener(beanRef = \"__x\", topics = \"#{__x.topic}\", groupId = \"#{__x.topic}.group\")\n----\n\nStarting with version 2.2.4, you can specify Kafka consumer properties directly on the annotation, these will override any properties with the same name configured in the consumer factory. You **cannot** specify the `group.id` and `client.id` properties this way; they will be ignored; use the `groupId` and `clientIdPrefix` annotation properties for those.\n\nThe properties are specified as individual strings with the normal Java `Properties` file format: `foo:bar`, `foo=bar`, or `foo bar`, as the following example shows:\n\n[source, java]\n----\n@KafkaListener(topics = \"myTopic\", groupId = \"group\", properties = {\n \"max.poll.interval.ms:60000\",\n ConsumerConfig.MAX_POLL_RECORDS_CONFIG + \"=100\"\n})\n----\n\nThe following is an example of the corresponding listeners for the example in xref:kafka/sending-messages.adoc#routing-template[Using `RoutingKafkaTemplate`].\n\n[source, java]\n----\n@KafkaListener(id = \"one\", topics = \"one\")\npublic void listen1(String in) {\n System.out.println(\"1: \" + in);\n}\n\n@KafkaListener(id = \"two\", topics = \"two\",\n properties = \"value.deserializer:org.apache.kafka.common.serialization.ByteArrayDeserializer\")\npublic void listen2(byte[] in) {\n System.out.println(\"2: \" + new String(in));\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc", "title": "listener-annotation", "heading": "Annotation Properties", "heading_level": 2, "file_order": 12, "section_index": 6, "content_hash": "a06872863938beafb110d3d9ee1a25dcaf09a7473f4a2ee7b76476594ffb85e9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/listener-annotation.adoc"}}
{"id": "sha256:97026578c3c0137609af422fb47e8424eadea4b9d99959790106261e765bec17", "content": "[[listener-group-id]]\n\nWhen running the same listener code in multiple containers, it may be useful to be able to determine which container (identified by its `group.id` consumer property) that a record came from.\n\nYou can call `KafkaUtils.getConsumerGroupId()` on the listener thread to do this.\nAlternatively, you can access the group id in a method parameter.\n\n[source, java]\n----\n@KafkaListener(id = \"id\", topics = \"someTopic\")\npublic void listener(@Payload String payload, @Header(KafkaHeaders.GROUP_ID) String groupId) {\n ...\n}\n----\n\nIMPORTANT: This is available in record listeners and batch listeners that receive a `List<?>` of records.\nIt is **not** available in a batch listener that receives a `ConsumerRecords<?, ?>` argument.\nUse the `KafkaUtils` mechanism in that case.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/listener-group-id.adoc", "title": "listener-group-id", "heading": "listener-group-id", "heading_level": 1, "file_order": 13, "section_index": 0, "content_hash": "97026578c3c0137609af422fb47e8424eadea4b9d99959790106261e765bec17", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/listener-group-id.adoc"}}
{"id": "sha256:d84a0f108cfc932cff3f4c6e4c6bb7c3fbdd158c9fb3083851674e797b653f66", "content": "[[kafka-listener-meta]]\n\nStarting with version 2.2, you can now use `@KafkaListener` as a meta annotation.\nThe following example shows how to do so:\n\n[source, java]\n----\n@Target(ElementType.METHOD)\n@Retention(RetentionPolicy.RUNTIME)\n@KafkaListener\npublic @interface MyThreeConsumersListener {\n\n @AliasFor(annotation = KafkaListener.class, attribute = \"id\")\n String id();\n\n @AliasFor(annotation = KafkaListener.class, attribute = \"topics\")\n String[] topics();\n\n @AliasFor(annotation = KafkaListener.class, attribute = \"concurrency\")\n String concurrency() default \"3\";\n\n}\n----\n\nYou must alias at least one of `topics`, `topicPattern`, or `topicPartitions` (and, usually, `id` or `groupId` unless you have specified a `group.id` in the consumer factory configuration).\nThe following example shows how to do so:\n\n[source, java]\n----\n@MyThreeConsumersListener(id = \"my.group\", topics = \"my.topic\")\npublic void listen1(String in) {\n ...\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/listener-meta.adoc", "title": "listener-meta", "heading": "listener-meta", "heading_level": 1, "file_order": 14, "section_index": 0, "content_hash": "d84a0f108cfc932cff3f4c6e4c6bb7c3fbdd158c9fb3083851674e797b653f66", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/listener-meta.adoc"}}
{"id": "sha256:4137ff6c726e1958c30eb31af0e8dce0892ec98060c2848b1c939b2679ac9842", "content": "[[message-listener-container]]\n\nTwo `MessageListenerContainer` implementations are provided:\n\n* `KafkaMessageListenerContainer`\n* `ConcurrentMessageListenerContainer`\n\nThe `KafkaMessageListenerContainer` receives all messages from all topics or partitions on a single thread.\nThe `ConcurrentMessageListenerContainer` delegates to one or more `KafkaMessageListenerContainer` instances to provide multi-threaded consumption.\n\nStarting with version 2.2.7, you can add a `RecordInterceptor` to the listener container; it will be invoked before calling the listener allowing inspection or modification of the record.\nIf the interceptor returns null, the listener is not called.\nStarting with version 2.7, it has additional methods which are called after the listener exits (normally, or by throwing an exception).\nAlso, starting with version 2.7, there is now a `BatchInterceptor`, providing similar functionality for xref:kafka/receiving-messages/listener-annotation.adoc#batch-listeners[Batch Listeners].\nIn addition, the `ConsumerAwareRecordInterceptor` (and `BatchInterceptor`) provide access to the `Consumer<?, ?>`.\nThis might be used, for example, to access the consumer metrics in the interceptor.\n\nIMPORTANT: You should not execute any methods that affect the consumer's positions and/or committed offsets in these interceptors; the container needs to manage such information.\n\nIMPORTANT: If the interceptor mutates the record (by creating a new one), the `topic`, `partition`, and `offset` must remain the same to avoid unexpected side effects such as record loss.\n\nThe `CompositeRecordInterceptor` and `CompositeBatchInterceptor` can be used to invoke multiple interceptors.\n\nStarting with version 4.0, `AbstractKafkaListenerContainerFactory` and `AbstractMessageListenerContainer` exposes `getRecordInterceptor()` and `getBatchInterceptor()` as public methods.\nIf the returned interceptor is an instance of `CompositeRecordInterceptor` or `CompositeBatchInterceptor`, additional `RecordInterceptor` or `BatchInterceptor` instances can be added to it even after the container instance extending `AbstractMessageListenerContainer` has been created and a `RecordInterceptor` or `BatchInterceptor` has already been configured.\nThe following example shows how to do so:\n\n[source, java]\n----\npublic void configureRecordInterceptor(AbstractKafkaListenerContainerFactory<Integer, String> containerFactory) {\n CompositeRecordInterceptor compositeInterceptor;\n\n RecordInterceptor<Integer, String> previousInterceptor = containerFactory.getRecordInterceptor();\n if (previousInterceptor instanceof CompositeRecordInterceptor interceptor) {\n compositeInterceptor = interceptor;\n } else {\n compositeInterceptor = new CompositeRecordInterceptor<>();\n containerFactory.setRecordInterceptor(compositeInterceptor);\n if (previousInterceptor != null) {\n compositeInterceptor.addRecordInterceptor(previousInterceptor);\n }\n }\n\n RecordInterceptor<Integer, String> recordInterceptor1 = new RecordInterceptor() {...};\n RecordInterceptor<Integer, String> recordInterceptor2 = new RecordInterceptor() {...};\n\n compositeInterceptor.addRecordInterceptor(recordInterceptor1);\n compositeInterceptor.addRecordInterceptor(recordInterceptor2);\n}\n----\n\nBy default, starting with version 2.8, when using transactions, the interceptor is invoked before the transaction has started.\nYou can set the listener container's `interceptBeforeTx` property to `false` to invoke the interceptor after the transaction has started instead.\nStarting with version 2.9, this will apply to any transaction manager, not just ``KafkaAwareTransactionManager``s.\nThis allows, for example, the interceptor to participate in a JDBC transaction started by the container.\n\nStarting with versions 2.3.8, 2.4.6, the `ConcurrentMessageListenerContainer` now supports {kafka-url}/documentation/#static_membership[Static Membership] when the concurrency is greater than one.\nThe `group.instance.id` is suffixed with `-n` with `n` starting at `1`.\nThis, together with an increased `session.timeout.ms`, can be used to reduce rebalance events, for example, when application instances are restarted.\n\n[[kafka-container]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc", "title": "message-listener-container", "heading": "message-listener-container", "heading_level": 1, "file_order": 15, "section_index": 0, "content_hash": "4137ff6c726e1958c30eb31af0e8dce0892ec98060c2848b1c939b2679ac9842", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc"}}
{"id": "sha256:c26e22992b69b056ec1997f081ca0a3ab9fc5845e50a5c3271f98f2ea0934bb2", "content": "The following constructor is available:\n\n[source, java]\n----\npublic KafkaMessageListenerContainer(ConsumerFactory<K, V> consumerFactory,\n ContainerProperties containerProperties)\n----\n\nIt receives a `ConsumerFactory` and information about topics and partitions, as well as other configuration, in a `ContainerProperties`\nobject.\n`ContainerProperties` has the following constructors:\n\n[source, java]\n----\npublic ContainerProperties(TopicPartitionOffset... topicPartitions)\n\npublic ContainerProperties(String... topics)\n\npublic ContainerProperties(Pattern topicPattern)\n----\n\nThe first constructor takes an array of `TopicPartitionOffset` arguments to explicitly instruct the container about which partitions to use (using the consumer `assign()` method) and with an optional initial offset.\nA positive value is an absolute offset by default.\nA negative value is relative to the current last offset within a partition by default.\nA constructor for `TopicPartitionOffset` that takes an additional `boolean` argument is provided.\nIf this is `true`, the initial offsets (positive or negative) are relative to the current position for this consumer.\nThe offsets are applied when the container is started.\nThe second takes an array of topics, and Kafka allocates the partitions based on the `group.id` property -- distributing partitions across the group.\nThe third uses a regex `Pattern` to select the topics.\n\nTo assign a `MessageListener` to a container, you can use the `ContainerProps.setMessageListener` method when creating the Container.\nThe following example shows how to do so:\n\n[source, java]\n----\nContainerProperties containerProps = new ContainerProperties(\"topic1\", \"topic2\");\ncontainerProps.setMessageListener(new MessageListener<Integer, String>() {\n ...\n});\nDefaultKafkaConsumerFactory<Integer, String> cf =\n new DefaultKafkaConsumerFactory<>(consumerProps());\nKafkaMessageListenerContainer<Integer, String> container =\n new KafkaMessageListenerContainer<>(cf, containerProps);\nreturn container;\n----\n\nNote that when creating a `DefaultKafkaConsumerFactory`, using the constructor that just takes in the properties as above means that key and value `Deserializer` classes are picked up from configuration.\nAlternatively, `Deserializer` instances may be passed to the `DefaultKafkaConsumerFactory` constructor for key and/or value, in which case all Consumers share the same instances.\nAnother option is to provide ``Supplier<Deserializer>``s (starting with version 2.3) that will be used to obtain separate `Deserializer` instances for each `Consumer`:\n\n[source, java]\n----\n\nDefaultKafkaConsumerFactory<Integer, CustomValue> cf =\n new DefaultKafkaConsumerFactory<>(consumerProps(), null, () -> new CustomValueDeserializer());\nKafkaMessageListenerContainer<Integer, String> container =\n new KafkaMessageListenerContainer<>(cf, containerProps);\nreturn container;\n----\n\nRefer to the javadoc:org.springframework.kafka.listener.ContainerProperties[Javadoc] for `ContainerProperties` for more information about the various properties that you can set.\n\nSince version 2.1.1, a new property called `logContainerConfig` is available.\nWhen `true` and `INFO` logging is enabled each listener container writes a log message summarizing its configuration properties.\n\nBy default, logging of topic offset commits is performed at the `DEBUG` logging level.\nStarting with version 2.1.2, a property in `ContainerProperties` called `commitLogLevel` lets you specify the log level for these messages.\nFor example, to change the log level to `INFO`, you can use `containerProperties.setCommitLogLevel(LogIfLevelEnabled.Level.INFO);`.\n\nStarting with version 2.2, a new container property called `missingTopicsFatal` has been added (default: `false` since 2.3.4).\nThis prevents the container from starting if any of the configured topics are not present on the broker.\nIt does not apply if the container is configured to listen to a topic pattern (regex).\nPreviously, the container threads looped within the `consumer.poll()` method waiting for the topic to appear while logging many messages.\nAside from the logs, there was no indication that there was a problem.\n\nAs of version 2.8, a new container property `authExceptionRetryInterval` has been introduced.\nThis causes the container to retry fetching messages after getting any `AuthenticationException` or `AuthorizationException` from the `KafkaConsumer`.\nThis can happen when, for example, the configured user is denied access to read a certain topic or credentials are incorrect.\nDefining `authExceptionRetryInterval` allows the container to recover when proper permissions are granted.\n\nNOTE: By default, no interval is configured - authentication and authorization errors are considered fatal, which causes the container to stop.\n\nStarting with version 2.8, when creating the consumer factory, if you provide deserializers as objects (in the constructor or via the setters), the factory will invoke the `configure()` method to configure them with the configuration properties.\n\n[[using-ConcurrentMessageListenerContainer]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc", "title": "message-listener-container", "heading": "Using `KafkaMessageListenerContainer`", "heading_level": 2, "file_order": 15, "section_index": 1, "content_hash": "c26e22992b69b056ec1997f081ca0a3ab9fc5845e50a5c3271f98f2ea0934bb2", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc"}}
{"id": "sha256:ffc5c521da60dc0799a2718244590408d06ebd8a2448397962c04a875d4dbef4", "content": "The single constructor is similar to the `KafkaListenerContainer` constructor.\nThe following listing shows the constructor's signature:\n\n[source, java]\n----\npublic ConcurrentMessageListenerContainer(ConsumerFactory<K, V> consumerFactory,\n ContainerProperties containerProperties)\n----\n\nIt also has a `concurrency` property.\nFor example, `container.setConcurrency(3)` creates three `KafkaMessageListenerContainer` instances.\n\nIf the container properties are configured for topics (or topic pattern), Kafka distributes the partitions across the consumers using its group management capabilities.\n\n[IMPORTANT]\n====\nWhen listening to multiple topics, the default partition distribution may not be what you expect.\nFor example, if you have three topics with five partitions each and you want to use `concurrency=15`, you see only five active consumers, each assigned one partition from each topic, with the other 10 consumers being idle.\nThis is because the default Kafka `ConsumerPartitionAssignor` is the `RangeAssignor` (see its Javadoc).\nFor this scenario, you may want to consider using the `RoundRobinAssignor` instead, which distributes the partitions across all of the consumers.\nThen, each consumer is assigned one topic or partition.\nTo change the `ConsumerPartitionAssignor`, you can set the `partition.assignment.strategy` consumer property (`ConsumerConfig.PARTITION_ASSIGNMENT_STRATEGY_CONFIG`) in the properties provided to the `DefaultKafkaConsumerFactory`.\n\nWhen using Spring Boot, you can assign set the strategy as follows:\n\n=====\n[source]\n----\nspring.kafka.consumer.properties.partition.assignment.strategy=\\\norg.apache.kafka.clients.consumer.RoundRobinAssignor\n----\n=====\n====\n\nWhen the container properties are configured with ``TopicPartitionOffset``s, the `ConcurrentMessageListenerContainer` distributes the `TopicPartitionOffset` instances across the delegate `KafkaMessageListenerContainer` instances.\n\nIf, say, six `TopicPartitionOffset` instances are provided and the `concurrency` is `3`; each container gets two partitions.\nFor five `TopicPartitionOffset` instances, two containers get two partitions, and the third gets one.\nIf the `concurrency` is greater than the number of `TopicPartitions`, the `concurrency` is adjusted down such that each container gets one partition.\n\nNOTE: The `client.id` property (if set) is appended with `-n` where `n` is the consumer instance that corresponds to the concurrency.\nThis is required to provide unique names for MBeans when JMX is enabled.\n\nStarting with version 1.3, the `MessageListenerContainer` provides access to the metrics of the underlying `KafkaConsumer`.\nIn the case of `ConcurrentMessageListenerContainer`, the `metrics()` method returns the metrics for all the target `KafkaMessageListenerContainer` instances.\nThe metrics are grouped into the `Map<MetricName, ? extends Metric>` by the `client-id` provided for the underlying `KafkaConsumer`.\n\nStarting with version 2.3, the `ContainerProperties` provides an `idleBetweenPolls` option to let the main loop in the listener container to sleep between `KafkaConsumer.poll()` calls.\nAn actual sleep interval is selected as the minimum from the provided option and difference between the `max.poll.interval.ms` consumer config and the current records batch processing time.\n\n[[committing-offsets]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc", "title": "message-listener-container", "heading": "Using `ConcurrentMessageListenerContainer`", "heading_level": 2, "file_order": 15, "section_index": 2, "content_hash": "ffc5c521da60dc0799a2718244590408d06ebd8a2448397962c04a875d4dbef4", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc"}}
{"id": "sha256:1e252e341299e71ab0cafc4292e9bdef9521312b6e4215ef86537c5a038a1de5", "content": "Several options are provided for committing offsets.\nIf the `enable.auto.commit` consumer property is `true`, Kafka auto-commits the offsets according to its configuration.\nIf it is `false`, the containers support several `AckMode` settings (described in the next list).\nThe default `AckMode` is `BATCH`.\nStarting with version 2.3, the framework sets `enable.auto.commit` to `false` unless explicitly set in the configuration.\nPreviously, the Kafka default (`true`) was used if the property was not set.\n\nThe consumer `poll()` method returns one or more `ConsumerRecords`.\nThe `MessageListener` is called for each record.\nThe following lists describes the action taken by the container for each `AckMode` (when transactions are not being used):\n\n* `RECORD`: Commit the offset when the listener returns after processing the record.\n* `BATCH`: Commit the offset when all the records returned by the `poll()` have been processed.\n* `TIME`: Commit the offset when all the records returned by the `poll()` have been processed, as long as the `ackTime` since the last commit has been exceeded.\n* `COUNT`: Commit the offset when all the records returned by the `poll()` have been processed, as long as `ackCount` records have been received since the last commit.\n* `COUNT_TIME`: Similar to `TIME` and `COUNT`, but the commit is performed if either condition is `true`.\n* `MANUAL`: The message listener is responsible to `acknowledge()` the `Acknowledgment`.\nAfter that, the same semantics as `BATCH` are applied.\n* `MANUAL_IMMEDIATE`: Commit the offset immediately when the `Acknowledgment.acknowledge()` method is called by the listener.\n\nWhen using xref:kafka/transactions.adoc[transactions], the offset(s) are sent to the transaction and the semantics are equivalent to `RECORD` or `BATCH`, depending on the listener type (record or batch).\n\nNOTE: `MANUAL` and `MANUAL_IMMEDIATE` require the listener to be an `AcknowledgingMessageListener` or a `BatchAcknowledgingMessageListener`.\nSee xref:kafka/receiving-messages/message-listeners.adoc[Message Listeners].\n\nDepending on the `syncCommits` container property, the `commitSync()` or `commitAsync()` method on the consumer is used.\n`syncCommits` is `true` by default; also see `setSyncCommitTimeout`.\nSee `setCommitCallback` to get the results of asynchronous commits; the default callback is the `LoggingCommitCallback` which logs errors (and successes at debug level).\n\nBecause the listener container has its own mechanism for committing offsets, it prefers the Kafka `ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG` to be `false`.\nStarting with version 2.3, it unconditionally sets it to false unless specifically set in the consumer factory or the container's consumer property overrides.\n\nThe `Acknowledgment` has the following method:\n\n[source, java]\n----\npublic interface Acknowledgment {\n\n void acknowledge();\n\n}\n----\n\nThis method gives the listener control over when offsets are committed.\n\nStarting with version 2.3, the `Acknowledgment` interface has two additional methods `nack(long sleep)` and `nack(int index, long sleep)`.\nThe first one is used with a record listener, the second with a batch listener.\nCalling the wrong method for your listener type will throw an `IllegalStateException`.\n\nNOTE: If you want to commit a partial batch, using `nack()`, When using transactions, set the `AckMode` to `MANUAL`; invoking `nack()` will send the offsets of the successfully processed records to the transaction.\n\nIMPORTANT: `nack()` can only be called on the consumer thread that invokes your listener.\n\nIMPORTANT: `nack()` is not allowed when using xref:kafka/receiving-messages/ooo-commits.adoc[Out of Order Commits].\n\nWith a record listener, when `nack()` is called, any pending offsets are committed, the remaining records from the last poll are discarded, and seeks are performed on their partitions so that the failed record and unprocessed records are redelivered on the next `poll()`.\nThe consumer can be paused before redelivery, by setting the `sleep` argument.\nThis is similar functionality to throwing an exception when the container is configured with a `DefaultErrorHandler`.\n\nIMPORTANT: `nack()` pauses the entire listener for the specified sleep duration including all assigned partitions.\n\nWhen using a batch listener, you can specify the index within the batch where the failure occurred.\nWhen `nack()` is called, offsets will be committed for records before the index and seeks are performed on the partitions for the failed and discarded records so that they will be redelivered on the next `poll()`.\n\nSee xref:kafka/annotation-error-handling.adoc#error-handlers[Container Error Handlers] for more information.\n\nIMPORTANT: The consumer is paused during the sleep so that we continue to poll the broker to keep the consumer alive.\nThe actual sleep time, and its resolution, depends on the container's `pollTimeout` which defaults to 5 seconds.\nThe minimum sleep time is equal to the `pollTimeout` and all sleep times will be a multiple of it.\nFor small sleep times or, to increase its accuracy, consider reducing the container's `pollTimeout`.\n\nStarting with version 3.0.10, batch listeners can commit the offsets of parts of the batch, using `acknowledge(index)` on the `Acknowledgment` argument.\nWhen this method is called, the offset of the record at the index (as well as all previous records) will be committed.\nCalling `acknowledge()` after a partial batch commit is performed will commit the offsets of the remainder of the batch.\nThe following limitations apply:\n\n* `AckMode.MANUAL_IMMEDIATE` is required\n* The method must be called on the listener thread\n* The listener must consume a `List` rather than the raw `ConsumerRecords`\n* The index must be in the range of the list's elements\n* The index must be larger than that used in a previous call\n\nThese restrictions are enforced and the method will throw an `IllegalArgumentException` or `IllegalStateException`, depending on the violation.\n\n[[container-auto-startup]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc", "title": "message-listener-container", "heading": "Committing Offsets", "heading_level": 2, "file_order": 15, "section_index": 3, "content_hash": "1e252e341299e71ab0cafc4292e9bdef9521312b6e4215ef86537c5a038a1de5", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc"}}
{"id": "sha256:da4ba01756fd1a9af27d32f6bb34a099b47755b4056621923018df399327d1e1", "content": "The listener containers implement `SmartLifecycle`, and `autoStartup` is `true` by default.\nThe containers are started in a late phase (`Integer.MAX-VALUE - 100`).\nOther components that implement `SmartLifecycle`, to handle data from listeners, should be started in an earlier phase.\nThe `- 100` leaves room for later phases to enable components to be auto-started after the containers.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc", "title": "message-listener-container", "heading": "Listener Container Auto Startup", "heading_level": 2, "file_order": 15, "section_index": 4, "content_hash": "da4ba01756fd1a9af27d32f6bb34a099b47755b4056621923018df399327d1e1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/message-listener-container.adoc"}}
{"id": "sha256:0221e70da98161d214f76c334bd6f567ae4249119ae797ce249028f9e63b9612", "content": "[[message-listeners]]\n\nWhen you use a xref:kafka/receiving-messages/message-listener-container.adoc[message listener container], you must provide a listener to receive data.\nThere are currently eight supported interfaces for message listeners.\nThe following listing shows these interfaces:\n\n[source, java]\n----\npublic interface MessageListener<K, V> { <1>\n\n void onMessage(ConsumerRecord<K, V> data);\n\n}\n\npublic interface AcknowledgingMessageListener<K, V> extends MessageListener<K, V> { <2>\n\n void onMessage(ConsumerRecord<K, V> data, Acknowledgment acknowledgment);\n\n}\n\npublic interface ConsumerAwareMessageListener<K, V> extends MessageListener<K, V> { <3>\n\n void onMessage(ConsumerRecord<K, V> data, Consumer<?, ?> consumer);\n\n}\n\npublic interface AcknowledgingConsumerAwareMessageListener<K, V> extends MessageListener<K, V> { <4>\n\n void onMessage(ConsumerRecord<K, V> data, Acknowledgment acknowledgment, Consumer<?, ?> consumer);\n\n}\n\npublic interface BatchMessageListener<K, V> { <5>\n\n void onMessage(List<ConsumerRecord<K, V>> data);\n\n}\n\npublic interface BatchAcknowledgingMessageListener<K, V> { <6>\n\n void onMessage(List<ConsumerRecord<K, V>> data, Acknowledgment acknowledgment);\n\n}\n\npublic interface BatchConsumerAwareMessageListener<K, V> extends BatchMessageListener<K, V> { <7>\n\n void onMessage(List<ConsumerRecord<K, V>> data, Consumer<?, ?> consumer);\n\n}\n\npublic interface BatchAcknowledgingConsumerAwareMessageListener<K, V> extends BatchMessageListener<K, V> { <8>\n\n void onMessage(List<ConsumerRecord<K, V>> data, Acknowledgment acknowledgment, Consumer<?, ?> consumer);\n\n}\n----\n\n<1> Use this interface for processing individual `ConsumerRecord` instances received from the Kafka consumer `poll()` operation when using auto-commit or one of the container-managed xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[commit methods].\n\n<2> Use this interface for processing individual `ConsumerRecord` instances received from the Kafka consumer `poll()` operation when using one of the manual xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[commit methods].\n\n<3> Use this interface for processing individual `ConsumerRecord` instances received from the Kafka consumer `poll()` operation when using auto-commit or one of the container-managed xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[commit methods].\nAccess to the `Consumer` object is provided.\n\n<4> Use this interface for processing individual `ConsumerRecord` instances received from the Kafka consumer `poll()` operation when using one of the manual xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[commit methods].\nAccess to the `Consumer` object is provided.\n\n<5> Use this interface for processing all `ConsumerRecord` instances received from the Kafka consumer `poll()` operation when using auto-commit or one of the container-managed xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[commit methods].\n`AckMode.RECORD` is not supported when you use this interface, since the listener is given the complete batch.\n\n<6> Use this interface for processing all `ConsumerRecord` instances received from the Kafka consumer `poll()` operation when using one of the manual xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[commit methods].\n\n<7> Use this interface for processing all `ConsumerRecord` instances received from the Kafka consumer `poll()` operation when using auto-commit or one of the container-managed xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[commit methods].\n`AckMode.RECORD` is not supported when you use this interface, since the listener is given the complete batch.\nAccess to the `Consumer` object is provided.\n\n<8> Use this interface for processing all `ConsumerRecord` instances received from the Kafka consumer `poll()` operation when using one of the manual xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[commit methods].\nAccess to the `Consumer` object is provided.\n\nIMPORTANT: The `Consumer` object is not thread-safe.\nYou must only invoke its methods on the thread that calls the listener.\n\nIMPORTANT: You should not execute any `Consumer<?, ?>` methods that affect the consumer's positions or committed offsets in your listener; the container needs to manage such information.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/message-listeners.adoc", "title": "message-listeners", "heading": "message-listeners", "heading_level": 1, "file_order": 16, "section_index": 0, "content_hash": "0221e70da98161d214f76c334bd6f567ae4249119ae797ce249028f9e63b9612", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/message-listeners.adoc"}}
{"id": "sha256:e548ab55d2408b55fffd0e98898c3ab351351bd15b89ea0452af919ddb925721", "content": "[[ooo-commits]]\n\nNormally, when using `AckMode.MANUAL` or `AckMode.MANUAL_IMMEDIATE`, the acknowledgments must be acknowledged in order, because Kafka does not maintain state for each record, only a committed offset for each group/partition.\nStarting with version 2.8, you can now set the container property `asyncAcks`, which allows the acknowledgments for records returned by the poll to be acknowledged in any order.\nThe listener container will defer the out-of-order commits until the missing acknowledgments are received.\nThe consumer will be paused (no new records delivered) until all the offsets for the previous poll have been committed.\n\nIMPORTANT: While this feature allows applications to process records asynchronously, it should be understood that it increases the possibility of duplicate deliveries after a failure.\n\nIMPORTANT: When `asyncAcks` is activated, it is not possible to use `nack()` (negative acknowledgments) when xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[Committing Offsets].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/ooo-commits.adoc", "title": "ooo-commits", "heading": "ooo-commits", "heading_level": 1, "file_order": 17, "section_index": 0, "content_hash": "e548ab55d2408b55fffd0e98898c3ab351351bd15b89ea0452af919ddb925721", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/ooo-commits.adoc"}}
{"id": "sha256:c49d6386b93e49b1cfeffba4d80f7f54986da8cbb20c3936ed7715d6de4286d9", "content": "[[rebalance-listeners]]\n\n`ContainerProperties` has a property called `consumerRebalanceListener`, which takes an implementation of the Kafka client's `ConsumerRebalanceListener` interface.\nIf this property is not provided, the container configures a logging listener that logs rebalance events at the `INFO` level.\nThe framework also adds a sub-interface `ConsumerAwareRebalanceListener`.\nThe following listing shows the `ConsumerAwareRebalanceListener` interface definition:\n\n[source, java]\n----\npublic interface ConsumerAwareRebalanceListener extends ConsumerRebalanceListener {\n\n void onPartitionsRevokedBeforeCommit(Consumer<?, ?> consumer, Collection<TopicPartition> partitions);\n\n void onPartitionsRevokedAfterCommit(Consumer<?, ?> consumer, Collection<TopicPartition> partitions);\n\n void onPartitionsAssigned(Consumer<?, ?> consumer, Collection<TopicPartition> partitions);\n\n void onPartitionsLost(Consumer<?, ?> consumer, Collection<TopicPartition> partitions);\n\n}\n----\n\nNotice that there are two callbacks when partitions are revoked.\nThe first is called immediately.\nThe second is called after any pending offsets are committed.\nThis is useful if you wish to maintain offsets in some external repository, as the following example shows:\n\n[source, java]\n----\ncontainerProperties.setConsumerRebalanceListener(new ConsumerAwareRebalanceListener() {\n\n @Override\n public void onPartitionsRevokedBeforeCommit(Consumer<?, ?> consumer, Collection<TopicPartition> partitions) {\n // acknowledge any pending Acknowledgments (if using manual acks)\n }\n\n @Override\n public void onPartitionsRevokedAfterCommit(Consumer<?, ?> consumer, Collection<TopicPartition> partitions) {\n // ...\n store(consumer.position(partition));\n // ...\n }\n\n @Override\n public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n // ...\n consumer.seek(partition, offsetTracker.getOffset() + 1);\n // ...\n }\n});\n----\n\nIMPORTANT: Starting with version 2.4, a new method `onPartitionsLost()` has been added (similar to a method with the same name in `ConsumerRebalanceLister`).\nThe default implementation on `ConsumerRebalanceLister` simply calls `onPartitionsRevoked`.\nThe default implementation on `ConsumerAwareRebalanceListener` does nothing.\nWhen supplying the listener container with a custom listener (of either type), it is important that your implementation does not call `onPartitionsRevoked` from `onPartitionsLost`.\nIf you implement `ConsumerRebalanceListener` you should override the default method.\nThis is because the listener container will call its own `onPartitionsRevoked` from its implementation of `onPartitionsLost` after calling the method on your implementation.\nIf you implementation delegates to the default behavior, `onPartitionsRevoked` will be called twice each time the `Consumer` calls that method on the container's listener.\n\n[[new-rebalance-protocol]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/rebalance-listeners.adoc", "title": "rebalance-listeners", "heading": "rebalance-listeners", "heading_level": 1, "file_order": 18, "section_index": 0, "content_hash": "c49d6386b93e49b1cfeffba4d80f7f54986da8cbb20c3936ed7715d6de4286d9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/rebalance-listeners.adoc"}}
{"id": "sha256:7cb67d118d05892e5e3a955a0f44ff9d6dd1f3cd63bdffbddff70ed6daf62e3c", "content": "Spring for Apache Kafka 4.0 supports Apache https://cwiki.apache.org/confluence/display/KAFKA/KIP-848%3A+The+Next+Generation+of+the+Consumer+Rebalance+Protocol[Kafka 4.0â€™s new consumer rebalance protocol] (KIP-848), which enhances performance with server-driven, incremental partition assignments.\nThis reduces rebalancing downtime for consumer groups.\n\nTo enable the new protocol, configure the `group.protocol` property:\n\n[source, properties]\n----\nspring.kafka.consumer.properties.group.protocol=consumer\n----\n\nKeep in mind that, the above property is a Spring Boot property.\nIf you are not using Spring Boot, you may want to set it manually as shown below.\n\nAlternatively, set it programmatically:\n\n[source, java]\n----\nMap<String, Object> props = new HashMap<>();\nprops.put(\"group.protocol\", \"consumer\");\nConsumerFactory<String, String> factory = new DefaultKafkaConsumerFactory<>(props);\n----\n\nThe new protocol works seamlessly with `ConsumerAwareRebalanceListener`.\nDue to incremental rebalancing, `onPartitionsAssigned` may be called multiple times with smaller partition sets, unlike the single callback typical of the legacy protocol.\n\nThe new protocol uses server-side partition assignments, ignoring client-side custom assignors set via `spring.kafka.consumer.partition-assignment-strategy`.\nA warning is logged if a custom assignor is detected.\nTo use custom assignors, set `group.protocol=classic` (which is the default if you don't specify a value for `group.protocol`).", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/rebalance-listeners.adoc", "title": "rebalance-listeners", "heading": "Kafka 4.0 Consumer Rebalance Protocol", "heading_level": 2, "file_order": 18, "section_index": 1, "content_hash": "7cb67d118d05892e5e3a955a0f44ff9d6dd1f3cd63bdffbddff70ed6daf62e3c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/rebalance-listeners.adoc"}}
{"id": "sha256:9c1212978a2559b6800d6f9f015a8e3468db674f55a2c7638d19f8bc62f8c8f1", "content": "[[retrying-deliveries]]\n\nSee the `DefaultErrorHandler` in xref:kafka/annotation-error-handling.adoc[Handling Exceptions].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/retrying-deliveries.adoc", "title": "retrying-deliveries", "heading": "retrying-deliveries", "heading_level": 1, "file_order": 19, "section_index": 0, "content_hash": "9c1212978a2559b6800d6f9f015a8e3468db674f55a2c7638d19f8bc62f8c8f1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/retrying-deliveries.adoc"}}
{"id": "sha256:e8c9cbd98c358798032d96464e2d85e72c3c2b0fda6ccc00d23f4c6e127e6be0", "content": "[[sequencing]]\n\nA common use case is to start a listener after another listener has consumed all the records in a topic.\nFor example, you may want to load the contents of one or more compacted topics into memory before processing records from other topics.\nStarting with version 2.7.3, a new component `ContainerGroupSequencer` has been introduced.\nIt uses the ``@KafkaListener``'s `containerGroup` property to group containers together and start the containers in the next group, when all the containers in the current group have gone idle.\n\nIt is best illustrated with an example.\n\n[source, java]\n----\n@KafkaListener(id = \"listen1\", topics = \"topic1\", containerGroup = \"g1\", concurrency = \"2\")\npublic void listen1(String in) {\n}\n\n@KafkaListener(id = \"listen2\", topics = \"topic2\", containerGroup = \"g1\", concurrency = \"2\")\npublic void listen2(String in) {\n}\n\n@KafkaListener(id = \"listen3\", topics = \"topic3\", containerGroup = \"g2\", concurrency = \"2\")\npublic void listen3(String in) {\n}\n\n@KafkaListener(id = \"listen4\", topics = \"topic4\", containerGroup = \"g2\", concurrency = \"2\")\npublic void listen4(String in) {\n}\n\n@Bean\nContainerGroupSequencer sequencer(KafkaListenerEndpointRegistry registry) {\n return new ContainerGroupSequencer(registry, 5000, \"g1\", \"g2\");\n}\n----\n\nHere, we have 4 listeners in two groups, `g1` and `g2`.\n\nDuring application context initialization, the sequencer sets the `autoStartup` property of all the containers in the provided groups to `false`.\nIt also sets the `idleEventInterval` for any containers (that do not already have one set) to the supplied value (5000ms in this case).\nThen, when the sequencer is started by the application context, the containers in the first group are started.\nAs ``ListenerContainerIdleEvent``s are received, each individual child container in each container is stopped.\nWhen all child containers in a `ConcurrentMessageListenerContainer` are stopped, the parent container is stopped.\nWhen all containers in a group have been stopped, the containers in the next group are started.\nThere is no limit to the number of groups or containers in a group.\n\nBy default, the containers in the final group (`g2` above) are not stopped when they go idle.\nTo modify that behavior, set `stopLastGroupWhenIdle` to `true` on the sequencer.\n\nAs an aside, previously containers in each group were added to a bean of type `Collection<MessageListenerContainer>` with the bean name being the `containerGroup`.\nThese collections are now deprecated in favor of beans of type `ContainerGroup` with a bean name that is the group name, suffixed with `.group`; in the example above, there would be 2 beans `g1.group` and `g2.group`.\nThe `Collection` beans will be removed in a future release.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/sequencing.adoc", "title": "sequencing", "heading": "sequencing", "heading_level": 1, "file_order": 20, "section_index": 0, "content_hash": "e8c9cbd98c358798032d96464e2d85e72c3c2b0fda6ccc00d23f4c6e127e6be0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/sequencing.adoc"}}
{"id": "sha256:625c34664fe2dba377269693e9c4adef5daead997de9b43f2384832713b187da", "content": "[[kafka-template-receive]]\n\nThis section covers how to use `KafkaTemplate` to receive messages.\n\nStarting with version 2.8, the template has four `receive()` methods:\n\n[source, java]\n----\nConsumerRecord<K, V> receive(String topic, int partition, long offset);\n\nConsumerRecord<K, V> receive(String topic, int partition, long offset, Duration pollTimeout);\n\nConsumerRecords<K, V> receive(Collection<TopicPartitionOffset> requested);\n\nConsumerRecords<K, V> receive(Collection<TopicPartitionOffset> requested, Duration pollTimeout);\n----\n\nAs you can see, you need to know the partition and offset of the record(s) you need to retrieve; a new `Consumer` is created (and closed) for each operation.\n\nWith the last two methods, each record is retrieved individually and the results assembled into a `ConsumerRecords` object.\nWhen creating the ``TopicPartitionOffset``s for the request, only positive, absolute offsets are supported.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/template-receive.adoc", "title": "template-receive", "heading": "template-receive", "heading_level": 1, "file_order": 21, "section_index": 0, "content_hash": "625c34664fe2dba377269693e9c4adef5daead997de9b43f2384832713b187da", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/template-receive.adoc"}}
{"id": "sha256:61acb5b0a3d23ed6671cf73a9ad4c36bd9870f7b58a36edc2c34c166cadcecc2", "content": "[[kafka-validation]]\n\nStarting with version 2.2, it is now easier to add a `Validator` to validate `@KafkaListener` `@Payload` arguments.\nPreviously, you had to configure a custom `DefaultMessageHandlerMethodFactory` and add it to the registrar.\nNow, you can add the validator to the registrar itself.\nThe following code shows how to do so:\n\n[source, java]\n----\n@Configuration\n@EnableKafka\npublic class Config implements KafkaListenerConfigurer {\n\n ...\n\n @Override\n public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {\n registrar.setValidator(new MyValidator());\n }\n\n}\n----\n\nNOTE: When you use Spring Boot with the validation starter, a `LocalValidatorFactoryBean` is auto-configured, as the following example shows:\n\n[source, java]\n----\n@Configuration\n@EnableKafka\npublic class Config implements KafkaListenerConfigurer {\n\n @Autowired\n private LocalValidatorFactoryBean validator;\n ...\n\n @Override\n public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {\n registrar.setValidator(this.validator);\n }\n}\n----\n\nThe following examples show how to validate:\n\n[source, java]\n----\npublic static class ValidatedClass {\n\n @Max(10)\n private int bar;\n\n public int getBar() {\n return this.bar;\n }\n\n public void setBar(int bar) {\n this.bar = bar;\n }\n\n}\n----\n\n[source, java]\n----\n@KafkaListener(id=\"validated\", topics = \"annotated35\", errorHandler = \"validationErrorHandler\",\n containerFactory = \"kafkaJsonListenerContainerFactory\")\npublic void validatedListener(@Payload @Valid ValidatedClass val) {\n ...\n}\n\n@Bean\npublic KafkaListenerErrorHandler validationErrorHandler() {\n return (m, e) -> {\n ...\n };\n}\n----\n\nStarting with version 2.5.11, validation now works on payloads for `@KafkaHandler` methods in a class-level listener.\nSee xref:kafka/receiving-messages/class-level-kafkalistener.adoc[`@KafkaListener` on a Class].\n\nStarting with version 3.1, you can perform validation in an `ErrorHandlingDeserializer` instead.\nSee xref:kafka/serdes.adoc#error-handling-deserializer[Using `ErrorHandlingDeserializer`] for more information.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages/validation.adoc", "title": "validation", "heading": "validation", "heading_level": 1, "file_order": 22, "section_index": 0, "content_hash": "61acb5b0a3d23ed6671cf73a9ad4c36bd9870f7b58a36edc2c34c166cadcecc2", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages/validation.adoc"}}
{"id": "sha256:f21cf61c129215fee98f1868ee5fbb560984b5ad64b84030fb40e2729e6f0a37", "content": "[[topicpartition-initial-offset]]\n\nThere are several ways to set the initial offset for a partition.\n\nWhen manually assigning partitions, you can set the initial offset (if desired) in the configured `TopicPartitionOffset` arguments (see xref:kafka/receiving-messages/message-listener-container.adoc[Message Listener Containers]).\nYou can also seek to a specific offset at any time.\n\nWhen you use group management where the broker assigns partitions:\n\n* For a new `group.id`, the initial offset is determined by the `auto.offset.reset` consumer property (`earliest` or `latest`).\n* For an existing group ID, the initial offset is the current offset for that group ID.\nYou can, however, seek to a specific offset during initialization (or at any time thereafter).", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/topic/partition-initial-offset.adoc", "title": "partition-initial-offset", "heading": "partition-initial-offset", "heading_level": 1, "file_order": 23, "section_index": 0, "content_hash": "f21cf61c129215fee98f1868ee5fbb560984b5ad64b84030fb40e2729e6f0a37", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/topic/partition-initial-offset.adoc"}}
{"id": "sha256:80fbc9f7d208b5710ddbc360ac6e2756da7fc81962c45d7728bed7f6c47b2f81", "content": "[[annotation-error-handling]]\n\nThis section describes how to handle various exceptions that may arise when you use Spring for Apache Kafka.\n\n[[listener-error-handlers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "annotation-error-handling", "heading_level": 1, "file_order": 24, "section_index": 0, "content_hash": "80fbc9f7d208b5710ddbc360ac6e2756da7fc81962c45d7728bed7f6c47b2f81", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:85f4779810e6ac8e96bf6c84926b7ae56cc3aa2b5af3da8443f40d58a3f4e0b6", "content": "Starting with version 2.0, the `@KafkaListener` annotation has a new attribute: `errorHandler`.\n\nYou can use the `errorHandler` to provide the bean name of a `KafkaListenerErrorHandler` implementation.\nThis functional interface has one method, as the following listing shows:\n\n[source, java]\n----\n@FunctionalInterface\npublic interface KafkaListenerErrorHandler {\n\n Object handleError(Message<?> message, ListenerExecutionFailedException exception) throws Exception;\n\n}\n----\n\nYou have access to the spring-messaging `Message<?>` object produced by the message converter and the exception that was thrown by the listener, which is wrapped in a `ListenerExecutionFailedException`.\nThe error handler can throw the original or a new exception, which is thrown to the container.\nAnything returned by the error handler is ignored.\n\nStarting with version 2.7, you can set the `rawRecordHeader` property on the `MessagingMessageConverter` and `BatchMessagingMessageConverter` which causes the raw `ConsumerRecord` to be added to the converted `Message<?>` in the `KafkaHeaders.RAW_DATA` header.\nThis is useful, for example, if you wish to use a `DeadLetterPublishingRecoverer` in a listener error handler.\nIt might be used in a request/reply scenario where you wish to send a failure result to the sender, after some number of retries, after capturing the failed record in a dead letter topic.\n\n[source, java]\n----\n@Bean\npublic KafkaListenerErrorHandler eh(DeadLetterPublishingRecoverer recoverer) {\n return (msg, ex) -> {\n if (msg.getHeaders().get(KafkaHeaders.DELIVERY_ATTEMPT, Integer.class) > 9) {\n recoverer.accept(msg.getHeaders().get(KafkaHeaders.RAW_DATA, ConsumerRecord.class), ex);\n return \"FAILED\";\n }\n throw ex;\n };\n}\n----\n\nIt has a sub-interface (`ConsumerAwareListenerErrorHandler`) that has access to the consumer object, through the following method:\n\n[source, java]\n----\nObject handleError(Message<?> message, ListenerExecutionFailedException exception, Consumer<?, ?> consumer);\n----\n\nAnother sub-interface (`ManualAckListenerErrorHandler`) provides access to the `Acknowledgment` object when using manual ``AckMode``s.\n\n[source, java]\n----\nObject handleError(Message<?> message, ListenerExecutionFailedException exception,\n Consumer<?, ?> consumer, @Nullable Acknowledgment ack);\n----\n\nIn either case, you should NOT perform any seeks on the consumer because the container would be unaware of them.\n\n[[error-handlers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Listener Error Handlers", "heading_level": 2, "file_order": 24, "section_index": 1, "content_hash": "85f4779810e6ac8e96bf6c84926b7ae56cc3aa2b5af3da8443f40d58a3f4e0b6", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:42089368fc758fec7aea6210581327ee42c8b71a5dd6a853ccb52a144c54f629", "content": "Starting with version 2.8, the legacy `ErrorHandler` and `BatchErrorHandler` interfaces have been superseded by a new `CommonErrorHandler`.\nThese error handlers can handle errors for both record and batch listeners, allowing a single listener container factory to create containers for both types of listener.\n`CommonErrorHandler` implementations to replace most legacy framework error handler implementations are provided.\n\nSee xref:kafka/annotation-error-handling.adoc#migrating-legacy-eh[Migrating Custom Legacy Error Handler Implementations to `CommonErrorHandler`] for information to migrate custom error handlers to `CommonErrorHandler`.\n\nWhen transactions are being used, no error handlers are configured, by default, so that the exception will roll back the transaction.\nError handling for transactional containers are handled by the xref:kafka/annotation-error-handling.adoc#after-rollback[`AfterRollbackProcessor`].\nIf you provide a custom error handler when using transactions, it must throw an exception if you want the transaction rolled back.\n\nThis interface has a default method `isAckAfterHandle()` which is called by the container to determine whether the offset(s) should be committed if the error handler returns without throwing an exception; it returns true by default.\n\nTypically, the error handlers provided by the framework will throw an exception when the error is not \"handled\" (e.g. after performing a seek operation).\nBy default, such exceptions are logged by the container at `ERROR` level.\nAll of the framework error handlers extend `KafkaExceptionLogLevelAware` which allows you to control the level at which these exceptions are logged.\n\n[source, java]\n----\n/**\n * Set the level at which the exception thrown by this handler is logged.\n * @param logLevel the level (default ERROR).\n */\npublic void setLogLevel(KafkaException.Level logLevel) {\n ...\n}\n----\n\nYou can specify a global error handler to be used for all listeners in the container factory.\nThe following example shows how to do so:\n\n[source, java]\n----\n@Bean\npublic KafkaListenerContainerFactory<ConcurrentMessageListenerContainer<Integer, String>>\n kafkaListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory<Integer, String> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n ...\n factory.setCommonErrorHandler(myErrorHandler);\n ...\n return factory;\n}\n----\n\nBy default, if an annotated listener method throws an exception, it is thrown to the container, and the message is handled according to the container configuration.\n\nThe container commits any pending offset commits before calling the error handler.\n\nIf you are using Spring Boot, you simply need to add the error handler as a `@Bean` and Boot will add it to the auto-configured factory.\n\n[[backoff-handlers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Container Error Handlers", "heading_level": 2, "file_order": 24, "section_index": 2, "content_hash": "42089368fc758fec7aea6210581327ee42c8b71a5dd6a853ccb52a144c54f629", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:97f0806e1b42d5c07c9e2bf0be1045d6891ae22671e31e68760ec673482b2b88", "content": "Error handlers such as the xref:kafka/annotation-error-handling.adoc#default-eh[DefaultErrorHandler] use a `BackOff` to determine how long to wait before retrying a delivery.\nStarting with version 2.9, you can configure a custom `BackOffHandler`.\nThe default handler simply suspends the thread until the back off time passes (or the container is stopped).\nThe framework also provides the `ContainerPausingBackOffHandler` which pauses the listener container until the back off time passes and then resumes the container.\nThis is useful when the delays are longer than the `max.poll.interval.ms` consumer property.\nNote that the resolution of the actual back off time will be affected by the `pollTimeout` container property.\n\n[[default-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Back Off Handlers", "heading_level": 2, "file_order": 24, "section_index": 3, "content_hash": "97f0806e1b42d5c07c9e2bf0be1045d6891ae22671e31e68760ec673482b2b88", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:4edd07b5b2f5d449594bff289795466ea83ffa230e3ecf48b745a0e412df961f", "content": "This new error handler replaces the `SeekToCurrentErrorHandler` and `RecoveringBatchErrorHandler`, which have been the default error handlers for several releases now.\nOne difference is that the fallback behavior for batch listeners (when an exception other than a `BatchListenerFailedException` is thrown) is the equivalent of the xref:kafka/annotation-error-handling.adoc#retrying-batch-eh[Retrying Complete Batches].\n\nIMPORTANT: Starting with version 2.9, the `DefaultErrorHandler` can be configured to provide the same semantics as seeking the unprocessed record offsets as discussed below, but without actually seeking.\nInstead, the records are retained by the listener container and resubmitted to the listener after the error handler exits (and after performing a single paused `poll()`, to keep the consumer alive; if xref:retrytopic.adoc[Non-Blocking Retries] or a `ContainerPausingBackOffHandler` are being used, the pause may extend over multiple polls).\nThe error handler returns a result to the container that indicates whether the current failing record can be resubmitted, or if it was recovered and then it will not be sent to the listener again.\nTo enable this mode, set the property `seekAfterError` to `false`.\n\nThe error handler can recover (skip) a record that keeps failing.\nBy default, after ten failures, the failed record is logged (at the `ERROR` level).\nYou can configure the handler with a custom recoverer (`BiConsumer`) and a `BackOff` that controls the delivery attempts and delays between each.\nUsing a `FixedBackOff` with `FixedBackOff.UNLIMITED_ATTEMPTS` causes (effectively) infinite retries.\nThe following example configures recovery after three tries:\n\n[source, java]\n----\nDefaultErrorHandler errorHandler =\n new DefaultErrorHandler((record, exception) -> {\n // recover after 3 failures, with no back off - e.g. send to a dead-letter topic\n }, new FixedBackOff(0L, 2L));\n----\n\nTo configure the listener container with a customized instance of this handler, add it to the container factory.\n\nFor example, with the `@KafkaListener` container factory, you can add `DefaultErrorHandler` as follows:\n\n[source, java]\n----\n@Bean\npublic ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\n factory.setConsumerFactory(consumerFactory());\n factory.getContainerProperties().setAckMode(AckMode.RECORD);\n factory.setCommonErrorHandler(new DefaultErrorHandler(new FixedBackOff(1000L, 2L)));\n return factory;\n}\n----\n\nFor a record listener, this will retry a delivery up to 2 times (3 delivery attempts) with a back off of 1 second, instead of the default configuration (`FixedBackOff(0L, 9)`).\nFailures are simply logged after retries are exhausted.\n\nAs an example, if the `poll` returns six records (two from each partition 0, 1, 2) and the listener throws an exception on the fourth record, the container acknowledges the first three messages by committing their offsets.\nThe `DefaultErrorHandler` seeks to offset 1 for partition 1 and offset 0 for partition 2.\nThe next `poll()` returns the three unprocessed records.\n\nIf the `AckMode` was `BATCH`, the container commits the offsets for the first two partitions before calling the error handler.\n\nFor a batch listener, the listener must throw a `BatchListenerFailedException` indicating which records in the batch failed.\n\nThe sequence of events is:\n\n* Commit the offsets of the records before the index.\n* If retries are not exhausted, perform seeks so that all the remaining records (including the failed record) will be redelivered.\n* If retries are exhausted, attempt recovery of the failed record (default log only) and perform seeks so that the remaining records (excluding the failed record) will be redelivered.\nThe recovered record's offset is committed.\n* If retries are exhausted and recovery fails, seeks are performed as if retries are not exhausted.\n\nIMPORTANT: Starting with version 2.9, the `DefaultErrorHandler` can be configured to provide the same semantics as seeking the unprocessed record offsets as discussed above, but without actually seeking.\nInstead, error handler creates a new `ConsumerRecords<?, ?>` containing just the unprocessed records which will then be submitted to the listener (after performing a single paused `poll()`, to keep the consumer alive).\nTo enable this mode, set the property `seekAfterError` to `false`.\n\nThe default recoverer logs the failed record after retries are exhausted.\nYou can use a custom recoverer, or one provided by the framework such as the xref:kafka/annotation-error-handling.adoc#dead-letters[`DeadLetterPublishingRecoverer`].\n\nWhen using a POJO batch listener (e.g. `List<Thing>`), and you don't have the full consumer record to add to the exception, you can just add the index of the record that failed:\n\n[source, java]\n----\n@KafkaListener(id = \"recovering\", topics = \"someTopic\")\npublic void listen(List<Thing> things) {\n for (int i = 0; i < things.size(); i++) {\n try {\n process(things.get(i));\n }\n catch (Exception e) {\n throw new BatchListenerFailedException(\"Failed to process\", i);\n }\n }\n}\n----\n\nWhen the container is configured with `AckMode.MANUAL_IMMEDIATE`, the error handler can be configured to commit the offset of recovered records; set the `commitRecovered` property to `true`.\n\nSee also xref:kafka/annotation-error-handling.adoc#dead-letters[Publishing Dead-letter Records].\n\nWhen using transactions, similar functionality is provided by the `DefaultAfterRollbackProcessor`.\nSee xref:kafka/annotation-error-handling.adoc#after-rollback[After-rollback Processor].\n\nThe `DefaultErrorHandler` considers certain exceptions to be fatal, and retries are skipped for such exceptions; the recoverer is invoked on the first failure.\nThe exceptions that are considered fatal, by default, are:\n\n* `DeserializationException`\n* `MessageConversionException`\n* `ConversionException`\n* `MethodArgumentResolutionException`\n* `NoSuchMethodException`\n* `ClassCastException`\n\nsince these exceptions are unlikely to be resolved on a retried delivery.\n\nYou can add more exception types to the not-retryable category, or completely replace the map of classified exceptions.\nSee the Javadocs for `DefaultErrorHandler.addNotRetryableException()` and `DefaultErrorHandler.setClassifications()` for more information, as well as `ExceptionMatcher`.\n\nHere is an example that adds `IllegalArgumentException` to the not-retryable exceptions:\n\n[source, java]\n----\n@Bean\npublic DefaultErrorHandler errorHandler(ConsumerRecordRecoverer recoverer) {\n DefaultErrorHandler handler = new DefaultErrorHandler(recoverer);\n handler.addNotRetryableExceptions(IllegalArgumentException.class);\n return handler;\n}\n----\n\n[IMPORTANT]\nThe `DefaultErrorHandler` only processes exceptions that inherit from `RuntimeException`.\nExceptions inheriting from `Error` bypass the error handler entirely, causing the consumer to terminate immediately, close the Kafka connection, and skip all retry/recovery mechanisms.\nThis critical distinction means applications may report healthy status despite having terminated consumers that no longer process messages.\nAlways ensure that exceptions thrown in message processing code explicitly extend from `RuntimeException` rather than `Error` to allow proper error handling.\nIn other words, if the application throws an exception, ensure that it is extended from `RuntimeException` and not inadvertently inherited from `Error`.\nStandard errors like `OutOfMemoryError`, `IllegalAccessError`, and other errors beyond the control of the application are still treated as ``Error``s and not retried.\n\nThe error handler can be configured with one or more ``RetryListener``s, receiving notifications of retry and recovery progress.\nStarting with version 2.8.10, methods for batch listeners were added.\n\n[source, java]\n----\n@FunctionalInterface\npublic interface RetryListener {\n\n void failedDelivery(ConsumerRecord<?, ?> record, Exception ex, int deliveryAttempt);\n\n default void recovered(ConsumerRecord<?, ?> record, Exception ex) {\n }\n\n default void recoveryFailed(ConsumerRecord<?, ?> record, Exception original, Exception failure) {\n }\n\n default void failedDelivery(ConsumerRecords<?, ?> records, Exception ex, int deliveryAttempt) {\n }\n\n default void recovered(ConsumerRecords<?, ?> records, Exception ex) {\n }\n\n\tdefault void recoveryFailed(ConsumerRecords<?, ?> records, Exception original, Exception failure) {\n\t}\n\n}\n----\n\nSee the JavaDocs for more information.\n\nIMPORTANT: If the recoverer fails (throws an exception), the failed record will be included in the seeks.\nIf the recoverer fails, the `BackOff` will be reset by default and redeliveries will again go through the back offs before recovery is attempted again.\nTo skip retries after a recovery failure, set the error handler's `resetStateOnRecoveryFailure` to `false`.\n\nYou can provide the error handler with a `BiFunction<ConsumerRecord<?, ?>, Exception, BackOff>` to determine the `BackOff` to use, based on the failed record and/or the exception:\n\n[source, java]\n----\nhandler.setBackOffFunction((record, ex) -> { ... });\n----\n\nIf the function returns `null`, the handler's default `BackOff` will be used.\n\nSet `resetStateOnExceptionChange` to `true` and the retry sequence will be restarted (including the selection of a new `BackOff`, if so configured) if the exception type changes between failures.\nWhen `false` (the default before version 2.9), the exception type is not considered.\n\nStarting with version 2.9, this is now `true` by default.\n\nAlso see xref:kafka/annotation-error-handling.adoc#delivery-header[Delivery Attempts Header].\n\n[[batch-listener-error-handling-dlt]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "DefaultErrorHandler", "heading_level": 2, "file_order": 24, "section_index": 4, "content_hash": "4edd07b5b2f5d449594bff289795466ea83ffa230e3ecf48b745a0e412df961f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:4ab41711db2404bb3c49da4b84449f6a13ba4de5b7348722e3f262b79b1eeac1", "content": "IMPORTANT: xref:retrytopic.adoc[Non-Blocking Retries] (the `@RetryableTopic` annotation) are NOT supported with batch listeners.\nFor Dead Letter Topic functionality with batch listeners, use `DefaultErrorHandler` with `DeadLetterPublishingRecoverer`.\n\n[[batch-listener-failed-exception]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Batch Listener Error Handling with Dead Letter Topics", "heading_level": 2, "file_order": 24, "section_index": 5, "content_hash": "4ab41711db2404bb3c49da4b84449f6a13ba4de5b7348722e3f262b79b1eeac1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:48260c871e52748d1747da6a02242a7b802f74dfcdafcd43836d07fc0be704be", "content": "To indicate which specific record in a batch failed, throw a `BatchListenerFailedException`:\n\n[source, java]\n----\n@KafkaListener(id = \"batch-listener\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void listen(List<ConsumerRecord<String, Order>> records) {\n for (ConsumerRecord<String, Order> record : records) {\n try {\n process(record.value());\n }\n catch (Exception e) {\n // Identifies the failed record for error handling\n throw new BatchListenerFailedException(\"Failed to process\", e, record);\n }\n }\n}\n----\n\nFor POJO batch listeners where you don't have the `ConsumerRecord`, use the index instead:\n\n[source, java]\n----\n@KafkaListener(id = \"batch-listener\", topics = \"myTopic\", containerFactory = \"batchFactory\")\npublic void listen(List<Order> orders) {\n for (int i = 0; i < orders.size(); i++) {\n try {\n process(orders.get(i));\n }\n catch (Exception e) {\n throw new BatchListenerFailedException(\"Failed to process\", e, i);\n }\n }\n}\n----\n\n[[batch-listener-dlt-config]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Using BatchListenerFailedException", "heading_level": 3, "file_order": 24, "section_index": 6, "content_hash": "48260c871e52748d1747da6a02242a7b802f74dfcdafcd43836d07fc0be704be", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:2a12fe886eb07c72fdf48b4123c11ad89fd813951260c87a7f1c4fde14447727", "content": "Configure a `DefaultErrorHandler` with a `DeadLetterPublishingRecoverer` on your batch listener container factory:\n\n[source, java]\n----\n@Bean\npublic ConcurrentKafkaListenerContainerFactory<String, Order> batchFactory(\n ConsumerFactory<String, Order> consumerFactory,\n KafkaTemplate<String, Order> kafkaTemplate) {\n\n ConcurrentKafkaListenerContainerFactory<String, Order> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n factory.setConsumerFactory(consumerFactory);\n factory.setBatchListener(true);\n\n // Configure Dead Letter Publishing\n DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(kafkaTemplate,\n (record, ex) -> new TopicPartition(record.topic() + \"-dlt\", record.partition()));\n\n // Configure retries: 3 attempts with 1 second between each\n DefaultErrorHandler errorHandler = new DefaultErrorHandler(recoverer,\n new FixedBackOff(1000L, 2L)); // 2 retries = 3 total attempts\n\n factory.setCommonErrorHandler(errorHandler);\n return factory;\n}\n----\n\n[[batch-listener-error-flow]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Configuring Dead Letter Topics for Batch Listeners", "heading_level": 3, "file_order": 24, "section_index": 7, "content_hash": "2a12fe886eb07c72fdf48b4123c11ad89fd813951260c87a7f1c4fde14447727", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:e9142866f23ca2485baedf51c0e8d3857572d6c63f5ddeb609055e9f88ca1311", "content": "When a `BatchListenerFailedException` is thrown, the `DefaultErrorHandler`:\n\n1. **Commits offsets** for all records before the failed record\n2. **Retries** the failed record (and subsequent records) according to the `BackOff` configuration\n3. **Publishes to DLT** when retries are exhausted - only the failed record is sent to the DLT\n4. **Commits the failed record's offset** and redelivers remaining records for processing\n\nExample flow with a batch of 6 records where record at index 2 fails:\n\n* First attempt: Records 0, 1 processed successfully; record 2 fails\n* Container commits offsets for records 0, 1\n* Retry attempt 1: Records 2, 3, 4, 5 are retried\n* Retry attempt 2: Records 2, 3, 4, 5 are retried again\n* After retries exhausted: Record 2 is published to DLT and its offset is committed\n* Container continues with records 3, 4, 5\n\n[[batch-listener-skip-retries]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "How Batch Error Handling Works", "heading_level": 3, "file_order": 24, "section_index": 8, "content_hash": "e9142866f23ca2485baedf51c0e8d3857572d6c63f5ddeb609055e9f88ca1311", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:52dfa5781f128ac544f4ce0c13a9ab26035a35253cebdde08be46b9dbb3d2de9", "content": "By default, the `DefaultErrorHandler` retries all exceptions except for fatal ones (like `DeserializationException`, `MessageConversionException`, etc.).\nTo skip retries for your own exception types, configure the error handler with exception classifications.\n\nThe error handler examines the **cause** of the `BatchListenerFailedException` to determine if it should skip retries:\n\n[source, java]\n----\n@Bean\npublic ConcurrentKafkaListenerContainerFactory<String, Order> batchFactory(\n ConsumerFactory<String, Order> consumerFactory,\n KafkaTemplate<String, Order> kafkaTemplate) {\n\n ConcurrentKafkaListenerContainerFactory<String, Order> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n factory.setConsumerFactory(consumerFactory);\n factory.setBatchListener(true);\n\n DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(kafkaTemplate);\n DefaultErrorHandler errorHandler = new DefaultErrorHandler(recoverer,\n new FixedBackOff(1000L, 2L));\n\n // Add custom exception types that should skip retries and go directly to DLT\n errorHandler.addNotRetryableExceptions(ValidationException.class, InvalidFormatException.class);\n\n factory.setCommonErrorHandler(errorHandler);\n return factory;\n}\n----\n\nNow in your listener:\n\n[source, java]\n----\n@KafkaListener(id = \"batch-listener\", topics = \"orders\", containerFactory = \"batchFactory\")\npublic void processOrders(List<ConsumerRecord<String, Order>> records) {\n for (ConsumerRecord<String, Order> record : records) {\n try {\n process(record.value());\n }\n catch (DatabaseException e) {\n // Will be retried 3 times (according to BackOff configuration)\n throw new BatchListenerFailedException(\"Database error\", e, record);\n }\n catch (ValidationException e) {\n // Skips retries - goes directly to DLT\n // (because ValidationException is configured as not retryable)\n throw new BatchListenerFailedException(\"Validation failed\", e, record);\n }\n }\n}\n----\n\nIMPORTANT: The error handler checks the **cause** (the second parameter) of the `BatchListenerFailedException`.\nIf the cause is classified as not retryable, the record is immediately sent to the DLT without retries.\n\n[[batch-listener-offset-commits]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Skipping Retries for Specific Exceptions", "heading_level": 3, "file_order": 24, "section_index": 9, "content_hash": "52dfa5781f128ac544f4ce0c13a9ab26035a35253cebdde08be46b9dbb3d2de9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:00cc11ad11cb67329b2d486911012d0c3b7e87ea36fc829b0947315f02460433", "content": "Understanding offset commits is important for batch error handling:\n\n* **AckMode.BATCH** (most common for batch listeners):\n - Offsets before the failed record are committed before error handling\n - The failed record's offset is committed after successful recovery (DLT publishing)\n\n* **AckMode.MANUAL_IMMEDIATE**:\n - Set `errorHandler.setCommitRecovered(true)` to commit recovered record offsets\n - You control acknowledgment timing in your listener\n\nExample with manual acknowledgment:\n\n[source, java]\n----\n@KafkaListener(id = \"manual-batch\", topics = \"myTopic\", containerFactory = \"manualBatchFactory\")\npublic void listen(List<ConsumerRecord<String, Order>> records, Acknowledgment ack) {\n for (ConsumerRecord<String, Order> record : records) {\n try {\n process(record.value());\n }\n catch (Exception e) {\n throw new BatchListenerFailedException(\"Processing failed\", e, record);\n }\n }\n ack.acknowledge();\n}\n----\n\n[[batch-listener-conv-errors]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Offset Commit Behavior", "heading_level": 3, "file_order": 24, "section_index": 10, "content_hash": "00cc11ad11cb67329b2d486911012d0c3b7e87ea36fc829b0947315f02460433", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:291849c0d1b7f0be2921d8ed44f7cb0dbc14b691e6ea79646ac0ba157926c3e8", "content": "Starting with version 2.8, batch listeners can now properly handle conversion errors, when using a `MessageConverter` with a `ByteArrayDeserializer`, a `BytesDeserializer` or a `StringDeserializer`, as well as a `DefaultErrorHandler`.\nWhen a conversion error occurs, the payload is set to null and a deserialization exception is added to the record headers, similar to the `ErrorHandlingDeserializer`.\nA list of ``ConversionException``s is available in the listener so the listener can throw a `BatchListenerFailedException` indicating the first index at which a conversion exception occurred.\n\nExample:\n\n[source, java]\n----\n@KafkaListener(id = \"test\", topics = \"topic\")\nvoid listen(List<Thing> in, @Header(KafkaHeaders.CONVERSION_FAILURES) List<ConversionException> exceptions) {\n for (int i = 0; i < in.size(); i++) {\n Foo foo = in.get(i);\n if (foo == null && exceptions.get(i) != null) {\n throw new BatchListenerFailedException(\"Conversion error\", exceptions.get(i), i);\n }\n process(foo);\n }\n}\n----\n\n[[batch-listener-deser-errors]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Conversion Errors with Batch Error Handlers", "heading_level": 3, "file_order": 24, "section_index": 11, "content_hash": "291849c0d1b7f0be2921d8ed44f7cb0dbc14b691e6ea79646ac0ba157926c3e8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:52e5243f9dbaea115e8401dcd4cc622dd6232bdcf469d27738cb11c5c9875854", "content": "IMPORTANT: Batch listeners require **manual handling** of deserialization errors.\nUnlike record listeners, there is no automatic error handler that detects and routes deserialization failures to the DLT.\nYou must explicitly check for failed records and throw `BatchListenerFailedException`.\n\nUse `ErrorHandlingDeserializer` to prevent deserialization failures from stopping the entire batch:\n\n[source, java]\n----\n@Bean\npublic ConsumerFactory<String, Order> consumerFactory() {\n Map<String, Object> props = new HashMap<>();\n props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n\n // Wrap your deserializer with ErrorHandlingDeserializer\n props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class);\n props.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS, JsonDeserializer.class.getName());\n\n return new DefaultKafkaConsumerFactory<>(props);\n}\n----\n\nIn your listener, you must manually check for `null` values which indicate deserialization failures:\n\n[source, java]\n----\n@KafkaListener(id = \"batch-deser\", topics = \"orders\", containerFactory = \"batchFactory\")\npublic void listen(List<ConsumerRecord<String, Order>> records) {\n for (ConsumerRecord<String, Order> record : records) {\n if (record.value() == null) {\n // Deserialization failed - throw exception to send to DLT\n throw new BatchListenerFailedException(\"Deserialization failed\", record);\n }\n process(record.value());\n }\n}\n----\n\nWhen `DeadLetterPublishingRecoverer` publishes a deserialization failure to the DLT:\n\n* The original `byte[]` data that failed to deserialize is restored as the record value\n* Exception information (class name, message, stacktrace) is added to standard DLT exception headers\n* The original `ErrorHandlingDeserializer` exception header is removed by default (set `setRetainExceptionHeader(true)` on the recoverer to keep it)\n\n[[retrying-batch-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Deserialization Errors with Batch Listeners", "heading_level": 3, "file_order": 24, "section_index": 12, "content_hash": "52e5243f9dbaea115e8401dcd4cc622dd6232bdcf469d27738cb11c5c9875854", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:338e1e7571954a90cd48e023b2b73317138e33f147121372410210b3f9ea911d", "content": "This is now the fallback behavior of the `DefaultErrorHandler` for a batch listener where the listener throws an exception other than a `BatchListenerFailedException`.\n\nThere is no guarantee that, when a batch is redelivered, the batch has the same number of records and/or the redelivered records are in the same order.\nIt is impossible, therefore, to easily maintain retry state for a batch.\nThe `FallbackBatchErrorHandler` takes the following approach.\nIf a batch listener throws an exception that is not a `BatchListenerFailedException`, the retries are performed from the in-memory batch of records.\nIn order to avoid a rebalance during an extended retry sequence, the error handler pauses the consumer, polls it before sleeping for the back off, for each retry, and calls the listener again.\nIf/when retries are exhausted, the `ConsumerRecordRecoverer` is called for each record in the batch.\nIf the recoverer throws an exception, or the thread is interrupted during its sleep, the batch of records will be redelivered on the next poll.\nBefore exiting, regardless of the outcome, the consumer is resumed.\n\nIMPORTANT: This mechanism cannot be used with transactions.\n\nWhile waiting for a `BackOff` interval, the error handler will loop with a short sleep until the desired delay is reached, while checking to see if the container has been stopped, allowing the sleep to exit soon after the `stop()` rather than causing a delay.\n\n[[container-stopping-error-handlers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Retrying Complete Batches", "heading_level": 2, "file_order": 24, "section_index": 13, "content_hash": "338e1e7571954a90cd48e023b2b73317138e33f147121372410210b3f9ea911d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:f07d8c60d9fd88dd25c6a57c3c19924b4398378d551842c432109b0156c598be", "content": "The `CommonContainerStoppingErrorHandler` stops the container if the listener throws an exception.\nFor record listeners, when the `AckMode` is `RECORD`, offsets for already processed records are committed.\nFor record listeners, when the `AckMode` is any manual value, offsets for already acknowledged records are committed.\nFor record listeners, when the `AckMode` is `BATCH`, or for batch listeners, the entire batch is replayed when the container is restarted.\n\nAfter the container stops, an exception that wraps the `ListenerExecutionFailedException` is thrown.\nThis is to cause the transaction to roll back (if transactions are enabled).\n\n[[cond-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Container Stopping Error Handlers", "heading_level": 2, "file_order": 24, "section_index": 14, "content_hash": "f07d8c60d9fd88dd25c6a57c3c19924b4398378d551842c432109b0156c598be", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:70e4295338a3884a6a935734102c8a5d169b63850f6cd8ba2e8229996ffdcf7a", "content": "The `CommonDelegatingErrorHandler` can delegate to different error handlers, depending on the exception type.\nFor example, you may wish to invoke a `DefaultErrorHandler` for most exceptions, or a `CommonContainerStoppingErrorHandler` for others.\n\nAll delegates must share the same compatible properties (`ackAfterHandle`, `seekAfterError` ...).\n\n[[log-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Delegating Error Handler", "heading_level": 2, "file_order": 24, "section_index": 15, "content_hash": "70e4295338a3884a6a935734102c8a5d169b63850f6cd8ba2e8229996ffdcf7a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:365bf0d30e5aad5ab2375b9c5e578e0508afa329a1370172df6ff60d154d5eb4", "content": "The `CommonLoggingErrorHandler` simply logs the exception; with a record listener, the remaining records from the previous poll are passed to the listener.\nFor a batch listener, all the records in the batch are logged.\n\n[[mixed-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Logging Error Handler", "heading_level": 2, "file_order": 24, "section_index": 16, "content_hash": "365bf0d30e5aad5ab2375b9c5e578e0508afa329a1370172df6ff60d154d5eb4", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:ed189b172467c261536a1b5d9c58fb5ccfec6427827443714654ce5eb7934b8b", "content": "If you wish to use a different error handling strategy for record and batch listeners, the `CommonMixedErrorHandler` is provided allowing the configuration of a specific error handler for each listener type.\n\n[[eh-summary]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Using Different Common Error Handlers for Record and Batch Listeners", "heading_level": 2, "file_order": 24, "section_index": 17, "content_hash": "ed189b172467c261536a1b5d9c58fb5ccfec6427827443714654ce5eb7934b8b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:b00a3016a20c7879870bb35c270d8ae3c4fcca816fa9296639981a332847b3ae", "content": "* `DefaultErrorHandler`\n* `CommonContainerStoppingErrorHandler`\n* `CommonDelegatingErrorHandler`\n* `CommonLoggingErrorHandler`\n* `CommonMixedErrorHandler`\n\n[[legacy-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Common Error Handler Summary", "heading_level": 2, "file_order": 24, "section_index": 18, "content_hash": "b00a3016a20c7879870bb35c270d8ae3c4fcca816fa9296639981a332847b3ae", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:d5c26581bcdf397a30b6618bc7b709fdac890d958120229b1627ab9794ba618f", "content": "[cols=\"16,16\" options=\"header\"]\n|===\n|Legacy Error Handler\n|Replacement\n\n|`LoggingErrorHandler`\n|`CommonLoggingErrorHandler`\n\n|`BatchLoggingErrorHandler`\n|`CommonLoggingErrorHandler`\n\n|`ConditionalDelegatingErrorHandler`\n|`DelegatingErrorHandler`\n\n|`ConditionalDelegatingBatchErrorHandler`\n|`DelegatingErrorHandler`\n\n|`ContainerStoppingErrorHandler`\n|`CommonContainerStoppingErrorHandler`\n\n|`ContainerStoppingBatchErrorHandler`\n|`CommonContainerStoppingErrorHandler`\n\n|`SeekToCurrentErrorHandler`\n|`DefaultErrorHandler`\n\n|`SeekToCurrentBatchErrorHandler`\n|No replacement, use `DefaultErrorHandler` with an infinite `BackOff`.\n\n|`RecoveringBatchErrorHandler`\n|`DefaultErrorHandler`\n\n|`RetryingBatchErrorHandler`\n|No replacements, use `DefaultErrorHandler` and throw an exception other than `BatchListenerFailedException`.\n|===\n\n[[migrating-legacy-eh]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Legacy Error Handlers and Their Replacements", "heading_level": 2, "file_order": 24, "section_index": 19, "content_hash": "d5c26581bcdf397a30b6618bc7b709fdac890d958120229b1627ab9794ba618f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:ce3368b35c28d0b5f80e9a7e7f3911928d2de6cc2cc189a67f94aa936cc68229", "content": "Refer to the JavaDocs in `CommonErrorHandler`.\n\nTo replace an `ErrorHandler` or `ConsumerAwareErrorHandler` implementation, you should implement `handleOne()` and leave `seeksAfterHandle()` to return `false` (default).\nYou should also implement `handleOtherException()` to handle exceptions that occur outside the scope of record processing (e.g. consumer errors).\n\nTo replace a `RemainingRecordsErrorHandler` implementation, you should implement `handleRemaining()` and override `seeksAfterHandle()` to return `true` (the error handler must perform the necessary seeks).\nYou should also implement `handleOtherException()` - to handle exceptions that occur outside the scope of record processing (e.g. consumer errors).\n\nTo replace any `BatchErrorHandler` implementation, you should implement `handleBatch()`\nYou should also implement `handleOtherException()` - to handle exceptions that occur outside the scope of record processing (e.g. consumer errors).\n\n[[after-rollback]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Migrating Custom Legacy Error Handler Implementations to `CommonErrorHandler`", "heading_level": 3, "file_order": 24, "section_index": 20, "content_hash": "ce3368b35c28d0b5f80e9a7e7f3911928d2de6cc2cc189a67f94aa936cc68229", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:ec974894cc4979bd60b113541f82ecc22aebb8713de11e511da9c5406124f080", "content": "When using transactions, if the listener throws an exception (and an error handler, if present, throws an exception), the transaction is rolled back.\nBy default, any unprocessed records (including the failed record) are re-fetched on the next poll.\nThis is achieved by performing `seek` operations in the `DefaultAfterRollbackProcessor`.\nWith a batch listener, the entire batch of records is reprocessed (the container has no knowledge of which record in the batch failed).\nTo modify this behavior, you can configure the listener container with a custom `AfterRollbackProcessor`.\nFor example, with a record-based listener, you might want to keep track of the failed record and give up after some number of attempts, perhaps by publishing it to a dead-letter topic.\n\nStarting with version 2.2, the `DefaultAfterRollbackProcessor` can now recover (skip) a record that keeps failing.\nBy default, after ten failures, the failed record is logged (at the `ERROR` level).\nYou can configure the processor with a custom recoverer (`BiConsumer`) and maximum failures.\nSetting the `maxFailures` property to a negative number causes infinite retries.\nThe following example configures recovery after three tries:\n\n[source, java]\n----\nAfterRollbackProcessor<String, String> processor =\n new DefaultAfterRollbackProcessor((record, exception) -> {\n // recover after 3 failures, with no back off - e.g. send to a dead-letter topic\n }, new FixedBackOff(0L, 2L));\n----\n\nWhen you do not use transactions, you can achieve similar functionality by configuring a `DefaultErrorHandler`.\nSee xref:kafka/annotation-error-handling.adoc#error-handlers[Container Error Handlers].\n\nStarting with version 3.2, Recovery can now recover (skip) entire batch of records that keeps failing.\nSet `ContainerProperties.setBatchRecoverAfterRollback(true)` to enable this feature.\n\nIMPORTANT: Default behavior, recovery is not possible with a batch listener, since the framework has no knowledge about which record in the batch keeps failing.\nIn such cases, the application listener must handle a record that keeps failing.\n\nSee also xref:kafka/annotation-error-handling.adoc#dead-letters[Publishing Dead-letter Records].\n\nStarting with version 2.2.5, the `DefaultAfterRollbackProcessor` can be invoked in a new transaction (started after the failed transaction rolls back).\nThen, if you are using the `DeadLetterPublishingRecoverer` to publish a failed record, the processor will send the recovered record's offset in the original topic/partition to the transaction.\nTo enable this feature, set the `commitRecovered` and `kafkaTemplate` properties on the `DefaultAfterRollbackProcessor`.\n\nIMPORTANT: If the recoverer fails (throws an exception), the failed record will be included in the seeks.\nStarting with version 2.5.5, if the recoverer fails, the `BackOff` will be reset by default and redeliveries will again go through the back offs before recovery is attempted again.\nWith earlier versions, the `BackOff` was not reset and recovery was re-attempted on the next failure.\nTo revert to the previous behavior, set the processor's `resetStateOnRecoveryFailure` property to `false`.\n\nStarting with version 2.6, you can now provide the processor with a `BiFunction<ConsumerRecord<?, ?>, Exception, BackOff>` to determine the `BackOff` to use, based on the failed record and/or the exception:\n\n[source, java]\n----\nhandler.setBackOffFunction((record, ex) -> { ... });\n----\n\nIf the function returns `null`, the processor's default `BackOff` will be used.\n\nStarting with version 2.6.3, set `resetStateOnExceptionChange` to `true` and the retry sequence will be restarted (including the selection of a new `BackOff`, if so configured) if the exception type changes between failures.\nBy default, the exception type is not considered.\n\nStarting with version 2.3.1, similar to the `DefaultErrorHandler`, the `DefaultAfterRollbackProcessor` considers certain exceptions to be fatal, and retries are skipped for such exceptions; the recoverer is invoked on the first failure.\nThe exceptions that are considered fatal, by default, are:\n\n* `DeserializationException`\n* `MessageConversionException`\n* `ConversionException`\n* `MethodArgumentResolutionException`\n* `NoSuchMethodException`\n* `ClassCastException`\n\nsince these exceptions are unlikely to be resolved on a retried delivery.\n\nYou can add more exception types to the not-retryable category, or completely replace the map of classified exceptions.\nSee the Javadocs for `DefaultAfterRollbackProcessor.setClassifications()` for more information, as well as `ExceptionMatcher`.\n\nHere is an example that adds `IllegalArgumentException` to the not-retryable exceptions:\n\n[source, java]\n----\n@Bean\npublic DefaultAfterRollbackProcessor errorHandler(BiConsumer<ConsumerRecord<?, ?>, Exception> recoverer) {\n DefaultAfterRollbackProcessor processor = new DefaultAfterRollbackProcessor(recoverer);\n processor.addNotRetryableException(IllegalArgumentException.class);\n return processor;\n}\n----\n\nAlso see xref:kafka/annotation-error-handling.adoc#delivery-header[Delivery Attempts Header].\n\nIMPORTANT: With current `kafka-clients`, the container cannot detect whether a `ProducerFencedException` is caused by a rebalance or if the producer's `transactional.id` has been revoked due to a timeout or expiry.\nBecause, in most cases, it is caused by a rebalance, the container does not call the `AfterRollbackProcessor` (because it's not appropriate to seek the partitions because we no longer are assigned them).\nIf you ensure the timeout is large enough to process each transaction and periodically perform an \"empty\" transaction (e.g. via a `ListenerContainerIdleEvent`) you can avoid fencing due to timeout and expiry.\nOr, you can set the `stopContainerWhenFenced` container property to `true` and the container will stop, avoiding the loss of records.\nYou can consume a `ConsumerStoppedEvent` and check the `Reason` property for `FENCED` to detect this condition.\nSince the event also has a reference to the container, you can restart the container using this event.\n\nStarting with version 2.7, while waiting for a `BackOff` interval, the error handler will loop with a short sleep until the desired delay is reached, while checking to see if the container has been stopped, allowing the sleep to exit soon after the `stop()` rather than causing a delay.\n\nStarting with version 2.7, the processor can be configured with one or more ``RetryListener``s, receiving notifications of retry and recovery progress.\n\n[source, java]\n----\n@FunctionalInterface\npublic interface RetryListener {\n\n void failedDelivery(ConsumerRecord<?, ?> record, Exception ex, int deliveryAttempt);\n\n default void recovered(ConsumerRecord<?, ?> record, Exception ex) {\n }\n\n default void recoveryFailed(ConsumerRecord<?, ?> record, Exception original, Exception failure) {\n }\n\n}\n----\n\nSee the JavaDocs for more information.\n\n[[delivery-header]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "After Rollback Processor", "heading_level": 2, "file_order": 24, "section_index": 21, "content_hash": "ec974894cc4979bd60b113541f82ecc22aebb8713de11e511da9c5406124f080", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:f533571b88a4889334d7ee714038ca2f05b10b3e35b15e563979c236acc2e3c7", "content": "The following applies to record listeners only, not batch listeners.\n\nStarting with version 2.5, when using an `ErrorHandler` or `AfterRollbackProcessor` that implements `DeliveryAttemptAware`, it is possible to enable the addition of the `KafkaHeaders.DELIVERY_ATTEMPT` header (`kafka_deliveryAttempt`) to the record.\nThe value of this header is an incrementing integer starting at 1.\nWhen receiving a raw `ConsumerRecord<?, ?>` the integer is in a `byte[4]`.\n\n[source, java]\n----\nint delivery = ByteBuffer.wrap(record.headers()\n .lastHeader(KafkaHeaders.DELIVERY_ATTEMPT).value())\n .getInt();\n----\n\nWhen using `@KafkaListener` with the `JsonKafkaHeaderMapper` or `SimpleKafkaHeaderMapper`, it can be obtained by adding `@Header(KafkaHeaders.DELIVERY_ATTEMPT) int delivery` as a parameter to the listener method.\n\nTo enable population of this header, set the container property `deliveryAttemptHeader` to `true`.\nIt is disabled by default to avoid the (small) overhead of looking up the state for each record and adding the header.\n\nThe `DefaultErrorHandler` and `DefaultAfterRollbackProcessor` support this feature.\n\n[[delivery-attempts-header-for-batch-listener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Delivery Attempts Header", "heading_level": 2, "file_order": 24, "section_index": 22, "content_hash": "f533571b88a4889334d7ee714038ca2f05b10b3e35b15e563979c236acc2e3c7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:3f9926a2b7a9a0ef48dcef0048e0dce85f8b835a2b3068cc26bd77dd58686966", "content": "When processing `ConsumerRecord` with the `BatchListener`, the `KafkaHeaders.DELIVERY_ATTEMPT` header can be present in a different way compared to `SingleRecordListener`.\n\nStarting with version 3.3, if you want to inject the `KafkaHeaders.DELIVERY_ATTEMPT` header into the `ConsumerRecord` when using the `BatchListener`, set the `DeliveryAttemptAwareRetryListener` as the `RetryListener` in the `ErrorHandler`.\n\nPlease refer to the code below.\n[source, java]\n----\nfinal FixedBackOff fixedBackOff = new FixedBackOff(1, 10);\nfinal DefaultErrorHandler errorHandler = new DefaultErrorHandler(fixedBackOff);\nerrorHandler.setRetryListeners(new DeliveryAttemptAwareRetryListener());\n\nConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();\nfactory.setConsumerFactory(consumerFactory);\nfactory.setCommonErrorHandler(errorHandler);\n----\n\nThen, whenever a batch fails to complete, the `DeliveryAttemptAwareRetryListener` will inject a `KafkaHeaders.DELIVERY_ATTMPT` header into the `ConsumerRecord`.\n\n[[li-header]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Delivery Attempts Header for batch listener", "heading_level": 2, "file_order": 24, "section_index": 23, "content_hash": "3f9926a2b7a9a0ef48dcef0048e0dce85f8b835a2b3068cc26bd77dd58686966", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:19aa34a02069d8276ef7d3fdcfb4db872462e3c083c79346197f44012d3ae7b8", "content": "In some cases, it is useful to be able to know which container a listener is running in.\n\nStarting with version 2.8.4, you can now set the `listenerInfo` property on the listener container, or set the `info` attribute on the `@KafkaListener` annotation.\nThen, the container will add this in the `KafkaListener.LISTENER_INFO` header to all incoming messages; it can then be used in record interceptors, filters, etc., or in the listener itself.\n\n[source, java]\n----\n@KafkaListener(id = \"something\", topics = \"topic\", filter = \"someFilter\",\n info = \"this is the something listener\")\npublic void listen(@Payload Thing thing,\n @Header(KafkaHeaders.LISTENER_INFO) String listenerInfo) {\n ...\n}\n----\n\nWhen used in a `RecordInterceptor` or `RecordFilterStrategy` implementation, the header is in the consumer record as a byte array, converted using the ``KafkaListenerAnnotationBeanPostProcessor``'s `charSet` property.\n\nThe header mappers also convert to `String` when creating `MessageHeaders` from the consumer record and never map this header on an outbound record.\n\nFor POJO batch listeners, starting with version 2.8.6, the header is copied into each member of the batch and is also available as a single `String` parameter after conversion.\n\n[source, java]\n----\n@KafkaListener(id = \"list2\", topics = \"someTopic\", containerFactory = \"batchFactory\",\n info = \"info for batch\")\npublic void listen(List<Thing> list,\n @Header(KafkaHeaders.RECEIVED_KEY) List<Integer> keys,\n @Header(KafkaHeaders.RECEIVED_PARTITION) List<Integer> partitions,\n @Header(KafkaHeaders.RECEIVED_TOPIC) List<String> topics,\n @Header(KafkaHeaders.OFFSET) List<Long> offsets,\n @Header(KafkaHeaders.LISTENER_INFO) String info) {\n ...\n}\n----\n\nNOTE: If the batch listener has a filter and the filter results in an empty batch, you will need to add `required = false` to the `@Header` parameter because the info is not available for an empty batch.\n\nIf you receive `List<Message<Thing>>` the info is in the `KafkaHeaders.LISTENER_INFO` header of each `Message<?>`.\n\nSee xref:kafka/receiving-messages/listener-annotation.adoc#batch-listeners[Batch Listeners] for more information about consuming batches.\n\n[[dead-letters]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Listener Info Header", "heading_level": 2, "file_order": 24, "section_index": 24, "content_hash": "19aa34a02069d8276ef7d3fdcfb4db872462e3c083c79346197f44012d3ae7b8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:2eea77625f8036a027769136c259bf4f501c0b67949c9335ce4005c91c640477", "content": "You can configure the `DefaultErrorHandler` and `DefaultAfterRollbackProcessor` with a record recoverer when the maximum number of failures is reached for a record.\nThe framework provides the `DeadLetterPublishingRecoverer`, which publishes the failed message to another topic.\nThe recoverer requires a `KafkaTemplate<Object, Object>`, which is used to send the record.\nYou can also, optionally, configure it with a `BiFunction<ConsumerRecord<?, ?>, Exception, TopicPartition>`, which is called to resolve the destination topic and partition.\n\nIMPORTANT: By default, the dead-letter record is sent to a topic named `<originalTopic>-dlt` (the original topic name suffixed with `-dlt`) and to the same partition as the original record.\nTherefore, when you use the default resolver, the dead-letter topic **must have at least as many partitions as the original topic.**\n\nIf the returned `TopicPartition` has a negative partition, the partition is not set in the `ProducerRecord`, so the partition is selected by Kafka.\nStarting with version 2.2.4, any `ListenerExecutionFailedException` (thrown, for example, when an exception is detected in a `@KafkaListener` method) is enhanced with the `groupId` property.\nThis allows the destination resolver to use this, in addition to the information in the `ConsumerRecord` to select the dead letter topic.\n\nThe following example shows how to wire a custom destination resolver:\n\n[source, java]\n----\nDeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template,\n (r, e) -> {\n if (e instanceof FooException) {\n return new TopicPartition(r.topic() + \".Foo.failures\", r.partition());\n }\n else {\n return new TopicPartition(r.topic() + \".other.failures\", r.partition());\n }\n });\nCommonErrorHandler errorHandler = new DefaultErrorHandler(recoverer, new FixedBackOff(0L, 2L));\n----\n\nThe record sent to the dead-letter topic is enhanced with the following headers:\n\n* `KafkaHeaders.DLT_EXCEPTION_FQCN`: The Exception class name (generally a `ListenerExecutionFailedException`, but can be others).\n* `KafkaHeaders.DLT_EXCEPTION_CAUSE_FQCN`: The Exception cause class name, if present (since version 2.8).\n* `KafkaHeaders.DLT_EXCEPTION_STACKTRACE`: The Exception stack trace.\n* `KafkaHeaders.DLT_EXCEPTION_MESSAGE`: The Exception message.\n* `KafkaHeaders.DLT_KEY_EXCEPTION_FQCN`: The Exception class name (key deserialization errors only).\n* `KafkaHeaders.DLT_KEY_EXCEPTION_STACKTRACE`: The Exception stack trace (key deserialization errors only).\n* `KafkaHeaders.DLT_KEY_EXCEPTION_MESSAGE`: The Exception message (key deserialization errors only).\n* `KafkaHeaders.DLT_ORIGINAL_TOPIC`: The original topic.\n* `KafkaHeaders.DLT_ORIGINAL_PARTITION`: The original partition.\n* `KafkaHeaders.DLT_ORIGINAL_OFFSET`: The original offset.\n* `KafkaHeaders.DLT_ORIGINAL_TIMESTAMP`: The original timestamp.\n* `KafkaHeaders.DLT_ORIGINAL_TIMESTAMP_TYPE`: The original timestamp type.\n* `KafkaHeaders.DLT_ORIGINAL_CONSUMER_GROUP`: The original consumer group that failed to process the record (since version 2.8).\n\nKey exceptions are only caused by ``DeserializationException``s so there is no `DLT_KEY_EXCEPTION_CAUSE_FQCN`.\n\nThere are two mechanisms to add more headers.\n\n1. Subclass the recoverer and override `createProducerRecord()` - call `super.createProducerRecord()` and add more headers.\n2. Provide a `BiFunction` to receive the consumer record and exception, returning a `Headers` object; headers from there will be copied to the final producer record; also see xref:kafka/annotation-error-handling.adoc#dlpr-headers[Managing Dead Letter Record Headers].\nUse `setHeadersFunction()` to set the `BiFunction`.\n\nThe second is simpler to implement but the first has more information available, including the already assembled standard headers.\n\nStarting with version 2.3, when used in conjunction with an `ErrorHandlingDeserializer`, the publisher will restore the record `value()`, in the dead-letter producer record, to the original value that failed to be deserialized.\nPreviously, the `value()` was null and user code had to decode the `DeserializationException` from the message headers.\nIn addition, you can provide multiple ``KafkaTemplate``s to the publisher; this might be needed, for example, if you want to publish the `byte[]` from a `DeserializationException`, as well as values using a different serializer from records that were deserialized successfully.\nHere is an example of configuring the publisher with ``KafkaTemplate``s that use a `String` and `byte[]` serializer:\n\n[source, java]\n----\n@Bean\npublic DeadLetterPublishingRecoverer publisher(KafkaTemplate<?, ?> stringTemplate,\n KafkaTemplate<?, ?> bytesTemplate) {\n Map<Class<?>, KafkaOperations<?, ?>> templates = new LinkedHashMap<>();\n templates.put(String.class, stringTemplate);\n templates.put(byte[].class, bytesTemplate);\n return new DeadLetterPublishingRecoverer(templates);\n}\n----\n\nThe publisher uses the map keys to locate a template that is suitable for the `value()` about to be published.\nA `LinkedHashMap` is recommended so that the keys are examined in order.\n\nWhen publishing `null` values, and there are multiple templates, the recoverer will look for a template for the `Void` class; if none is present, the first template from the `values().iterator()` will be used.\n\nSince 2.7 you can use the `setFailIfSendResultIsError` method so that an exception is thrown when message publishing fails.\nYou can also set a timeout for the verification of the sender success with `setWaitForSendResultTimeout`.\n\nIMPORTANT: If the recoverer fails (throws an exception), the failed record will be included in the seeks.\nStarting with version 2.5.5, if the recoverer fails, the `BackOff` will be reset by default and redeliveries will again go through the back offs before recovery is attempted again.\nWith earlier versions, the `BackOff` was not reset and recovery was re-attempted on the next failure.\nTo revert to the previous behavior, set the error handler's `resetStateOnRecoveryFailure` property to `false`.\n\nStarting with version 2.6.3, set `resetStateOnExceptionChange` to `true` and the retry sequence will be restarted (including the selection of a new `BackOff`, if so configured) if the exception type changes between failures.\nBy default, the exception type is not considered.\n\nStarting with version 2.3, the recoverer can also be used with Kafka Streams - see xref:streams.adoc#streams-deser-recovery[Recovery from Deserialization Exceptions] for more information.\n\nThe `ErrorHandlingDeserializer` adds the deserialization exception(s) in headers `ErrorHandlingDeserializer.VALUE_DESERIALIZER_EXCEPTION_HEADER` and `ErrorHandlingDeserializer.KEY_DESERIALIZER_EXCEPTION_HEADER` (using Java serialization).\nBy default, these headers are not retained in the message published to the dead letter topic.\nStarting with version 2.7, if both the key and value fail deserialization, the original values of both are populated in the record sent to the DLT.\n\nIf incoming records are dependent on each other, but may arrive out of order, it may be useful to republish a failed record to the tail of the original topic (for some number of times), instead of sending it directly to the dead letter topic.\nSee https://stackoverflow.com/questions/64646996[this Stack Overflow Question] for an example.\n\nThe following error handler configuration will do exactly that:\n\n[source, java]\n----\n@Bean\npublic ErrorHandler eh(KafkaOperations<String, String> template) {\n return new DefaultErrorHandler(new DeadLetterPublishingRecoverer(template,\n (rec, ex) -> {\n org.apache.kafka.common.header.Header retries = rec.headers().lastHeader(\"retries\");\n if (retries == null) {\n retries = new RecordHeader(\"retries\", new byte[] { 1 });\n rec.headers().add(retries);\n }\n else {\n retries.value()[0]++;\n }\n return retries.value()[0] > 5\n ? new TopicPartition(\"topic-dlt\", rec.partition())\n : new TopicPartition(\"topic\", rec.partition());\n }), new FixedBackOff(0L, 0L));\n}\n----\n\nStarting with version 2.7, the recoverer checks that the partition selected by the destination resolver actually exists.\nIf the partition is not present, the partition in the `ProducerRecord` is set to `null`, allowing the `KafkaProducer` to select the partition.\nYou can disable this check by setting the `verifyPartition` property to `false`.\n\nStarting with version 3.1, setting the `logRecoveryRecord` property to `true` will log the recovery record and exception.\n\n[[dlpr-headers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Publishing Dead-letter Records", "heading_level": 2, "file_order": 24, "section_index": 25, "content_hash": "2eea77625f8036a027769136c259bf4f501c0b67949c9335ce4005c91c640477", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:777a3ca68220c3805a8813feef59509732b2b222671234a3f91f51a3e5e04e15", "content": "Referring to xref:kafka/annotation-error-handling.adoc#dead-letters[Publishing Dead-letter Records] above, the `DeadLetterPublishingRecoverer` has two properties used to manage headers when those headers already exist (such as when reprocessing a dead letter record that failed, including when using xref:retrytopic.adoc[Non-Blocking Retries]).\n\n* `appendOriginalHeaders` (default `true`)\n* `stripPreviousExceptionHeaders` (default `true` since version 2.8)\n\nApache Kafka supports multiple headers with the same name; to obtain the \"latest\" value, you can use `headers.lastHeader(headerName)`; to get an iterator over multiple headers, use `headers.headers(headerName).iterator()`.\n\nWhen repeatedly republishing a failed record, these headers can grow (and eventually cause publication to fail due to a `RecordTooLargeException`); this is especially true for the exception headers and particularly for the stack trace headers.\n\nThe reason for the two properties is because, while you might want to retain only the last exception information, you might want to retain the history of which topic(s) the record passed through for each failure.\n\n`appendOriginalHeaders` is applied to all headers named `*ORIGINAL*` while `stripPreviousExceptionHeaders` is applied to all headers named `*EXCEPTION*`.\n\nStarting with version 2.8.4, you now can control which of the standard headers will be added to the output record.\nSee the `enum HeadersToAdd` for the generic names of the (currently) 10 standard headers that are added by default (these are not the actual header names, just an abstraction; the actual header names are set up by the `getHeaderNames()` method which subclasses can override.\n\nTo exclude headers, use the `excludeHeaders()` method; for example, to suppress adding the exception stack trace in a header, use:\n\n[source, java]\n----\nDeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);\nrecoverer.excludeHeaders(HeaderNames.HeadersToAdd.EX_STACKTRACE);\n----\n\nIn addition, you can completely customize the addition of exception headers by adding an `ExceptionHeadersCreator`; this also disables all standard exception headers.\n\n[source, java]\n----\nDeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);\nrecoverer.setExceptionHeadersCreator((kafkaHeaders, exception, isKey, headerNames) -> {\n kafkaHeaders.add(new RecordHeader(..., ...));\n});\n----\n\nAlso starting with version 2.8.4, you can now provide multiple headers functions, via the `addHeadersFunction` method.\nThis allows additional functions to apply, even if another function has already been registered, for example, when using xref:retrytopic.adoc[Non-Blocking Retries].\n\nAlso see xref:retrytopic/features.adoc#retry-headers[Failure Header Management] with xref:retrytopic.adoc[Non-Blocking Retries].\n\n[[exp-backoff]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "Managing Dead Letter Record Headers", "heading_level": 2, "file_order": 24, "section_index": 26, "content_hash": "777a3ca68220c3805a8813feef59509732b2b222671234a3f91f51a3e5e04e15", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:975a83cb035fbd52efdf4a3646ea81192d6755e7be983b1a591d031871ccd3a7", "content": "Spring Framework provides a number of `BackOff` implementations.\nBy default, the `ExponentialBackOff` will retry indefinitely; to give up after some number of retry attempts requires calculating the `maxElapsedTime`.\nSince version 2.7.3, Spring for Apache Kafka provides the `ExponentialBackOffWithMaxRetries` which is a subclass that receives the `maxRetries` property and automatically calculates the `maxElapsedTime`, which is a little more convenient.\n\n[source, java]\n----\n@Bean\nDefaultErrorHandler handler() {\n ExponentialBackOffWithMaxRetries bo = new ExponentialBackOffWithMaxRetries(6);\n bo.setInitialInterval(1_000L);\n bo.setMultiplier(2.0);\n bo.setMaxInterval(10_000L);\n return new DefaultErrorHandler(myRecoverer, bo);\n}\n----\n\nThis will retry after `1, 2, 4, 8, 10, 10` seconds, before calling the recoverer.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc", "title": "annotation-error-handling", "heading": "`ExponentialBackOffWithMaxRetries` Implementation", "heading_level": 2, "file_order": 24, "section_index": 27, "content_hash": "975a83cb035fbd52efdf4a3646ea81192d6755e7be983b1a591d031871ccd3a7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/annotation-error-handling.adoc"}}
{"id": "sha256:073a77da504c331e6cf7af41703554e80bc1620028a1898ba0e432a1bd5daf6f", "content": "[[configuring-topics]]\n\nIf you define a `KafkaAdmin` bean in your application context, it can automatically add topics to the broker.\nTo do so, you can add a `NewTopic` `@Bean` for each topic to the application context.\nVersion 2.3 introduced a new class `TopicBuilder` to make creation of such beans more convenient.\nThe following example shows how to do so:\n\n[tabs]\n======\nJava::\n+\n[source, java, indent=0, role=\"primary\"]\n----\ninclude::{java-examples}/topics/Config.java[tag=topicBeans]\n----\n\nKotlin::\n+\n[source, kotlin, indent=0, role=\"secondary\"]\n----\ninclude::{kotlin-examples}/topics/Config.kt[tag=topicBeans]\n----\n======\n\nStarting with version 2.6, you can omit `partitions()` and/or `replicas()` and the broker defaults will be applied to those properties.\nThe broker version must be at least 2.4.0 to support this feature - see https://cwiki.apache.org/confluence/display/KAFKA/KIP-464%3A+Defaults+for+AdminClient%23createTopic[KIP-464].\n\n[tabs]\n======\nJava::\n+\n[source, java, indent=0, role=\"primary\"]\n----\ninclude::{java-examples}/topics/Config.java[tag=brokerProps]\n----\n\nKotlin::\n+\n[source, kotlin, indent=0, role=\"secondary\"]\n----\ninclude::{kotlin-examples}/topics/Config.kt[tag=brokerProps]\n----\n======\n\nStarting with version 2.7, you can declare multiple ``NewTopic``s in a single `KafkaAdmin.NewTopics` bean definition:\n\n[tabs]\n======\nJava::\n+\n[source, java, indent=0, role=\"primary\"]\n----\ninclude::{java-examples}/topics/Config.java[tag=newTopicsBean]\n----\n\nKotlin::\n+\n[source, kotlin, indent=0, role=\"secondary\"]\n----\ninclude::{kotlin-examples}/topics/Config.kt[tag=newTopicsBean]\n----\n======\n\nIMPORTANT: When using Spring Boot, a `KafkaAdmin` bean is automatically registered so you only need the `NewTopic` (and/or `NewTopics`) ``@Bean``s.\n\nBy default, if the broker is not available, a message is logged, but the context continues to load.\nYou can programmatically invoke the admin's `initialize()` method to try again later.\nIf you wish this condition to be considered fatal, set the admin's `fatalIfBrokerNotAvailable` property to `true`.\nThe context then fails to initialize.\n\nNOTE: If the broker supports it (1.0.0 or higher), the admin increases the number of partitions if it is found that an existing topic has fewer partitions than the `NewTopic.numPartitions`.\n\nStarting with version 2.7, the `KafkaAdmin` provides methods to create and examine topics at runtime.\nStarting with version 4.0, it also provides a method to delete topics.\n\n* `createOrModifyTopics`\n* `describeTopics`\n* `deleteTopics` (since 4.0)\n\nFor more advanced administrative features, you can use the `AdminClient` directly.\nThe following example shows how to do so:\n\n[source, java]\n----\n@Autowired\nprivate KafkaAdmin admin;\n\n...\n\n AdminClient client = AdminClient.create(admin.getConfigurationProperties());\n ...\n client.close();\n----\n\nStarting with versions 2.9.10, 3.0.9, you can provide a `Predicate<NewTopic>` which can be used to determine whether a particular `NewTopic` bean should be considered for creation or modification.\nThis is useful, for example, if you have multiple `KafkaAdmin` instances pointing to different clusters and you wish to select those topics that should be created or modified by each admin.\n\n[source, java]\n----\nadmin.setCreateOrModifyTopic(nt -> !nt.name().equals(\"dontCreateThisOne\"));\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/configuring-topics.adoc", "title": "configuring-topics", "heading": "configuring-topics", "heading_level": 1, "file_order": 25, "section_index": 0, "content_hash": "073a77da504c331e6cf7af41703554e80bc1620028a1898ba0e432a1bd5daf6f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/configuring-topics.adoc"}}
{"id": "sha256:25fa224f76f71d0f6ac59e29c37a256707076cfcde9eb35bf9399bfa378a7aae", "content": "[[connecting]]\n\n* `KafkaAdmin` - see xref:kafka/configuring-topics.adoc[Configuring Topics]\n* `ProducerFactory` - see xref:kafka/sending-messages.adoc[Sending Messages]\n* `ConsumerFactory` - see xref:kafka/receiving-messages.adoc[Receiving Messages]\n\nStarting with version 2.5, each of these extends `KafkaResourceFactory`.\nThis allows changing the bootstrap servers at runtime by adding a `Supplier<String>` to their configuration: `setBootstrapServersSupplier(() +++->+++ ...)`.\nThis will be called for all new connections to get the list of servers.\nConsumers and Producers are generally long-lived.\nTo close existing Producers, call `reset()` on the `DefaultKafkaProducerFactory`.\nTo close existing Consumers, call `stop()` (and then `start()`) on the `KafkaListenerEndpointRegistry` and/or `stop()` and `start()` on any other listener container beans.\n\nFor convenience, the framework also provides an `ABSwitchCluster` which supports two sets of bootstrap servers; one of which is active at any time.\nConfigure the `ABSwitchCluster` and add it to the producer and consumer factories, and the `KafkaAdmin`, by calling `setBootstrapServersSupplier()`.\nWhen you want to switch, call `primary()` or `secondary()` and call `reset()` on the producer factory to establish new connection(s); for consumers, `stop()` and `start()` all listener containers.\nWhen using ``@KafkaListener``s, `stop()` and `start()` the `KafkaListenerEndpointRegistry` bean.\n\nSee the Javadocs for more information.\n\n[[factory-listeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/connecting.adoc", "title": "connecting", "heading": "connecting", "heading_level": 1, "file_order": 26, "section_index": 0, "content_hash": "25fa224f76f71d0f6ac59e29c37a256707076cfcde9eb35bf9399bfa378a7aae", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/connecting.adoc"}}
{"id": "sha256:23cc1b5fe49a63105cce5c62fd0f82494e3845ca96fd163c9610ac18e2dfb860", "content": "Starting with version 2.5, the `DefaultKafkaProducerFactory` and `DefaultKafkaConsumerFactory` can be configured with a `Listener` to receive notifications whenever a producer or consumer is created or closed.\n\n.Producer Factory Listener\n[source, java]\n----\ninterface Listener<K, V> {\n\n default void producerAdded(String id, Producer<K, V> producer) {\n }\n\n default void producerRemoved(String id, Producer<K, V> producer) {\n }\n\n}\n----\n\n.Consumer Factory Listener\n[source, java]\n----\ninterface Listener<K, V> {\n\n default void consumerAdded(String id, Consumer<K, V> consumer) {\n }\n\n default void consumerRemoved(String id, Consumer<K, V> consumer) {\n }\n\n}\n----\n\nIn each case, the `id` is created by appending the `client-id` property (obtained from the `metrics()` after creation) to the factory `beanName` property, separated by `.`.\n\nThese listeners can be used, for example, to create and bind a Micrometer `KafkaClientMetrics` instance when a new client is created (and close it when the client is closed).\n\nThe framework provides listeners that do exactly that; see xref:kafka/micrometer.adoc#micrometer-native[Micrometer Native Metrics].\n\n[[default-client-id-prefixes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/connecting.adoc", "title": "connecting", "heading": "Factory Listeners", "heading_level": 2, "file_order": 26, "section_index": 1, "content_hash": "23cc1b5fe49a63105cce5c62fd0f82494e3845ca96fd163c9610ac18e2dfb860", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/connecting.adoc"}}
{"id": "sha256:c2d10cf0f7af16ad2f8d646efda4e5f7c77046c33fe425fba415ac0eb00f5fbb", "content": "Starting with version 3.2, for Spring Boot applications which define an application name using the `spring.application.name` property, this name is now used\nas a default prefix for auto-generated client IDs for these client types:\n\n- consumer clients which don't use a consumer group\n- producer clients\n- admin clients\n\nThis makes it easier to identify these clients at server side for troubleshooting or applying quotas.\n\n.Example client ids resulting for a Spring Boot application with `spring.application.name=myapp`\n[%autowidth]\n|===\n|Client Type |Without application name |With application name\n\n|consumer without consumer group\n|consumer-null-1\n|myapp-consumer-1\n\n|consumer with consumer group \"mygroup\"\n|consumer-mygroup-1\n|consumer-mygroup-1\n\n|producer\n|producer-1\n|myapp-producer-1\n\n|admin\n|adminclient-1\n|myapp-admin-1\n|===", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/connecting.adoc", "title": "connecting", "heading": "Default client ID prefixes", "heading_level": 2, "file_order": 26, "section_index": 2, "content_hash": "c2d10cf0f7af16ad2f8d646efda4e5f7c77046c33fe425fba415ac0eb00f5fbb", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/connecting.adoc"}}
{"id": "sha256:8936cb6ba545f0d407306e75558701b256c003b164074506fd4d6e8853c50a46", "content": "[[container-factory]]\n\nAs discussed in xref:kafka/receiving-messages/listener-annotation.adoc[`@KafkaListener` Annotation], a `ConcurrentKafkaListenerContainerFactory` is used to create containers for annotated methods.\n\nStarting with version 2.2, you can use the same factory to create any `ConcurrentMessageListenerContainer`.\nThis might be useful if you want to create several containers with similar properties or you wish to use some externally configured factory, such as the one provided by Spring Boot auto-configuration.\nOnce the container is created, you can further modify its properties, many of which are set by using `container.getContainerProperties()`.\nThe following example configures a `ConcurrentMessageListenerContainer`:\n\n[source, java]\n----\n@Bean\npublic ConcurrentMessageListenerContainer<String, String>(\n ConcurrentKafkaListenerContainerFactory<String, String> factory) {\n\n ConcurrentMessageListenerContainer<String, String> container =\n factory.createContainer(\"topic1\", \"topic2\");\n container.setMessageListener(m -> { ... } );\n return container;\n}\n----\n\nIMPORTANT: Containers created this way are not added to the endpoint registry.\nThey should be created as `@Bean` definitions so that they are registered with the application context.\n\nStarting with version 2.3.4, you can add a `ContainerCustomizer` to the factory to further configure each container after it has been created and configured.\n\n[source, java]\n----\n@Bean\npublic KafkaListenerContainerFactory<?> kafkaListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory<Integer, String> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n ...\n factory.setContainerCustomizer(container -> { /* customize the container */ });\n return factory;\n}\n----\n\nStarting with version 3.1, it's also possible to apply the same kind of customization on a single listener by specifying the bean name of a 'ContainerPostProcessor' on the KafkaListener annotation.\n\n[source, java]\n----\n@Bean\npublic ContainerPostProcessor<String, String, AbstractMessageListenerContainer<String, String>> customContainerPostProcessor() {\n return container -> { /* customize the container */ };\n}\n\n...\n\n@KafkaListener(..., containerPostProcessor=\"customContainerPostProcessor\", ...)\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/container-factory.adoc", "title": "container-factory", "heading": "container-factory", "heading_level": 1, "file_order": 27, "section_index": 0, "content_hash": "8936cb6ba545f0d407306e75558701b256c003b164074506fd4d6e8853c50a46", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/container-factory.adoc"}}
{"id": "sha256:ea7eef4f0fd2ec4cc2a31b141cfc21f8d8ee07f0b3bdd395866f2b4d72228f81", "content": "[[container-props]]\n\n.`ContainerProperties` Properties\n[cols=\"13,9,16\", options=\"header\"]\n|===\n| Property\n| Default\n| Description\n\n|[[ackCount]]<<ackCount,`ackCount`>>\n|1\n|The number of records before committing pending offsets when the `ackMode` is `COUNT` or `COUNT_TIME`.\n\n|[[adviceChain]]<<adviceChain,`adviceChain`>>\n|`null`\n|A chain of `Advice` objects (e.g. `MethodInterceptor` around advice) wrapping the message listener, invoked in order.\n\n|[[ackMode]]<<ackMode,`ackMode`>>\n|BATCH\n|Controls how often offsets are committed - see xref:kafka/receiving-messages/message-listener-container.adoc#committing-offsets[Committing Offsets].\n\n|[[ackTime]]<<ackTime,`ackTime`>>\n|5000\n|The time in milliseconds after which pending offsets are committed when the `ackMode` is `TIME` or `COUNT_TIME`.\n\n|[[assignmentCommitOption]]<<assignmentCommitOption,`assignmentCommitOption`>>\n|LATEST_ONLY _NO_TX\n|Whether or not to commit the initial position on assignment; by default, the initial offset will only be committed if the `ConsumerConfig.AUTO_OFFSET_RESET_CONFIG` is `latest` and it won't run in a transaction even if there is a transaction manager present.\nSee the JavaDocs for `ContainerProperties.AssignmentCommitOption` for more information about the available options.\n\n|[[asyncAcks]]<<asyncAcks,`asyncAcks`>>\n|`false`\n|Enable out-of-order commits (see xref:kafka/receiving-messages/ooo-commits.adoc[Manually Committing Offsets]); the consumer is paused and commits are deferred until gaps are filled.\n\n|[[authExceptionRetryInterval]]<<authExceptionRetryInterval,`authExceptionRetryInterval`>>\n|`null`\n|When not null, a `Duration` to sleep between polls when an `AuthenticationException` or `AuthorizationException` is thrown by the Kafka client.\nWhen null, such exceptions are considered fatal and the container will stop.\n\n|[[batchRecoverAfterRollback]]<<batchRecoverAfterRollback,`batchRecoverAfterRollback`>>\n|`false`\n|Set to `true` to enable batch recovery, See xref:kafka/annotation-error-handling.adoc#after-rollback[After Rollback Processor].\n\n|[[clientId]]<<clientId,`clientId`>>\n|(empty string)\n|A prefix for the `client.id` consumer property.\nOverrides the consumer factory `client.id` property; in a concurrent container, `-n` is added as a suffix for each consumer instance.\n\n|[[checkDeserExWhenKeyNull]]<<checkDeserExWhenKeyNull,`checkDeserExWhenKeyNull`>>\n|false\n|Set to `true` to always check for a `DeserializationException` header when a `null` `key` is received.\nUseful when the consumer code cannot determine that an `ErrorHandlingDeserializer` has been configured, such as when using a delegating deserializer.\n\n|[[checkDeserExWhenValueNull]]<<checkDeserExWhenValueNull,`checkDeserExWhenValueNull`>>\n|false\n|Set to `true` to always check for a `DeserializationException` header when a `null` `value` is received.\nUseful when the consumer code cannot determine that an `ErrorHandlingDeserializer` has been configured, such as when using a delegating deserializer.\n\n|[[commitCallback]]<<commitCallback,`commitCallback`>>\n|`null`\n|When present and `syncCommits` is `false` a callback invoked after the commit completes.\n\n|[[commitLogLevel]]<<commitLogLevel,`commitLogLevel`>>\n|DEBUG\n|The logging level for logs pertaining to committing offsets.\n\n|[[consumerRebalanceListener]]<<consumerRebalanceListener,`consumerRebalanceListener`>>\n|`null`\n|A rebalance listener; see xref:kafka/receiving-messages/rebalance-listeners.adoc[Rebalancing Listeners].\n\n|[[commitRetries]]<<commitRetries,`commitRetries`>>\n|3\n|Set the number of retries `RetriableCommitFailedException` when using `syncCommits` set to true.\nDefault 3 (4-attempt total).\n\n|[[consumerStartTimeout]]<<consumerStartTimeout,`consumerStartTimeout`>>\n|30s\n|The time to wait for the consumer to start before logging an error; this might happen if, say, you use a task executor with insufficient threads.\n\n|[[deliveryAttemptHeader]]<<deliveryAttemptHeader,`deliveryAttemptHeader`>>\n|`false`\n|See xref:kafka/annotation-error-handling.adoc#delivery-header[Delivery Attempts Header].\n\n|[[eosMode]]<<eosMode,`eosMode`>>\n|`V2`\n|Exactly Once Semantics mode; see xref:kafka/exactly-once.adoc[Exactly Once Semantics].\n\n|[[fixTxOffsets]]<<fixTxOffsets,`fixTxOffsets`>>\n|`false`\n|When consuming records produced by a transactional producer, and the consumer is positioned at the end of a partition, the lag can incorrectly be reported as greater than zero, due to the pseudo record used to indicate transaction commit/rollback and, possibly, the presence of rolled-back records.\nThis does not functionally affect the consumer but some users have expressed concern that the \"lag\" is non-zero.\nSet this property to `true` and the container will correct such mis-reported offsets.\nThe check is performed before the next poll to avoid adding significant complexity to the commit processing.\nAt the time of writing, the lag will only be corrected if the consumer is configured with `isolation.level=read_committed` and `max.poll.records` is greater than 1.\nSee https://issues.apache.org/jira/browse/KAFKA-10683[KAFKA-10683] for more information.\n\n|[[groupId]]<<groupId,`groupId`>>\n|`null`\n|Overrides the consumer `group.id` property; automatically set by the `@KafkaListener` `id` or `groupId` property.\n\n|[[idleBeforeDataMultiplier]]<<idleBeforeDataMultiplier,`idleBeforeDataMultiplier`>>\n|5.0\n|Multiplier for `idleEventInterval` that is applied before any records are received.\nAfter a record is received, the multiplier is no longer applied.\nAvailable since version 2.8.\n\n|[[idleBetweenPolls]]<<idleBetweenPolls,`idleBetweenPolls`>>\n|0\n|Used to slow down deliveries by sleeping the thread between polls.\nThe time to process a batch of records plus this value must be less than the `max.poll.interval.ms` consumer property.\n\n|[[idleEventInterval]]<<idleEventInterval,`idleEventInterval`>>\n|`null`\n|When set, enables publication of ``ListenerContainerIdleEvent``s, see xref:kafka/events.adoc[Application Events] and xref:kafka/events.adoc#idle-containers[Detecting Idle and Non-Responsive Consumers].\nAlso see `idleBeforeDataMultiplier`.\n\n|[[idlePartitionEventInterval]]<<idlePartitionEventInterval,`idlePartitionEventInterval`>>\n|`null`\n|When set, enables publication of ``ListenerContainerIdlePartitionEvent``s, see xref:kafka/events.adoc[Application Events] and xref:kafka/events.adoc#idle-containers[Detecting Idle and Non-Responsive Consumers].\n\n|[[kafkaConsumerProperties]]<<kafkaConsumerProperties,`kafkaConsumerProperties`>>\n|None\n|Used to override any arbitrary consumer properties configured on the consumer factory.\n\n|[[kafkaAwareTransactionManager]]<<kafkaAwareTransactionManager,`kafkaAwareTransactionManager`>>\n|`null`\n|See xref:kafka/transactions.adoc[Transactions].\n\n|[[listenerTaskExecutor]]<<listenerTaskExecutor,`listenerTaskExecutor`>>\n|`SimpleAsyncTaskExecutor`\n|A task executor to run the consumer threads.\nThe default executor creates threads named `<name>-C-n`; with the `KafkaMessageListenerContainer`, the name is the bean name; with the `ConcurrentMessageListenerContainer` the name is the bean name suffixed with `-m` where `m` is incremented for each child container. See xref:kafka/receiving-messages/container-thread-naming.adoc#container-thread-naming[Container Thread Naming].\n\n|[[logContainerConfig]]<<logContainerConfig,`logContainerConfig`>>\n|`false`\n|Set to `true` to log at INFO level all container properties.\n\n|[[messageListener]]<<messageListener,`messageListener`>>\n|`null`\n|The message listener.\n\n|[[micrometerEnabled]]<<micrometerEnabled,`micrometerEnabled`>>\n|`true`\n|Whether or not to maintain Micrometer timers for the consumer threads.\n\n|[[micrometerTags]]<<micrometerTags,`micrometerTags`>>\n|empty\n|A map of static tags to be added to micrometer metrics.\n\n|[[micrometerTagsProvider]]<<micrometerTagsProvider,`micrometerTagsProvider`>>\n|`null`\n|A function that provides dynamic tags, based on the consumer record.\n\n|[[missingTopicsFatal]]<<missingTopicsFatal,`missingTopicsFatal`>>\n|`false`\n|When true prevents the container from starting if the configured topic(s) are not present on the broker.\n\n|[[monitorInterval]]<<monitorInterval,`monitorInterval`>>\n|30s\n|How often to check the state of the consumer threads for ``NonResponsiveConsumerEvent``s.\nSee `noPollThreshold` and `pollTimeout`.\n\n|[[noPollThreshold]]<<noPollThreshold,`noPollThreshold`>>\n|3.0\n|Multiplied by `pollTimeOut` to determine whether to publish a `NonResponsiveConsumerEvent`.\nSee `monitorInterval`.\n\n|[[observationConvention]]<<observationConvention,`observationConvention`>>\n|`null`\n|When set, add dynamic tags to the timers and traces, based on information in the consumer records.\n\n|[[observationEnabled]]<<observationEnabled,`observationEnabled`>>\n|`false`\n|Set to `true` to enable observation via Micrometer.\n\n|[[offsetAndMetadataProvider]]<<offsetAndMetadataProvider,`offsetAndMetadataProvider`>>\n|`null`\n|A provider for `OffsetAndMetadata`; by default, the provider creates an offset and metadata with empty metadata. The provider gives a way to customize the metadata.\n\n|[[onlyLogRecordMetadata]]<<onlyLogRecordMetadata,`onlyLogRecordMetadata`>>\n|`false`\n|Set to `false` to log the complete consumer record (in error, debug logs etc.) instead of just `topic-partition@offset`.\n\n|[[pauseImmediate]]<<pauseImmediate,`pauseImmediate`>>\n|`false`\n|When the container is paused, stop processing after the current record instead of after processing all the records from the previous poll; the remaining records are retained in memory and will be passed to the listener when the container is resumed.\n\n|[[pollTimeout]]<<pollTimeout,`pollTimeout`>>\n|5000\n|The timeout passed into `Consumer.poll()` in milliseconds.\n\n|[[pollTimeoutWhilePaused]]<<pollTimeoutWhilePaused,`pollTimeoutWhilePaused`>>\n|100\n|The timeout passed into `Consumer.poll()` (in milliseconds) when the container is in a paused state.\n\n|[[restartAfterAuthExceptions]]<<restartAfterAuthExceptions,`restartAfterAuthExceptions`>>\n|false\n|True to restart the container if it is stopped due to authorization/authentication exceptions.\n\n|[[scheduler]]<<scheduler,`scheduler`>>\n|`ThreadPoolTaskScheduler`\n|A scheduler on which to run the consumer monitor task.\n\n|[[shutdownTimeout]]<<shutdownTimeout,`shutdownTimeout`>>\n|10000\n|The maximum time in ms to block the `stop()` method until all consumers stop and before publishing the container stopped event.\n\n|[[stopContainerWhenFenced]]<<stopContainerWhenFenced,`stopContainerWhenFenced`>>\n|`false`\n|Stop the listener container if a `ProducerFencedException` is thrown.\nSee xref:kafka/annotation-error-handling.adoc#after-rollback[After-rollback Processor] for more information.\n\n|[[stopImmediate]]<<stopImmediate,`stopImmediate`>>\n|`false`\n|When the container is stopped, stop processing after the current record instead of after processing all the records from the previous poll.\n\n|[[subBatchPerPartition]]<<subBatchPerPartition,`subBatchPerPartition`>>\n|See desc.\n|When using a batch listener, if this is `true`, the listener is called with the results of the poll split into sub batches, one per partition.\nDefault `false`.\n\n|[[syncCommitTimeout]]<<syncCommitTimeout,`syncCommitTimeout`>>\n|`null`\n|The timeout to use when `syncCommits` is `true`.\nWhen not set, the container will attempt to determine the `default.api.timeout.ms` consumer property and use that; otherwise it will use 60 seconds.\n\n|[[syncCommits]]<<syncCommits,`syncCommits`>>\n|`true`\n|Whether to use sync or async commits for offsets; see `commitCallback`.\n\n|[[topics]]<<topics,`topics` `topicPattern` `topicPartitions`>>\n|n/a\n|The configured topics, topic pattern or explicitly assigned topics/partitions.\nMutually exclusive; at least one must be provided; enforced by `ContainerProperties` constructors.\n\n|[[transactionManager]]<<transactionManager,`transactionManager`>>\n|`null`\n|Deprecated since 3.2, see <<kafkaAwareTransactionManager>>, xref:kafka/transactions.adoc#transaction-synchronization[Other transaction managers].\n|===\n\n[[amlc-props]]\n.`AbstractMessageListenerContainer` Properties\n[cols=\"9,10,16\", options=\"header\"]\n|===\n| Property\n| Default\n| Description\n\n|[[afterRollbackProcessor]]<<afterRollbackProcessor,`afterRollbackProcessor`>>\n|`DefaultAfterRollbackProcessor`\n|An `AfterRollbackProcessor` to invoke after a transaction is rolled back.\n\n|[[applicationEventPublisher]]<<applicationEventPublisher,`applicationEventPublisher`>>\n|application context\n|The event publisher.\n\n|[[batchErrorHandler]]<<batchErrorHandler,`batchErrorHandler`>>\n|See desc.\n|Deprecated - see `commonErrorHandler`.\n\n|[[batchInterceptor]]<<batchInterceptor,`batchInterceptor`>>\n|`null`\n|Set a `BatchInterceptor` to call before invoking the batch listener; does not apply to record listeners.\nAlso see `interceptBeforeTx`.\n\n|[[beanName]]<<beanName,`beanName`>>\n|bean name\n|The bean name of the container; suffixed with `-n` for child containers.\n\n|[[commonErrorHandler]]<<commonErrorHandler,`commonErrorHandler`>>\n|See desc.\n|`DefaultErrorHandler` or `null` when a `transactionManager` is provided when a `DefaultAfterRollbackProcessor` is used.\nSee xref:kafka/annotation-error-handling.adoc#error-handlers[Container Error Handlers].\n\n|[[containerProperties]]<<containerProperties,`containerProperties`>>\n|`ContainerProperties`\n|The container properties instance.\n\n|[[groupId2]]<<groupId2,`groupId`>>\n|See desc.\n|The `containerProperties.groupId`, if present, otherwise the `group.id` property from the consumer factory.\n\n|[[interceptBeforeTx]]<<interceptBeforeTx,`interceptBeforeTx`>>\n|`true`\n|Determines whether the `recordInterceptor` is called before or after a transaction starts.\n\n|[[listenerId]]<<listenerId,`listenerId`>>\n|See desc.\n|The bean name for user-configured containers or the `id` attribute of ``@KafkaListener``s.\n\n|[[listenerInfo]]<<listenerInfo,`listenerInfo`>>\n|null\n|A value to populate in the `KafkaHeaders.LISTENER_INFO` header.\nWith `@KafkaListener`, this value is obtained from the `info` attribute.\nThis header can be used in various places, such as a `RecordInterceptor`, `RecordFilterStrategy` and in the listener code itself.\n\n|[[pauseRequested]]<<pauseRequested,`pauseRequested`>>\n|(read only)\n|True if a consumer pause has been requested.\n\n|[[recordInterceptor]]<<recordInterceptor,`recordInterceptor`>>\n|`null`\n|Set a `RecordInterceptor` to call before invoking the record listener; does not apply to batch listeners.\nAlso see `interceptBeforeTx`.\n\n|[[topicCheckTimeout]]<<topicCheckTimeout,`topicCheckTimeout`>>\n|30s\n|When the `missingTopicsFatal` container property is `true`, how long to wait, in seconds, for the `describeTopics` operation to complete.\n|===\n\n.`KafkaMessageListenerContainer` Properties\n[cols=\"8,3,16\", options=\"header\"]\n|===\n| Property\n| Default\n| Description\n\n|[[assignedPartitions]]<<assignedPartitions,`assignedPartitions`>>\n|(read only)\n|The partitions currently assigned to this container (explicitly or not).\n\n|[[clientIdSuffix]]<<clientIdSuffix,`clientIdSuffix`>>\n|`null`\n|Used by the concurrent container to give each child container's consumer a unique `client.id`.\n\n|[[containerPaused]]<<containerPaused,`containerPaused`>>\n|n/a\n|True if pause has been requested and the consumer has actually paused.\n|===\n\n.`ConcurrentMessageListenerContainer` Properties\n[cols=\"8,3,16\", options=\"header\"]\n|===\n| Property\n| Default\n| Description\n\n|[[alwaysClientIdSuffix]]<<alwaysClientIdSuffix,`alwaysClientIdSuffix`>>\n|`true`\n|Set to false to suppress adding a suffix to the `client.id` consumer property, when the `concurrency` is only 1.\n\n|[[assignedPartitions2]]<<assignedPartitions2,`assignedPartitions`>>\n|(read only)\n|The aggregate of partitions currently assigned to this container's child ``KafkaMessageListenerContainer``s (explicitly or not).\n\n|[[concurrency]]<<concurrency,`concurrency`>>\n|1\n|The number of child ``KafkaMessageListenerContainer``s to manage.\n\n|[[containerPaused2]]<<containerPaused2,`containerPaused`>>\n|n/a\n|True if pause has been requested and all child containers' consumer has actually paused.\n\n|[[containers]]<<containers,`containers`>>\n|n/a\n|A reference to all child ``KafkaMessageListenerContainer``s.\n|===", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/container-props.adoc", "title": "container-props", "heading": "container-props", "heading_level": 1, "file_order": 28, "section_index": 0, "content_hash": "ea7eef4f0fd2ec4cc2a31b141cfc21f8d8ee07f0b3bdd395866f2b4d72228f81", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/container-props.adoc"}}
{"id": "sha256:e7981eaf3c5d801e51abbe6e27f399508fbc172f85e036231c03f4dbb10f6429", "content": "[[dynamic-containers]]\n\nThere are several techniques that can be used to create listener containers at runtime.\nThis section explores some of those techniques.\n\n[[messagelistener-implementations]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/dynamic-containers.adoc", "title": "dynamic-containers", "heading": "dynamic-containers", "heading_level": 1, "file_order": 29, "section_index": 0, "content_hash": "e7981eaf3c5d801e51abbe6e27f399508fbc172f85e036231c03f4dbb10f6429", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/dynamic-containers.adoc"}}
{"id": "sha256:da0a81e89f9a7c95e8bac4cdf5d900c21388c1910b03e50170e89a5f9efa183f", "content": "If you implement your own listener directly, you can simply use the container factory to create a raw container for that listener:\n\n.User Listener\n[tabs]\n======\nJava::\n+\n[source, java, role=\"primary\", indent=0]\n----\ninclude::{java-examples}/dynamic/MyListener.java[tag=listener]\ninclude::{java-examples}/dynamic/Application.java[tag=create]\n----\n\nKotlin::\n+\n[source, kotlin, role=\"secondary\",indent=0]\n----\ninclude::{kotlin-examples}/dynamic/Application.kt[tag=listener]\ninclude::{kotlin-examples}/dynamic/Application.kt[tag=create]\n----\n======\n\n[[prototype-beans]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/dynamic-containers.adoc", "title": "dynamic-containers", "heading": "MessageListener Implementations", "heading_level": 2, "file_order": 29, "section_index": 1, "content_hash": "da0a81e89f9a7c95e8bac4cdf5d900c21388c1910b03e50170e89a5f9efa183f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/dynamic-containers.adoc"}}
{"id": "sha256:8a02802a32fb90f9844ab4b604948faeca9ff3202725a9420ddf674923b72746", "content": "Containers for methods annotated with `@KafkaListener` can be created dynamically by declaring the bean as prototype:\n\n.Prototype\n[tabs]\n======\nJava::\n+\n[source, java, role=\"primary\", indent=0]\n----\ninclude::{java-examples}/dynamic/MyPojo.java[tag=pojo]\ninclude::{java-examples}/dynamic/Application.java[tag=pojoBean]\ninclude::{java-examples}/dynamic/Application.java[tag=getBeans]\n----\n\nKotlin::\n+\n[source, kotlin, role=\"secondary\",indent=0]\n----\ninclude::{kotlin-examples}/dynamic/Application.kt[tag=pojo]\ninclude::{kotlin-examples}/dynamic/Application.kt[tag=pojoBean]\ninclude::{kotlin-examples}/dynamic/Application.kt[tag=getBeans]\n----\n======\n\nIMPORTANT: Listeners must have unique IDs.\nStarting with version 2.8.9, the `KafkaListenerEndpointRegistry` has a new method `unregisterListenerContainer(String id)` to allow you to re-use an id.\nUnregistering a container does not `stop()` the container, you must do that yourself.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/dynamic-containers.adoc", "title": "dynamic-containers", "heading": "Prototype Beans", "heading_level": 2, "file_order": 29, "section_index": 2, "content_hash": "8a02802a32fb90f9844ab4b604948faeca9ff3202725a9420ddf674923b72746", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/dynamic-containers.adoc"}}
{"id": "sha256:bf3dd379d0519958432b9639963723ec7d47f22f6a22f0f3aad3e73059a57583", "content": "[[events]]\n\nThe following Spring application events are published by listener containers and their consumers:\n\n* `ConsumerStartingEvent`: published when a consumer thread is first started, before it starts polling.\n* `ConsumerStartedEvent`: published when a consumer is about to start polling.\n* `ConsumerFailedToStartEvent`: published if no `ConsumerStartingEvent` is published within the `consumerStartTimeout` container property.\nThis event might signal that the configured task executor has insufficient threads to support the containers it is used in and their concurrency.\nAn error message is also logged when this condition occurs.\n* `ListenerContainerIdleEvent`: published when no messages have been received in `idleEventInterval` (if configured).\n* `ListenerContainerNoLongerIdleEvent`: published when a record is consumed after previously publishing a `ListenerContainerIdleEvent`.\n* `ListenerContainerPartitionIdleEvent`: published when no messages have been received from that partition in `idlePartitionEventInterval` (if configured).\n* `ListenerContainerPartitionNoLongerIdleEvent`: published when a record is consumed from a partition that has previously published a `ListenerContainerPartitionIdleEvent`.\n* `NonResponsiveConsumerEvent`: published when the consumer appears to be blocked in the `poll` method.\n* `ConsumerPartitionPausedEvent`: published by each consumer when a partition is paused.\n* `ConsumerPartitionResumedEvent`: published by each consumer when a partition is resumed.\n* `ConsumerPausedEvent`: published by each consumer when the container is paused.\n* `ConsumerResumedEvent`: published by each consumer when the container is resumed.\n* `ConsumerStoppingEvent`: published by each consumer just before stopping.\n* `ConsumerStoppedEvent`: published after the consumer is closed.\nSee xref:kafka/thread-safety.adoc[Thread Safety].\n* `ConsumerRetryAuthEvent`: published when authentication or authorization of a consumer fails and is being retried.\n* `ConsumerRetryAuthSuccessfulEvent`: published when authentication or authorization has been retried successfully. Can only occur when there has been a `ConsumerRetryAuthEvent` before.\n* `ContainerStoppedEvent`: published when all consumers have stopped.\n* `ConcurrentContainerStoppedEvent`: published when the `ConcurrentMessageListenerContainer` has stopped.\n\nIMPORTANT: By default, the application context's event multicaster invokes event listeners on the calling thread.\nIf you change the multicaster to use an async executor, you must not invoke any `Consumer` methods when the event contains a reference to the consumer.\n\nThe `ListenerContainerIdleEvent` has the following properties:\n\n* `source`: The listener container instance that published the event.\n* `container`: The listener container or the parent listener container, if the source container is a child.\n* `id`: The listener ID (or container bean name).\n* `idleTime`: The time the container had been idle when the event was published.\n* `topicPartitions`: The topics and partitions that the container was assigned at the time the event was generated.\n* `consumer`: A reference to the Kafka `Consumer` object.\nFor example, if the consumer's `pause()` method was previously called, it can `resume()` when the event is received.\n* `paused`: Whether the container is currently paused.\nSee xref:kafka/pause-resume.adoc[Pausing and Resuming Listener Containers] for more information.\n\nThe `ListenerContainerNoLongerIdleEvent` has the same properties, except `idleTime` and `paused`.\n\nThe `ListenerContainerPartitionIdleEvent` has the following properties:\n\n* `source`: The listener container instance that published the event.\n* `container`: The listener container or the parent listener container, if the source container is a child.\n* `id`: The listener ID (or container bean name).\n* `idleTime`: The time partition consumption had been idle when the event was published.\n* `topicPartition`: The topic and partition that triggered the event.\n* `consumer`: A reference to the Kafka `Consumer` object.\nFor example, if the consumer's `pause()` method was previously called, it can `resume()` when the event is received.\n* `paused`: Whether that partition consumption is currently paused for that consumer.\nSee xref:kafka/pause-resume.adoc[Pausing and Resuming Listener Containers] for more information.\n\nThe `ListenerContainerPartitionNoLongerIdleEvent` has the same properties, except `idleTime` and `paused`.\n\nThe `NonResponsiveConsumerEvent` has the following properties:\n\n* `source`: The listener container instance that published the event.\n* `container`: The listener container or the parent listener container, if the source container is a child.\n* `id`: The listener ID (or container bean name).\n* `timeSinceLastPoll`: The time just before the container last called `poll()`.\n* `topicPartitions`: The topics and partitions that the container was assigned at the time the event was generated.\n* `consumer`: A reference to the Kafka `Consumer` object.\nFor example, if the consumer's `pause()` method was previously called, it can `resume()` when the event is received.\n* `paused`: Whether the container is currently paused.\nSee xref:kafka/pause-resume.adoc[Pausing and Resuming Listener Containers] for more information.\n\nThe `ConsumerPausedEvent`, `ConsumerResumedEvent`, and `ConsumerStopping` events have the following properties:\n\n* `source`: The listener container instance that published the event.\n* `container`: The listener container or the parent listener container, if the source container is a child.\n* `partitions`: The `TopicPartition` instances involved.\n\nThe `ConsumerPartitionPausedEvent`, `ConsumerPartitionResumedEvent` events have the following properties:\n\n* `source`: The listener container instance that published the event.\n* `container`: The listener container or the parent listener container, if the source container is a child.\n* `partition`: The `TopicPartition` instance involved.\n\nThe `ConsumerRetryAuthEvent` event has the following properties:\n\n* `source`: The listener container instance that published the event.\n* `container`: The listener container or the parent listener container, if the source container is a child.\n* `reason`:\n** `AUTHENTICATION` - the event was published because of an authentication exception.\n** `AUTHORIZATION` - the event was published because of an authorization exception.\n\nThe `ConsumerStartingEvent`, `ConsumerStartedEvent`, `ConsumerFailedToStartEvent`, `ConsumerStoppedEvent`, `ConsumerRetryAuthSuccessfulEvent` and `ContainerStoppedEvent` events have the following properties:\n\n* `source`: The listener container instance that published the event.\n* `container`: The listener container or the parent listener container, if the source container is a child.\n\nAll containers (whether a child or a parent) publish `ContainerStoppedEvent`.\nFor a parent container, the source and container properties are identical.\n\nIn addition, the `ConsumerStoppedEvent` has the following additional property:\n\n* `reason`:\n** `NORMAL` - the consumer stopped normally (container was stopped).\n** `ABNORMAL` - the consumer stopped abnormally (container was stopped abnormally).\n** `ERROR` - a `java.lang.Error` was thrown.\n** `FENCED` - the transactional producer was fenced and the `stopContainerWhenFenced` container property is `true`.\n** `AUTH` - an `AuthenticationException` or `AuthorizationException` was thrown and the `authExceptionRetryInterval` is not configured.\n** `NO_OFFSET` - there is no offset for a partition and the `auto.offset.reset` policy is `none`.\n\nYou can use this event to restart the container after such a condition:\n\n[source, java]\n----\nif (event.getReason().equals(Reason.FENCED)) {\n event.getSource(MessageListenerContainer.class).start();\n}\n----\n\n[[idle-containers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/events.adoc", "title": "events", "heading": "events", "heading_level": 1, "file_order": 30, "section_index": 0, "content_hash": "bf3dd379d0519958432b9639963723ec7d47f22f6a22f0f3aad3e73059a57583", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/events.adoc"}}
{"id": "sha256:439c91a4bb630603c1a5486a56a9866800403e4b766bff4ee79f9be2a2d70ac9", "content": "While efficient, one problem with asynchronous consumers is detecting when they are idle.\nYou might want to take some action if no messages arrive for some period of time.\n\nYou can configure the listener container to publish a `ListenerContainerIdleEvent` when some time passes with no message delivery.\nWhile the container is idle, an event is published every `idleEventInterval` milliseconds.\n\nTo configure this feature, set the `idleEventInterval` on the container.\nThe following example shows how to do so:\n\n[source, java]\n----\n@Bean\npublic KafkaMessageListenerContainer(ConsumerFactory<String, String> consumerFactory) {\n ContainerProperties containerProps = new ContainerProperties(\"topic1\", \"topic2\");\n ...\n containerProps.setIdleEventInterval(60000L);\n ...\n KafkaMessageListenerContainer<String, String> container = new KafKaMessageListenerContainer<>(consumerFactory, containerProps);\n return container;\n}\n----\n\nThe following example shows how to set the `idleEventInterval` for a `@KafkaListener`:\n\n[source, java]\n----\n@Bean\npublic ConcurrentKafkaListenerContainerFactory kafkaListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory<String, String> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n ...\n factory.getContainerProperties().setIdleEventInterval(60000L);\n ...\n return factory;\n}\n----\n\nIn each of these cases, an event is published once per minute while the container is idle.\n\nIf, for some reason, the consumer `poll()` method does not exit, no messages are received and idle events cannot be generated (this was a problem with early versions of the `kafka-clients` when the broker wasn't reachable).\nIn this case, the container publishes a `NonResponsiveConsumerEvent` if a poll does not return within `3x` the `pollTimeout` property.\nBy default, this check is performed once every 30 seconds in each container.\nYou can modify this behavior by setting the `monitorInterval` (default 30 seconds) and `noPollThreshold` (default 3.0) properties in the `ContainerProperties` when configuring the listener container.\nThe `noPollThreshold` should be greater than `1.0` to avoid getting spurious events due to a race condition.\nReceiving such an event lets you stop the containers, thus waking the consumer so that it can stop.\n\nStarting with version 2.6.2, if a container has published a `ListenerContainerIdleEvent`, it will publish a `ListenerContainerNoLongerIdleEvent` when a record is subsequently received.\n\n[[event-consumption]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/events.adoc", "title": "events", "heading": "Detecting Idle and Non-Responsive Consumers", "heading_level": 2, "file_order": 30, "section_index": 1, "content_hash": "439c91a4bb630603c1a5486a56a9866800403e4b766bff4ee79f9be2a2d70ac9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/events.adoc"}}
{"id": "sha256:a64ddcda59e96568e5ace2fd3c2b2f31c749f91a8dd6713bf32448420f69cb49", "content": "You can capture these events by implementing `ApplicationListener` -- either a general listener or one narrowed to only receive this specific event.\nYou can also use `@EventListener`, introduced in Spring Framework 4.2.\n\nThe next example combines `@KafkaListener` and `@EventListener` into a single class.\nYou should understand that the application listener gets events for all containers, so you may need to check the listener ID if you want to take specific action based on which container is idle.\nYou can also use the ``@EventListener``'s `condition` for this purpose.\n\nSee xref:kafka/events.adoc[Application Events] for information about event properties.\n\nThe event is normally published on the consumer thread, so it is safe to interact with the `Consumer` object.\n\nThe following example uses both `@KafkaListener` and `@EventListener`:\n\n[source, java]\n----\npublic class Listener {\n\n @KafkaListener(id = \"qux\", topics = \"annotated\")\n public void listen4(@Payload String foo, Acknowledgment ack) {\n ...\n }\n\n @EventListener(condition = \"event.listenerId.startsWith('qux-')\")\n public void eventHandler(ListenerContainerIdleEvent event) {\n ...\n }\n\n}\n----\n\nIMPORTANT: Event listeners see events for all containers.\nConsequently, in the preceding example, we narrow the events received based on the listener ID.\nSince containers created for the `@KafkaListener` support concurrency, the actual containers are named `id-n` where the `n` is a unique value for each instance to support the concurrency.\nThat is why we use `startsWith` in the condition.\n\nCAUTION: If you wish to use the idle event to stop the listener container, you should not call `container.stop()` on the thread that calls the listener.\nDoing so causes delays and unnecessary log messages.\nInstead, you should hand off the event to a different thread that can then stop the container.\nAlso, you should not `stop()` the container instance if it is a child container.\nYou should stop the concurrent container instead.\n\n[[current-positions-when-idle]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/events.adoc", "title": "events", "heading": "Event Consumption", "heading_level": 2, "file_order": 30, "section_index": 2, "content_hash": "a64ddcda59e96568e5ace2fd3c2b2f31c749f91a8dd6713bf32448420f69cb49", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/events.adoc"}}
{"id": "sha256:a3e808c0620f99bd39625c545dd744c04fc6259dad6076c277c212ee30f7b82f", "content": "Note that you can obtain the current positions when idle is detected by implementing `ConsumerSeekAware` in your listener.\nSee `onIdleContainer()` in xref:kafka/seek.adoc[seek].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/events.adoc", "title": "events", "heading": "Current Positions when Idle", "heading_level": 3, "file_order": 30, "section_index": 3, "content_hash": "a3e808c0620f99bd39625c545dd744c04fc6259dad6076c277c212ee30f7b82f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/events.adoc"}}
{"id": "sha256:fb37dde6a246fa3ec3e517c324ec98128db26675abd0209a6fc70bdfd10498d7", "content": "[[exactly-once]]\n\nYou can provide a listener container with a `KafkaAwareTransactionManager` instance.\nWhen so configured, the container starts a transaction before invoking the listener.\nAny `KafkaTemplate` operations performed by the listener participate in the transaction.\nIf the listener successfully processes the record (or multiple records, when using a `BatchMessageListener`), the container sends the offset(s) to the transaction by using `producer.sendOffsetsToTransaction()`), before the transaction manager commits the transaction.\nIf the listener throws an exception, the transaction is rolled back and the consumer is repositioned so that the rolled-back record(s) can be retrieved on the next poll.\nSee xref:kafka/annotation-error-handling.adoc#after-rollback[After-rollback Processor] for more information and for handling records that repeatedly fail.\n\nUsing transactions enables Exactly Once Semantics (EOS).\n\nThis means that, for a `read -> process -> write` sequence, it is guaranteed that the **sequence** is completed exactly once.\n(The read and process have at least once semantics).\n\nSpring for Apache Kafka version 3.0 and later only supports `EOSMode.V2`:\n\n* `V2` - aka fetch-offset-request fencing (since version 2.5)\n\nIMPORTANT: This requires the brokers to be version 2.5 or later.\n\nWith mode `V2`, it is not necessary to have a producer for each `group.id/topic/partition` because consumer metadata is sent along with the offsets to the transaction and the broker can determine if the producer is fenced using that information instead.\n\nRefer to https://cwiki.apache.org/confluence/display/KAFKA/KIP-447%3A+Producer+scalability+for+exactly+once+semantics[KIP-447] for more information.\n\n`V2` was previously `BETA`; the `EOSMode` has been changed to align the framework with https://cwiki.apache.org/confluence/display/KAFKA/KIP-732%3A+Deprecate+eos-alpha+and+replace+eos-beta+with+eos-v2[KIP-732].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/exactly-once.adoc", "title": "exactly-once", "heading": "exactly-once", "heading_level": 1, "file_order": 31, "section_index": 0, "content_hash": "fb37dde6a246fa3ec3e517c324ec98128db26675abd0209a6fc70bdfd10498d7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/exactly-once.adoc"}}
{"id": "sha256:30d73a4d2146add5c4afc1ce65914c4cd4910423b6037a865b66ffe13a2b04ab", "content": "[[headers]]\n\nThe 0.11.0.0 client introduced support for headers in messages.\nAs of version 2.0, Spring for Apache Kafka now supports mapping these headers to and from `spring-messaging` `MessageHeaders`.\n\nNOTE: Previous versions mapped `ConsumerRecord` and `ProducerRecord` to spring-messaging `Message<?>`, where the value property is mapped to and from the `payload` and other properties (`topic`, `partition`, and so on) were mapped to headers.\nThis is still the case, but additional (arbitrary) headers can now be mapped.\n\nApache Kafka headers have a simple API, shown in the following interface definition:\n\n[source, java]\n----\npublic interface Header {\n\n String key();\n\n byte[] value();\n\n}\n----\n\nThe `KafkaHeaderMapper` strategy is provided to map header entries between Kafka `Headers` and `MessageHeaders`.\nIts interface definition is as follows:\n\n[source, java]\n----\npublic interface KafkaHeaderMapper {\n\n void fromHeaders(MessageHeaders headers, Headers target);\n\n void toHeaders(Headers source, Map<String, Object> target);\n\n}\n----\n\nThe `SimpleKafkaHeaderMapper` maps raw headers as `byte[]`, with configuration options for conversion to `String` values.\n\nThe `JsonKafkaHeaderMapper` maps the key to the `MessageHeaders` header name and, in order to support rich header types for outbound messages, JSON conversion is performed.\nA +++\"+++`special`+++\"+++ header (with a key of `spring_json_header_types`) contains a JSON map of `<key>:<type>`.\nThis header is used on the inbound side to provide appropriate conversion of each header value to the original type.\n\nOn the inbound side, all Kafka `Header` instances are mapped to `MessageHeaders`.\nOn the outbound side, by default, all `MessageHeaders` are mapped, except `id`, `timestamp`, and the headers that map to `ConsumerRecord` properties.\n\nYou can specify which headers are to be mapped for outbound messages, by providing patterns to the mapper.\nThe following listing shows a number of example mappings:\n\n[source, java]\n----\npublic JsonKafkaHeaderMapper() { <1>\n ...\n}\n\npublic JsonKafkaHeaderMapper(ObjectMapper objectMapper) { <2>\n ...\n}\n\npublic JsonKafkaHeaderMapper(String... patterns) { <3>\n ...\n}\n\npublic JsonKafkaHeaderMapper(ObjectMapper objectMapper, String... patterns) { <4>\n ...\n}\n----\n\n<1> Uses a default Jackson `ObjectMapper` and maps most headers, as discussed before the example.\n<2> Uses the provided Jackson `ObjectMapper` and maps most headers, as discussed before the example.\n<3> Uses a default Jackson `ObjectMapper` and maps headers according to the provided patterns.\n<4> Uses the provided Jackson `ObjectMapper` and maps headers according to the provided patterns.\n\nPatterns are rather simple and can contain a leading wildcard (`+++*+++`), a trailing wildcard, or both (for example, `+++*+++.cat.+++*+++`).\nYou can negate patterns with a leading `!`.\nThe first pattern that matches a header name (whether positive or negative) wins.\n\nWhen you provide your own patterns, we recommend including `!id` and `!timestamp`, since these headers are read-only on the inbound side.\n\nIMPORTANT: By default, the mapper deserializes only classes in `java.lang` and `java.util`.\nYou can trust other (or all) packages by adding trusted packages with the `addTrustedPackages` method.\nIf you receive messages from untrusted sources, you may wish to add only those packages you trust.\nTo trust all packages, you can use `mapper.addTrustedPackages(\"+++*+++\")`.\n\nNOTE: Mapping `String` header values in a raw form is useful when communicating with systems that are not aware of the mapper's JSON format.\n\nStarting with version 2.2.5, you can specify that certain string-valued headers should not be mapped using JSON, but to/from a raw `byte[]`.\nThe `AbstractKafkaHeaderMapper` has new properties; `mapAllStringsOut` when set to true, all string-valued headers will be converted to `byte[]` using the `charset` property (default `UTF-8`).\nIn addition, there is a property `rawMappedHeaders`, which is a map of `header name : boolean`; if the map contains a header name, and the header contains a `String` value, it will be mapped as a raw `byte[]` using the charset.\nThis map is also used to map raw incoming `byte[]` headers to `String` using the charset if, and only if, the boolean in the map value is `true`.\nIf the boolean is `false`, or the header name is not in the map with a `true` value, the incoming header is simply mapped as the raw unmapped header.\n\nThe following test case illustrates this mechanism.\n\n[source, java]\n----\n@Test\npublic void testSpecificStringConvert() {\n JsonKafkaHeaderMapper mapper = new JsonKafkaHeaderMapper();\n Map<String, Boolean> rawMappedHeaders = new HashMap<>();\n rawMappedHeaders.put(\"thisOnesAString\", true);\n rawMappedHeaders.put(\"thisOnesBytes\", false);\n mapper.setRawMappedHeaders(rawMappedHeaders);\n Map<String, Object> headersMap = new HashMap<>();\n headersMap.put(\"thisOnesAString\", \"thing1\");\n headersMap.put(\"thisOnesBytes\", \"thing2\");\n headersMap.put(\"alwaysRaw\", \"thing3\".getBytes());\n MessageHeaders headers = new MessageHeaders(headersMap);\n Headers target = new RecordHeaders();\n mapper.fromHeaders(headers, target);\n assertThat(target).containsExactlyInAnyOrder(\n new RecordHeader(\"thisOnesAString\", \"thing1\".getBytes()),\n new RecordHeader(\"thisOnesBytes\", \"thing2\".getBytes()),\n new RecordHeader(\"alwaysRaw\", \"thing3\".getBytes()));\n headersMap.clear();\n mapper.toHeaders(target, headersMap);\n assertThat(headersMap).contains(\n entry(\"thisOnesAString\", \"thing1\"),\n entry(\"thisOnesBytes\", \"thing2\".getBytes()),\n entry(\"alwaysRaw\", \"thing3\".getBytes()));\n}\n----\n\nBoth header mappers map all inbound headers, by default.\nStarting with version 2.8.8, the patterns, can also applied to inbound mapping.\nTo create a mapper for inbound mapping, use one of the static methods on the respective mapper:\n\n[source, java]\n----\npublic static JsonKafkaHeaderMapper forInboundOnlyWithMatchers(String... patterns) {\n}\n\npublic static JsonKafkaHeaderMapper forInboundOnlyWithMatchers(ObjectMapper objectMapper, String... patterns) {\n}\n\npublic static SimpleKafkaHeaderMapper forInboundOnlyWithMatchers(String... patterns) {\n}\n----\n\nFor example:\n\n[source, java]\n----\nJsonKafkaHeaderMapper inboundMapper = JsonKafkaHeaderMapper.forInboundOnlyWithMatchers(\"!abc*\", \"*\");\n----\n\nThis will exclude all headers beginning with `abc` and include all others.\n\nBy default, the `JsonKafkaHeaderMapper` is used in the `MessagingMessageConverter` and `BatchMessagingMessageConverter`, as long as Jackson is on the classpath.\n\nWith the batch converter, the converted headers are available in the `KafkaHeaders.BATCH_CONVERTED_HEADERS` as a `List<Map<String, Object>>` where the map in a position of the list corresponds to the data position in the payload.\n\nIf there is no converter (either because Jackson is not present or it is explicitly set to `null`), the headers from the consumer record are provided unconverted in the `KafkaHeaders.NATIVE_HEADERS` header.\nThis header is a `Headers` object (or a `List<Headers>` in the case of the batch converter), where the position in the list corresponds to the data position in the payload.\n\nIMPORTANT: Certain types are not suitable for JSON serialization, and a simple `toString()` serialization might be preferred for these types.\nThe `JsonKafkaHeaderMapper` has a method called `addToStringClasses()` that lets you supply the names of classes that should be treated this way for outbound mapping.\nDuring inbound mapping, they are mapped as `String`.\nBy default, only `org.springframework.util.MimeType` and `org.springframework.http.MediaType` are mapped this way.\n\nNOTE: Starting with version 2.3, handling of String-valued headers is simplified.\nSuch headers are no longer JSON encoded, by default (i.e. they do not have enclosing `\"+++...+++\"` added).\nThe type is still added to the JSON_TYPES header so the receiving system can convert back to a String (from `byte[]`).\nThe mapper can handle (decode) headers produced by older versions (it checks for a leading `+++\"+++`); in this way an application using 2.3 can consume records from older versions.\n\nIMPORTANT: To be compatible with earlier versions, set `encodeStrings` to `true`, if records produced by a version using 2.3 might be consumed by applications using earlier versions.\nWhen all applications are using 2.3 or higher, you can leave the property at its default value of `false`.\n\n[source, java]\n----\n@Bean\nMessagingMessageConverter converter() {\n MessagingMessageConverter converter = new MessagingMessageConverter();\n JsonKafkaHeaderMapper mapper = new JsonKafkaHeaderMapper();\n mapper.setEncodeStrings(true);\n converter.setHeaderMapper(mapper);\n return converter;\n}\n----\n\nIf using Spring Boot, it will auto configure this converter bean into the auto-configured `KafkaTemplate`; otherwise you should add this converter to the template.\n\n[[multi-value-header]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/headers.adoc", "title": "headers", "heading": "headers", "heading_level": 1, "file_order": 32, "section_index": 0, "content_hash": "30d73a4d2146add5c4afc1ce65914c4cd4910423b6037a865b66ffe13a2b04ab", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/headers.adoc"}}
{"id": "sha256:34455c39b366d3ef56d216a104adf3c6f43ef24c8580946e14435bbd33fdf4af", "content": "Starting with 4.0, multi-value header mapping is supported, where the same logical header key appears more than once in a Kafka record.\n\nBy default, the `HeaderMapper` does **not** create multiple Kafka headers with the same name.\nInstead, when it encounters a collection value (e.g., a `List<byte[]>`), it serializes the entire collection into **one** Kafka header whose value is a JSON array.\n\n* **Producer side:** `JsonKafkaHeaderMapper` writes the JSON bytes, while `SimpleKafkaHeaderMapper` ignore it.\n* **Consumer side:** the mapper exposes the header as a single valueâ€”the **last occurrence wins**; earlier duplicates are silently discarded.\n\nPreserving each individual header requires explicit registration of patterns that designate the header as multiâ€‘valued.\n\n`JsonKafkaHeaderMapper#setMultiValueHeaderPatterns(String... patterns)` accepts a list of patterns, which can be either wildcard expressions or exact header names.\n\n[source, java]\n----\nJsonKafkaHeaderMapper mapper = new JsonKafkaHeaderMapper();\n\nmapper.setMultiValueHeaderPatterns(\"test-multi-value1\", \"test-multi-value2\");\n\nmapper.setMultiValueHeaderPatterns(\"test-multi-*\");\n----\n\nAny header whose name matches one of the supplied patterns is\n\n* **Producer side:** written as separate Kafka headers, one per element.\n* **Consumer side:** collected into a `List<?>` that contains the individual header values; each element is returned to the application **after the usual deserialization or type conversion performed by the configured `HeaderMapper`.**\n\nNOTE: Regular expressions are *not* supported; only the +*+ wildcard is allowed in simple patternsâ€”supporting direct equality and forms such as: +xxx*+, +*xxx+, +*xxx*+, +xxx*yyy+.\n\n[IMPORTANT]\n====\nOn the *Producer Side*, When `JsonKafkaHeaderMapper` serializes a multi-value header, every element in that collection must be of a single Java typeâ€”mixing, for example, `String` and `byte[]` values under a single header key will lead to a conversion error.\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/headers.adoc", "title": "headers", "heading": "Support multi-value header mapping", "heading_level": 2, "file_order": 32, "section_index": 1, "content_hash": "34455c39b366d3ef56d216a104adf3c6f43ef24c8580946e14435bbd33fdf4af", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/headers.adoc"}}
{"id": "sha256:215635f6f6a681b71bb88aefb776f52ab85371ffc329bc743a8276d052b129ae", "content": "[[interceptors]]\n\nApache Kafka provides a mechanism to add interceptors to producers and consumers.\nThese objects are managed by Kafka, not Spring, and so normal Spring dependency injection won't work for wiring in dependent Spring Beans.\nHowever, you can manually wire in those dependencies using the interceptor `config()` method.\nThe following Spring Boot application shows how to do this by overriding Spring Boot's default factories to add some dependent bean into the configuration properties.\n\n[source, java]\n----\n@SpringBootApplication\npublic class Application {\n\n public static void main(String[] args) {\n SpringApplication.run(Application.class, args);\n }\n\n @Bean\n public ConsumerFactory<?, ?> kafkaConsumerFactory(SomeBean someBean) {\n Map<String, Object> consumerProperties = new HashMap<>();\n // consumerProperties.put(..., ...)\n // ...\n consumerProperties.put(ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, MyConsumerInterceptor.class.getName());\n consumerProperties.put(\"some.bean\", someBean);\n return new DefaultKafkaConsumerFactory<>(consumerProperties);\n }\n\n @Bean\n public ProducerFactory<?, ?> kafkaProducerFactory(SomeBean someBean) {\n Map<String, Object> producerProperties = new HashMap<>();\n // producerProperties.put(..., ...)\n // ...\n producerProperties.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, MyProducerInterceptor.class.getName());\n producerProperties.put(\"some.bean\", someBean);\n return new DefaultKafkaProducerFactory<>(producerProperties);\n }\n\n @Bean\n public SomeBean someBean() {\n return new SomeBean();\n }\n\n @KafkaListener(id = \"kgk897\", topics = \"kgh897\")\n public void listen(String in) {\n System.out.println(\"Received \" + in);\n }\n\n @Bean\n public ApplicationRunner runner(KafkaTemplate<String, String> template) {\n return args -> template.send(\"kgh897\", \"test\");\n }\n\n @Bean\n public NewTopic kRequests() {\n return TopicBuilder.name(\"kgh897\")\n .partitions(1)\n .replicas(1)\n .build();\n }\n\n}\n----\n\n[source, java]\n----\npublic class SomeBean {\n\n public void someMethod(String what) {\n System.out.println(what + \" in my foo bean\");\n }\n\n}\n----\n[source, java]\n----\npublic class MyProducerInterceptor implements ProducerInterceptor<String, String> {\n\n private SomeBean bean;\n\n @Override\n public void configure(Map<String, ?> configs) {\n this.bean = (SomeBean) configs.get(\"some.bean\");\n }\n\n @Override\n public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {\n this.bean.someMethod(\"producer interceptor\");\n return record;\n }\n\n @Override\n public void onAcknowledgement(RecordMetadata metadata, Exception exception) {\n }\n\n @Override\n public void close() {\n }\n\n}\n----\n[source, java]\n----\npublic class MyConsumerInterceptor implements ConsumerInterceptor<String, String> {\n\n private SomeBean bean;\n\n @Override\n public void configure(Map<String, ?> configs) {\n this.bean = (SomeBean) configs.get(\"some.bean\");\n }\n\n @Override\n public ConsumerRecords<String, String> onConsume(ConsumerRecords<String, String> records) {\n this.bean.someMethod(\"consumer interceptor\");\n return records;\n }\n\n @Override\n public void onCommit(Map<TopicPartition, OffsetAndMetadata> offsets) {\n }\n\n @Override\n public void close() {\n }\n\n}\n----\n\nResult:\n\n[source]\n----\nproducer interceptor in my foo bean\nconsumer interceptor in my foo bean\nReceived test\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/interceptors.adoc", "title": "interceptors", "heading": "interceptors", "heading_level": 1, "file_order": 33, "section_index": 0, "content_hash": "215635f6f6a681b71bb88aefb776f52ab85371ffc329bc743a8276d052b129ae", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/interceptors.adoc"}}
{"id": "sha256:cb2c6fc15659fb951f3abb977c6f3845383cbefd0e64277ecf188b5f0b2dd1ae", "content": "[[kafka-queues]]\n\nStarting with version 4.0, Spring for Apache Kafka provides support for Kafka Queues through share consumers, which are part of Apache Kafka 4.0.0 and implement https://cwiki.apache.org/confluence/display/KAFKA/KIP-932%3A+Queues+for+Kafka[KIP-932 (Queues for Kafka)].\nThis feature is currently in early access.\n\nKafka Queues enable a different consumption model compared to traditional consumer groups.\nInstead of the partition-based assignment model where each partition is exclusively assigned to one consumer, share consumers can cooperatively consume from the same partitions, with records being distributed among the consumers in the share group.\n\n[[share-consumer-factory]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "kafka-queues", "heading_level": 1, "file_order": 34, "section_index": 0, "content_hash": "cb2c6fc15659fb951f3abb977c6f3845383cbefd0e64277ecf188b5f0b2dd1ae", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:147921cc345b3df9b2526f1e252b5030ac535119d64f76dd5f49d83afd37a14f", "content": "The `ShareConsumerFactory` is responsible for creating share consumer instances.\nSpring Kafka provides the `DefaultShareConsumerFactory` implementation.\n\n[[share-consumer-factory-configuration]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Share Consumer Factory", "heading_level": 2, "file_order": 34, "section_index": 1, "content_hash": "147921cc345b3df9b2526f1e252b5030ac535119d64f76dd5f49d83afd37a14f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:903d0545510c879912a81ec350f86632ac7906ba7db151dc71a05b0b48b62272", "content": "You can configure a `DefaultShareConsumerFactory` similar to how you configure a regular `ConsumerFactory`:\n\n[source,java]\n----\n@Bean\npublic ShareConsumerFactory<String, String> shareConsumerFactory() {\n Map<String, Object> props = new HashMap<>();\n props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n props.put(ConsumerConfig.GROUP_ID_CONFIG, \"my-share-group\");\n props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n return new DefaultShareConsumerFactory<>(props);\n}\n----\n\n[[share-consumer-factory-constructors]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Configuration", "heading_level": 3, "file_order": 34, "section_index": 2, "content_hash": "903d0545510c879912a81ec350f86632ac7906ba7db151dc71a05b0b48b62272", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:ea10898dee1b615b7d8fd8d531fafd06f21bae8ba049a6b9dde42231c09f1881", "content": "The `DefaultShareConsumerFactory` provides several constructor options:\n\n[source,java]\n----\nnew DefaultShareConsumerFactory<>(configs);\n\nnew DefaultShareConsumerFactory<>(configs, keyDeserializerSupplier, valueDeserializerSupplier);\n\nnew DefaultShareConsumerFactory<>(configs, keyDeserializer, valueDeserializer, configureDeserializers);\n----\n\n[[share-consumer-factory-deserializers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Constructor Options", "heading_level": 3, "file_order": 34, "section_index": 3, "content_hash": "ea10898dee1b615b7d8fd8d531fafd06f21bae8ba049a6b9dde42231c09f1881", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:07003c176e14d98461c4c046abfa8b959c87cd6b925d5053b03dccf8ee860703", "content": "You can configure deserializers in several ways:\n\n1. **Via Configuration Properties** (recommended for simple cases):\n+\n[source,java]\n----\nprops.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\nprops.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n----\n\n2. **Via Setters**:\n+\n[source,java]\n----\nfactory.setKeyDeserializer(new StringDeserializer());\nfactory.setValueDeserializer(new StringDeserializer());\n----\n\n3. **Via Suppliers** (for cases where deserializers need to be created per consumer):\n+\n[source,java]\n----\nfactory.setKeyDeserializerSupplier(() -> new StringDeserializer());\nfactory.setValueDeserializerSupplier(() -> new StringDeserializer());\n----\n\nSet `configureDeserializers` to `false` if your deserializers are already fully configured and should not be reconfigured by the factory.\n\n[[share-consumer-factory-listeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Deserializer Configuration", "heading_level": 3, "file_order": 34, "section_index": 4, "content_hash": "07003c176e14d98461c4c046abfa8b959c87cd6b925d5053b03dccf8ee860703", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:7764d96133833f3e9bffd5fd53d362c3fb90bb55ae5fe0aa5e722ebb1b667858", "content": "You can add listeners to monitor the lifecycle of share consumers:\n\n[source,java]\n----\nfactory.addListener(new ShareConsumerFactory.Listener<String, String>() {\n @Override\n public void consumerAdded(String id, ShareConsumer<String, String> consumer) {\n // Called when a new consumer is created\n System.out.println(\"Consumer added: \" + id);\n }\n\n @Override\n public void consumerRemoved(String id, ShareConsumer<String, String> consumer) {\n // Called when a consumer is closed\n System.out.println(\"Consumer removed: \" + id);\n }\n});\n----\n\n[[share-message-listener-containers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Lifecycle Listeners", "heading_level": 3, "file_order": 34, "section_index": 5, "content_hash": "7764d96133833f3e9bffd5fd53d362c3fb90bb55ae5fe0aa5e722ebb1b667858", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:502b2630cecf533447a2ca58e11600a04cc48764b29585ccb8809d5bf7d77795", "content": "[[share-kafka-message-listener-container]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Share Message Listener Containers", "heading_level": 2, "file_order": 34, "section_index": 6, "content_hash": "502b2630cecf533447a2ca58e11600a04cc48764b29585ccb8809d5bf7d77795", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:77e6ee957b6a538abe17e103c04717f260193da850b50138f296520fec43f7fa", "content": "The `ShareKafkaMessageListenerContainer` provides a container for share consumers with support for concurrent processing:\n\n[source,java]\n----\n@Bean\npublic ShareKafkaMessageListenerContainer<String, String> container(\n ShareConsumerFactory<String, String> shareConsumerFactory) {\n\n ContainerProperties containerProps = new ContainerProperties(\"my-topic\");\n containerProps.setGroupId(\"my-share-group\");\n\n ShareKafkaMessageListenerContainer<String, String> container =\n new ShareKafkaMessageListenerContainer<>(shareConsumerFactory, containerProps);\n\n container.setupMessageListener(new MessageListener<String, String>() {\n @Override\n public void onMessage(ConsumerRecord<String, String> record) {\n System.out.println(\"Received: \" + record.value());\n }\n });\n\n return container;\n}\n----\n\n[[share-container-properties]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "ShareKafkaMessageListenerContainer", "heading_level": 3, "file_order": 34, "section_index": 7, "content_hash": "77e6ee957b6a538abe17e103c04717f260193da850b50138f296520fec43f7fa", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:58616aa9388b92d8eb53b5070c2ecc1583cd5a13ad4f610b7a738c51800d8e0f", "content": "Share containers support a subset of the container properties available for regular consumers:\n\n* `topics`: Array of topic names to subscribe to\n* `groupId`: The share group ID\n* `clientId`: The client ID for the consumer\n* `kafkaConsumerProperties`: Additional consumer properties\n\n[IMPORTANT]\n====\nShare consumers do not support:\n\n* Explicit partition assignment (`TopicPartitionOffset`)\n* Topic patterns\n* Manual offset management\n====\n\n[[share-container-concurrency]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Container Properties", "heading_level": 3, "file_order": 34, "section_index": 8, "content_hash": "58616aa9388b92d8eb53b5070c2ecc1583cd5a13ad4f610b7a738c51800d8e0f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:e0a72a78dec1a3f20030b5054afbe9bfcc65b49312b819fa17afad22c4e3ab8e", "content": "The `ShareKafkaMessageListenerContainer` supports concurrent processing by creating multiple consumer threads within a single container.\nEach thread runs its own `ShareConsumer` instance that participates in the same share group.\n\nUnlike traditional consumer groups where concurrency involves partition distribution, share consumers leverage Kafka's record-level distribution at the broker.\nThis means multiple consumer threads in the same container work together as part of the share group, with the Kafka broker distributing records across all consumer instances.\n\n[IMPORTANT]\n====\n**Concurrency is Additive Across Application Instances**\n\nFrom the share group's perspective, each `ShareConsumer` instance is an independent member, regardless of where it runs.\nSetting `concurrency=3` in a single container creates 3 share group members.\nIf you run multiple application instances with the same share group ID, all their consumer threads combine into one pool.\n\nFor example:\n* Application Instance 1: `concurrency=3` â†’ 3 share group members\n* Application Instance 2: `concurrency=3` â†’ 3 share group members\n* **Total**: 6 share group members available for the broker to distribute records to\n\nThis means setting `concurrency=5` in a single container is operationally equivalent to running 5 separate application instances with `concurrency=1` each (all using the same `group.id`).\nThe Kafka broker treats all consumer instances equally and distributes records across the entire pool.\n====", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Concurrency", "heading_level": 3, "file_order": 34, "section_index": 9, "content_hash": "e0a72a78dec1a3f20030b5054afbe9bfcc65b49312b819fa17afad22c4e3ab8e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:847c116a2fba65f0b6a7a504ba26aa427050470dc5c374f18adce3c748e09a15", "content": "[source,java]\n----\n@Bean\npublic ShareKafkaMessageListenerContainer<String, String> concurrentContainer(\n ShareConsumerFactory<String, String> shareConsumerFactory) {\n\n ContainerProperties containerProps = new ContainerProperties(\"my-topic\");\n containerProps.setGroupId(\"my-share-group\");\n\n ShareKafkaMessageListenerContainer<String, String> container =\n new ShareKafkaMessageListenerContainer<>(shareConsumerFactory, containerProps);\n\n // Set concurrency to create 5 consumer threads\n container.setConcurrency(5);\n\n container.setupMessageListener(new MessageListener<String, String>() {\n @Override\n public void onMessage(ConsumerRecord<String, String> record) {\n System.out.println(\"Received on \" + Thread.currentThread().getName() + \": \" + record.value());\n }\n });\n\n return container;\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Configuring Concurrency Programmatically", "heading_level": 4, "file_order": 34, "section_index": 10, "content_hash": "847c116a2fba65f0b6a7a504ba26aa427050470dc5c374f18adce3c748e09a15", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:d9314b535af14bb6e3fef4bac02c10012d703c313ab1abbc7aa91c528adba320", "content": "You can set default concurrency at the factory level, which applies to all containers created by that factory:\n\n[source,java]\n----\n@Bean\npublic ShareKafkaListenerContainerFactory<String, String> shareKafkaListenerContainerFactory(\n ShareConsumerFactory<String, String> shareConsumerFactory) {\n\n ShareKafkaListenerContainerFactory<String, String> factory =\n new ShareKafkaListenerContainerFactory<>(shareConsumerFactory);\n\n // Set default concurrency for all containers created by this factory\n factory.setConcurrency(3);\n\n return factory;\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Configuring Concurrency via Factory", "heading_level": 4, "file_order": 34, "section_index": 11, "content_hash": "d9314b535af14bb6e3fef4bac02c10012d703c313ab1abbc7aa91c528adba320", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:01f9d01712026d096fffb5f8c2d5205686887f6edd6a03a4e951f10542fbddc3", "content": "The concurrency setting can be overridden per listener using the `concurrency` attribute:\n\n[source,java]\n----\n@Component\npublic class ConcurrentShareListener {\n\n @KafkaListener(\n topics = \"high-throughput-topic\",\n containerFactory = \"shareKafkaListenerContainerFactory\",\n groupId = \"my-share-group\",\n concurrency = \"10\" // Override factory default\n )\n public void listen(ConsumerRecord<String, String> record) {\n // This listener will use 10 consumer threads\n System.out.println(\"Processing: \" + record.value());\n }\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Per-Listener Concurrency", "heading_level": 4, "file_order": 34, "section_index": 12, "content_hash": "01f9d01712026d096fffb5f8c2d5205686887f6edd6a03a4e951f10542fbddc3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:93de23e421c3b275a737338c838470959fb45d4bcd84ee411f64e78ce2d442ba", "content": "* **Thread Safety**: Each consumer thread has its own `ShareConsumer` instance and manages its own acknowledgments independently\n* **Client IDs**: Each consumer thread receives a unique client ID with a numeric suffix (e.g., `myContainer-0`, `myContainer-1`, etc.)\n* **Metrics**: Metrics from all consumer threads are aggregated and accessible via `container.metrics()`\n* **Lifecycle**: All consumer threads start and stop together as a unit\n* **Work Distribution**: The Kafka broker handles record distribution across all consumer instances in the share group\n* **Explicit Acknowledgment**: Each thread independently manages acknowledgments for its records; unacknowledged records in one thread don't block other threads", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Concurrency Considerations", "heading_level": 4, "file_order": 34, "section_index": 13, "content_hash": "93de23e421c3b275a737338c838470959fb45d4bcd84ee411f64e78ce2d442ba", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:b0e7df563c78eaa983a2c08cf23a723aff6b16cd4ed4dadaef6aef92c87ab162", "content": "Concurrency works seamlessly with explicit acknowledgment mode.\nEach consumer thread independently tracks and acknowledges its own records:\n\n[source,java]\n----\n@KafkaListener(\n topics = \"order-queue\",\n containerFactory = \"explicitShareKafkaListenerContainerFactory\",\n groupId = \"order-processors\",\n concurrency = \"5\"\n)\npublic void processOrder(ConsumerRecord<String, String> record, ShareAcknowledgment acknowledgment) {\n try {\n // Process the order\n processOrderLogic(record.value());\n acknowledgment.acknowledge(); // ACCEPT\n }\n catch (RetryableException e) {\n acknowledgment.release(); // Will be redelivered\n }\n catch (Exception e) {\n acknowledgment.reject(); // Permanent failure\n }\n}\n----\n\n[NOTE]\n====\n**Record Acquisition and Distribution Behavior:**\n\nShare consumers use a pull-based model where each consumer thread calls `poll()` to fetch records from the broker.\nWhen a consumer polls, the broker's share-partition leader:\n\n* Selects records in \"Available\" state\n* Moves them to \"Acquired\" state with a time-limited acquisition lock (default 30 seconds, configurable via `group.share.record.lock.duration.ms`)\n* Prefers to return complete record batches for efficiency\n* Applies `max.poll.records` as a soft limit, meaning complete record batches will be acquired even if it exceeds this value\n\nWhile records are acquired by one consumer, they are not available to other consumers.\nWhen the acquisition lock expires, unacknowledged records automatically return to \"Available\" state and can be delivered to another consumer.\n\nThe broker limits the number of records that can be acquired per partition using `group.share.partition.max.record.locks`.\nOnce this limit is reached, subsequent polls temporarily return no records until locks expire.\n\n**Implications for Concurrency:**\n\n* Each consumer thread independently polls and may acquire different numbers of records per poll\n* Record distribution across threads depends on polling timing and batch availability\n* Multiple threads increase the pool of consumers available to acquire records\n* With low message volume or single partitions, records may concentrate on fewer threads\n* For long-running workloads, distribution tends to be more even\n\n**Configuration:**\n\n* Each thread polls and processes records independently\n* Acknowledgment constraints apply per-thread (one thread's unacknowledged records don't block other threads)\n* Concurrency setting must be greater than 0 and cannot be changed while the container is running\n====\n\n[[share-annotation-driven-listeners]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Concurrency with Explicit Acknowledgment", "heading_level": 4, "file_order": 34, "section_index": 14, "content_hash": "b0e7df563c78eaa983a2c08cf23a723aff6b16cd4ed4dadaef6aef92c87ab162", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:471d5a65e62e77b38c3d634c5e169fd4d4917e450a4f916adf489babf33c1e6c", "content": "[[share-kafka-listener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Annotation-Driven Listeners", "heading_level": 2, "file_order": 34, "section_index": 15, "content_hash": "471d5a65e62e77b38c3d634c5e169fd4d4917e450a4f916adf489babf33c1e6c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:95e86cd7e3615424448c3f55e752050068630f62d520cf82cf8be6d51331f3ef", "content": "You can use `@KafkaListener` with share consumers by configuring a `ShareKafkaListenerContainerFactory`:\n\n[source,java]\n----\n@Configuration\n@EnableKafka\npublic class ShareConsumerConfig {\n\n @Bean\n public ShareConsumerFactory<String, String> shareConsumerFactory() {\n Map<String, Object> props = new HashMap<>();\n props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n return new DefaultShareConsumerFactory<>(props);\n }\n\n @Bean\n public ShareKafkaListenerContainerFactory<String, String> shareKafkaListenerContainerFactory(\n ShareConsumerFactory<String, String> shareConsumerFactory) {\n return new ShareKafkaListenerContainerFactory<>(shareConsumerFactory);\n }\n}\n----\n\nThen use it in your listener:\n\n[source,java]\n----\n@Component\npublic class ShareMessageListener {\n\n @KafkaListener(\n topics = \"my-queue-topic\",\n containerFactory = \"shareKafkaListenerContainerFactory\",\n groupId = \"my-share-group\"\n )\n public void listen(ConsumerRecord<String, String> record) {\n System.out.println(\"Received from queue: \" + record.value());\n // Record is automatically acknowledged with ACCEPT\n }\n}\n----\n\n[[share-group-offset-reset]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "@KafkaListener with Share Consumers", "heading_level": 3, "file_order": 34, "section_index": 16, "content_hash": "95e86cd7e3615424448c3f55e752050068630f62d520cf82cf8be6d51331f3ef", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:e91434594204b0f186c18ee34d258e4bbd475bd621cdb5f5442c14d6a9d9bf8e", "content": "Unlike regular consumer groups, share groups use a different configuration for offset reset behavior.\nYou can configure this programmatically:\n\n[source,java]\n----\nprivate void configureShareGroup(String bootstrapServers, String groupId) throws Exception {\n Map<String, Object> adminProps = new HashMap<>();\n adminProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);\n\n try (Admin admin = Admin.create(adminProps)) {\n ConfigResource configResource = new ConfigResource(ConfigResource.Type.GROUP, groupId);\n ConfigEntry configEntry = new ConfigEntry(\"share.auto.offset.reset\", \"earliest\");\n\n Map<ConfigResource, Collection<AlterConfigOp>> configs = Map.of(\n configResource, List.of(new AlterConfigOp(configEntry, AlterConfigOp.OpType.SET))\n );\n\n admin.incrementalAlterConfigs(configs).all().get();\n }\n}\n----\n\n[[share-record-acknowledgment]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Share Group Offset Reset", "heading_level": 3, "file_order": 34, "section_index": 17, "content_hash": "e91434594204b0f186c18ee34d258e4bbd475bd621cdb5f5442c14d6a9d9bf8e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:3f26946dede52616febfbed1676b81e001541063f88d4387d1fcc2c749bf02b0", "content": "Share consumers support two acknowledgment modes that control how records are acknowledged after processing.\n\n[[share-implicit-acknowledgment]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Record Acknowledgment", "heading_level": 2, "file_order": 34, "section_index": 18, "content_hash": "3f26946dede52616febfbed1676b81e001541063f88d4387d1fcc2c749bf02b0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:4cfb07eb71db5ff70af33297fa8050613f94a5ffea64052f00105eb2be3cb8f2", "content": "In implicit mode, records are automatically acknowledged based on processing outcome:\n\nSuccessful processing: Records are acknowledged as `ACCEPT`\nProcessing errors: Records are acknowledged as `REJECT`\n\n[source,java]\n----\n@Bean\npublic ShareKafkaListenerContainerFactory<String, String> shareKafkaListenerContainerFactory(\n ShareConsumerFactory<String, String> shareConsumerFactory) {\n // Implicit mode is the default - no additional configuration needed\n return new ShareKafkaListenerContainerFactory<>(shareConsumerFactory);\n}\n----\n\n[[share-explicit-acknowledgment]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Implicit Acknowledgment (Default)", "heading_level": 3, "file_order": 34, "section_index": 19, "content_hash": "4cfb07eb71db5ff70af33297fa8050613f94a5ffea64052f00105eb2be3cb8f2", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:6b829f98c7ad7ece7116b1d2da8a25d67f464b3564bff5fcd19489de3fb639c8", "content": "In explicit mode, the application must manually acknowledge each record using the provided ShareAcknowledgment.\n\nThere are two ways to configure explicit acknowledgment mode:", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Explicit Acknowledgment", "heading_level": 3, "file_order": 34, "section_index": 20, "content_hash": "6b829f98c7ad7ece7116b1d2da8a25d67f464b3564bff5fcd19489de3fb639c8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:c92eebc07036f1674e24e8c242bafa7b106de19faa4a75d0ad1e2230bb664255", "content": "[source,java]\n----\n@Bean\npublic ShareConsumerFactory<String, String> explicitShareConsumerFactory() {\n Map<String, Object> props = new HashMap<>();\n props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n props.put(ConsumerConfig.SHARE_ACKNOWLEDGEMENT_MODE_CONFIG, \"explicit\"); // Official Kafka client config\n return new DefaultShareConsumerFactory<>(props);\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Option 1: Using Kafka Client Configuration", "heading_level": 4, "file_order": 34, "section_index": 21, "content_hash": "c92eebc07036f1674e24e8c242bafa7b106de19faa4a75d0ad1e2230bb664255", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:8239ced501db27899cc3249efe246dc0b4a905945110029c35520e169a1aa68a", "content": "[source,java]\n----\n@Bean\npublic ShareKafkaListenerContainerFactory<String, String> explicitShareKafkaListenerContainerFactory(\n ShareConsumerFactory<String, String> shareConsumerFactory) {\n\n ShareKafkaListenerContainerFactory<String, String> factory =\n new ShareKafkaListenerContainerFactory<>(shareConsumerFactory);\n\n // Configure acknowledgment mode at container factory level\n // true means explicit acknowledgment is required\n factory.getContainerProperties().setExplicitShareAcknowledgment(true);\n\n return factory;\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Option 2: Using Spring Container Configuration", "heading_level": 4, "file_order": 34, "section_index": 22, "content_hash": "8239ced501db27899cc3249efe246dc0b4a905945110029c35520e169a1aa68a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:4b7ccb3f8506a35025b30b4be78b2846b6008c93c2aaf3c12418ecb28d949505", "content": "When both configuration methods are used, Spring Kafka follows this precedence order (highest to lowest):\n\n1. **Container Properties**: `containerProperties.setExplicitShareAcknowledgment(true/false)`\n2. **Consumer Config**: `ConsumerConfig.SHARE_ACKNOWLEDGEMENT_MODE_CONFIG` (\"implicit\" or \"explicit\")\n3. **Default**: `false` (implicit acknowledgment)\n\n[[share-acknowledgment-types]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Configuration Precedence", "heading_level": 4, "file_order": 34, "section_index": 23, "content_hash": "4b7ccb3f8506a35025b30b4be78b2846b6008c93c2aaf3c12418ecb28d949505", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:ee354eef4f041bc57de0c73388b62ddcd46e14a7f666bb386da41a0ea091e75a", "content": "Share consumers support three acknowledgment types:\n\n ACCEPT: Record processed successfully, mark as completed\n RELEASE: Temporary failure, make record available for redelivery\n REJECT: Permanent failure, do not retry\n\n[[share-acknowledgment-api]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Acknowledgment Types", "heading_level": 3, "file_order": 34, "section_index": 24, "content_hash": "ee354eef4f041bc57de0c73388b62ddcd46e14a7f666bb386da41a0ea091e75a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:0b70812f097a875db8a7aab5d2869e6dbc1e785a163ae3a6dc7122e5ab4e00cc", "content": "The `ShareAcknowledgment` interface provides methods for explicit acknowledgment:\n\n[source,java]\n----\npublic interface ShareAcknowledgment {\n void acknowledge();\n void release();\n void reject();\n}\n----\n\n[[share-listener-interfaces]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "ShareAcknowledgment API", "heading_level": 3, "file_order": 34, "section_index": 25, "content_hash": "0b70812f097a875db8a7aab5d2869e6dbc1e785a163ae3a6dc7122e5ab4e00cc", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:61f8abae13180c350daa08f13293ceb54dd66df6ca653c3449397c0661f456c9", "content": "Share consumers support specialized listener interfaces for different use cases:\n\n[[share-basic-listener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Listener Interfaces", "heading_level": 3, "file_order": 34, "section_index": 26, "content_hash": "61f8abae13180c350daa08f13293ceb54dd66df6ca653c3449397c0661f456c9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:35c595d5a93236ad48044743d67053630e74ae2bea439cbbab3b1ef7bd922be3", "content": "Use the standard MessageListener for simple cases:\n[source,java]\n----\n@KafkaListener(topics = \"my-topic\", containerFactory = \"shareKafkaListenerContainerFactory\")\npublic void listen(ConsumerRecord<String, String> record) {\n System.out.println(\"Received: \" + record.value());\n // Automatically acknowledged in implicit mode\n}\n----\n\n[[share-acknowledging-listener]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Basic Message Listener", "heading_level": 4, "file_order": 34, "section_index": 27, "content_hash": "35c595d5a93236ad48044743d67053630e74ae2bea439cbbab3b1ef7bd922be3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:8e26f40c4ef33b52b56453cb413a430232b563d905d0e1fd34c9ed2e62484539", "content": "This interface provides access to the `ShareConsumer` instance with optional acknowledgment support.\nThe acknowledgment parameter is nullable and depends on the container's acknowledgment mode:", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "AcknowledgingShareConsumerAwareMessageListener", "heading_level": 4, "file_order": 34, "section_index": 28, "content_hash": "8e26f40c4ef33b52b56453cb413a430232b563d905d0e1fd34c9ed2e62484539", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:52162b8478e10f5ef57ab58273dda23694a817b1feb3749e719d9dc97af536d1", "content": "[source,java]\n----\n@KafkaListener(\n topics = \"my-topic\",\n containerFactory = \"shareKafkaListenerContainerFactory\" // Implicit mode by default\n)\npublic void listen(ConsumerRecord<String, String> record,\n @Nullable ShareAcknowledgment acknowledgment,\n ShareConsumer<?, ?> consumer) {\n\n // In implicit mode, acknowledgment is null\n System.out.println(\"Received: \" + record.value());\n\n // Access consumer metrics if needed\n Map<MetricName, ? extends Metric> metrics = consumer.metrics();\n\n // Record is auto-acknowledged as ACCEPT on success, REJECT on error\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Implicit Mode Example (acknowledgment is null)", "heading_level": 5, "file_order": 34, "section_index": 29, "content_hash": "52162b8478e10f5ef57ab58273dda23694a817b1feb3749e719d9dc97af536d1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:7ad94a106f6707590eea207b0c55a7a32404ef4e6f1ac2b8faca515f7ded5001", "content": "[source,java]\n----\n@Component\npublic class ExplicitAckListener {\n @KafkaListener(\n topics = \"my-topic\",\n containerFactory = \"explicitShareKafkaListenerContainerFactory\"\n )\n public void listen(ConsumerRecord<String, String> record,\n @Nullable ShareAcknowledgment acknowledgment,\n ShareConsumer<?, ?> consumer) {\n\n // In explicit mode, acknowledgment is non-null\n try {\n processRecord(record);\n acknowledgment.acknowledge(); // ACCEPT\n }\n catch (RetryableException e) {\n acknowledgment.release(); // Will be redelivered\n }\n catch (Exception e) {\n acknowledgment.reject(); // Permanent failure\n }\n }\n\n private void processRecord(ConsumerRecord<String, String> record) {\n // Business logic here\n }\n}\n----\n\n[[share-acknowledgment-constraints]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Explicit Mode Example (acknowledgment is non-null)", "heading_level": 5, "file_order": 34, "section_index": 30, "content_hash": "7ad94a106f6707590eea207b0c55a7a32404ef4e6f1ac2b8faca515f7ded5001", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:ee2a2c3d64b479350fab67586452097915dcb50092cdae44dd8ef3996a6f5c8b", "content": "In explicit acknowledgment mode, the container enforces important constraints:\n\n Poll Blocking: Subsequent polls are blocked until all records from the previous poll are acknowledged.\n One-time Acknowledgment: Each record can only be acknowledged once.\n Error Handling: If processing throws an exception, the record is automatically acknowledged as `REJECT`.\n\n[WARNING]\nIn explicit mode, failing to acknowledge records will block further message processing.\nAlways ensure records are acknowledged in all code paths.\n\n[[share-acknowledgment-timeout]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Acknowledgment Constraints", "heading_level": 3, "file_order": 34, "section_index": 31, "content_hash": "ee2a2c3d64b479350fab67586452097915dcb50092cdae44dd8ef3996a6f5c8b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:58c678b3ede8c878eeb947496a9f816336856cb1c59002b5fb6ff465aee37051", "content": "To help identify missing acknowledgments, Spring Kafka provides configurable timeout detection.\nWhen a record is not acknowledged within the specified timeout, a warning is logged with details about the unacknowledged record.\n\n[source,java]\n----\n@Bean\npublic ShareKafkaListenerContainerFactory<String, String> shareKafkaListenerContainerFactory(\n ShareConsumerFactory<String, String> shareConsumerFactory) {\n ShareKafkaListenerContainerFactory<String, String> factory =\n new ShareKafkaListenerContainerFactory<>(shareConsumerFactory);\n\n // Set acknowledgment timeout (default is 30 seconds)\n factory.getContainerProperties().setShareAcknowledgmentTimeout(Duration.ofSeconds(30));\n\n return factory;\n}\n----\n\nWhen a record exceeds the timeout, you'll see a warning like:\n----\nWARN: Record not acknowledged within timeout (30 seconds).\nIn explicit acknowledgment mode, you must call ack.acknowledge(), ack.release(),\nor ack.reject() for every record.\n----\n\nThis feature helps developers quickly identify when acknowledgment calls are missing from their code, preventing the common issue of \"Spring Kafka does not consume new records any more\" due to forgotten acknowledgments.\n\n[[share-acknowledgment-examples]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Acknowledgment Timeout Detection", "heading_level": 4, "file_order": 34, "section_index": 32, "content_hash": "58c678b3ede8c878eeb947496a9f816336856cb1c59002b5fb6ff465aee37051", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:5d75ab5e95d9a25472b433f6fbb17999f1f001a6c5f5f751362897e582c6522e", "content": "[[share-mixed-acknowledgment-example]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Acknowledgment Examples", "heading_level": 3, "file_order": 34, "section_index": 33, "content_hash": "5d75ab5e95d9a25472b433f6fbb17999f1f001a6c5f5f751362897e582c6522e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:250749bde609d446e06138136ce92da314b80253b940ed134b4ac73de116d18b", "content": "[source,java]\n----\n@KafkaListener(topics = \"order-processing\", containerFactory = \"explicitShareKafkaListenerContainerFactory\")\n public void processOrder(ConsumerRecord<String, String> record, ShareAcknowledgment acknowledgment) {\n String orderId = record.key();\n String orderData = record.value();\n try {\n if (isValidOrder(orderData)) {\n if (processOrder(orderData)) {\n acknowledgment.acknowledge(); // Success - ACCEPT\n }\n else {\n acknowledgment.release(); // Temporary failure - retry later\n }\n }\n else {\n acknowledgment.reject(); // Invalid order - don't retry\n }\n }\n catch (Exception e) {\n // Exception automatically triggers REJECT\n throw e;\n }\n}\n----\n\n[[share-conditional-acknowledgment-example]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Mixed Acknowledgment Patterns", "heading_level": 4, "file_order": 34, "section_index": 34, "content_hash": "250749bde609d446e06138136ce92da314b80253b940ed134b4ac73de116d18b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:4f3664cef4f00f1697aff39893a066432ed014a3199f379777b95784fa3e9204", "content": "[source,java]\n----\n@KafkaListener(topics = \"data-validation\", containerFactory = \"explicitShareKafkaListenerContainerFactory\")\npublic void validateData(ConsumerRecord<String, String> record, ShareAcknowledgment acknowledgment) {\n ValidationResult result = validator.validate(record.value());\n switch (result.getStatus()) {\n case VALID:\n acknowledgment.acknowledge(AcknowledgeType.ACCEPT);\n break;\n case INVALID_RETRYABLE:\n acknowledgment.acknowledge(AcknowledgeType.RELEASE);\n break;\n case INVALID_PERMANENT:\n acknowledgment.acknowledge(AcknowledgeType.REJECT);\n break;\n }\n}\n----\n\n[[share-poison-message-protection]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Conditional Acknowledgment", "heading_level": 4, "file_order": 34, "section_index": 35, "content_hash": "4f3664cef4f00f1697aff39893a066432ed014a3199f379777b95784fa3e9204", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:ff660f1d6811dca8129fe1f2c8eb3d6a9e5f2b98001652e4f45bf7dfa7f34159", "content": "KIP-932 includes broker-side poison message protection to prevent unprocessable records from being endlessly redelivered.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Poison Message Protection and Delivery Count", "heading_level": 2, "file_order": 34, "section_index": 36, "content_hash": "ff660f1d6811dca8129fe1f2c8eb3d6a9e5f2b98001652e4f45bf7dfa7f34159", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:6abacf3ea43d99b7278e739cab2b5d0cedb5571187064e2cb4f3a0ea7180ae42", "content": "Every time a record is acquired by a consumer in a share group, the broker increments an internal delivery count.\nThe first acquisition sets the delivery count to 1, and each subsequent acquisition increments it.\nWhen the delivery count reaches the configured limit (default: 5), the record moves to **Archived** state and is not eligible for additional delivery attempts.\n\n[IMPORTANT]\n====\n**Delivery Count is Not Exposed to Applications**\nThe delivery count is maintained internally by the broker and is **not exposed to consumer applications**.\nThis is an intentional design decision in KIP-932.\nThe delivery count is approximate and serves as a poison message protection mechanism, not a precise redelivery counter.\nApplications cannot query or access this value through any API.\n====\n\nFor application-level retry logic, use the acknowledgment types:\n\n* `RELEASE` - Make record available for redelivery (contributes to delivery count)\n* `REJECT` - Mark as permanently failed (does not cause redelivery)\n* `ACCEPT` - Successfully processed (does not cause redelivery)\n\nThe broker automatically prevents endless redelivery once `group.share.delivery.count.limit` is reached, moving the record to Archived state.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "How It Works", "heading_level": 3, "file_order": 34, "section_index": 37, "content_hash": "6abacf3ea43d99b7278e739cab2b5d0cedb5571187064e2cb4f3a0ea7180ae42", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:762d3fa48e5a01746ca35f2952766f077f95301dcbd5cdb13b52e57a1020f1e8", "content": "Here is an example of how to use the various acknowledgement types based exception types.\n\n[source,java]\n----\n@KafkaListener(topics = \"orders\", containerFactory = \"explicitShareKafkaListenerContainerFactory\")\npublic void processOrder(ConsumerRecord<String, String> record, ShareAcknowledgment ack) {\n try {\n // Attempt to process the order\n orderService.process(record.value());\n ack.acknowledge(); // ACCEPT - successfully processed\n }\n catch (TransientException e) {\n // Temporary failure (network issue, service unavailable, etc.)\n // Release the record for redelivery\n // Broker will retry up to group.share.delivery.count.limit times\n logger.warn(\"Transient error processing order, will retry: {}\", e.getMessage());\n ack.release(); // RELEASE - make available for retry\n }\n catch (ValidationException e) {\n // Permanent semantic error (invalid data format, business rule violation, etc.)\n // Do not retry - this record will never succeed\n logger.error(\"Invalid order data, rejecting: {}\", e.getMessage());\n ack.reject(); // REJECT - permanent failure, do not retry\n }\n catch (Exception e) {\n // Unknown error - typically safer to reject to avoid infinite loops\n // But could also release if you suspect it might be transient\n logger.error(\"Unexpected error processing order, rejecting: {}\", e.getMessage());\n ack.reject(); // REJECT - avoid poison message loops\n }\n}\n----\n\nThe broker's poison message protection ensures that even if you always use `RELEASE` for errors, records won't be retried endlessly.\nThey will automatically be archived after reaching the delivery attempt limit.\n\n[[share-differences-from-regular-consumers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Retry Strategy Recommendations", "heading_level": 3, "file_order": 34, "section_index": 38, "content_hash": "762d3fa48e5a01746ca35f2952766f077f95301dcbd5cdb13b52e57a1020f1e8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:a557841468bdff4dcad13e3480c26b1c61c5891cdd9b023f28b3d499fc72d98c", "content": "Share consumers differ from regular consumers in several key ways:\n\n1. **No Partition Assignment**: Share consumers cannot be assigned specific partitions\n2. **No Topic Patterns**: Share consumers do not support subscribing to topic patterns\n3. **Cooperative Consumption**: Multiple consumers in the same share group can consume from the same partitions simultaneously\n4. **Record-Level Acknowledgment**: Supports explicit acknowledgment with `ACCEPT`, `RELEASE`, and `REJECT` types\n5. **Different Group Management**: Share groups use different coordinator protocols\n6. **No Batch Processing**: Share consumers process records individually, not in batches\n7. **Broker-Side Retry Management**: Delivery count tracking and poison message protection are managed by the broker, not exposed to applications\n\n[[share-limitations-and-considerations]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Differences from Regular Consumers", "heading_level": 2, "file_order": 34, "section_index": 39, "content_hash": "a557841468bdff4dcad13e3480c26b1c61c5891cdd9b023f28b3d499fc72d98c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:46c56912fb1a46295ef0c98d988e49ed9f251e300df031631216d0619f0a22ff", "content": "[[share-current-limitations]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Limitations and Considerations", "heading_level": 2, "file_order": 34, "section_index": 40, "content_hash": "46c56912fb1a46295ef0c98d988e49ed9f251e300df031631216d0619f0a22ff", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:0ebe62ce0eafe1b6258b8a2eaf67bddd886694cc29a7bbe5b0e3fbe8e0ffee78", "content": "* **In preview**: This feature is in preview mode and may change in future versions\n* **No Message Converters**: Message converters are not yet supported for share consumers\n* **No Batch Listeners**: Batch processing is not supported with share consumers\n* **Poll Constraints**: In explicit acknowledgment mode, unacknowledged records block subsequent polls within each consumer thread", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kafka-queues.adoc", "title": "kafka-queues", "heading": "Current Limitations", "heading_level": 3, "file_order": 34, "section_index": 41, "content_hash": "0ebe62ce0eafe1b6258b8a2eaf67bddd886694cc29a7bbe5b0e3fbe8e0ffee78", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kafka-queues.adoc"}}
{"id": "sha256:33cedb0603f1c1f903124cd46e91140939cf8f9970826c60f91d7aa7865dc3cf", "content": "[[kerberos]]\n\nStarting with version 2.0, a `KafkaJaasLoginModuleInitializer` class has been added to assist with Kerberos configuration.\nYou can add this bean, with the desired configuration, to your application context.\nThe following example configures such a bean:\n\n[source, java]\n----\n@Bean\npublic KafkaJaasLoginModuleInitializer jaasConfig() throws IOException {\n KafkaJaasLoginModuleInitializer jaasConfig = new KafkaJaasLoginModuleInitializer();\n jaasConfig.setControlFlag(\"REQUIRED\");\n Map<String, String> options = new HashMap<>();\n options.put(\"useKeyTab\", \"true\");\n options.put(\"storeKey\", \"true\");\n options.put(\"keyTab\", \"/etc/security/keytabs/kafka_client.keytab\");\n options.put(\"principal\", \"kafka-client-1@EXAMPLE.COM\");\n jaasConfig.setOptions(options);\n return jaasConfig;\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/kerberos.adoc", "title": "kerberos", "heading": "kerberos", "heading_level": 1, "file_order": 35, "section_index": 0, "content_hash": "33cedb0603f1c1f903124cd46e91140939cf8f9970826c60f91d7aa7865dc3cf", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/kerberos.adoc"}}
{"id": "sha256:42fff6e7910b6c6fd92f9b74165981bf9f9ba373e92927306545d85c13e99dcf", "content": "[[micrometer]]\n\n[[monitoring-listener-performance]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/micrometer.adoc", "title": "micrometer", "heading": "micrometer", "heading_level": 1, "file_order": 36, "section_index": 0, "content_hash": "42fff6e7910b6c6fd92f9b74165981bf9f9ba373e92927306545d85c13e99dcf", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/micrometer.adoc"}}
{"id": "sha256:9d9acb99767269ebf3c9ce905fc3d1e929f84168e49bfc8c39c42d41916bf4e5", "content": "Starting with version 2.3, the listener container will automatically create and update Micrometer ``Timer``s for the listener, if `Micrometer` is detected on the classpath, and a single `MeterRegistry` is present in the application context.\nThe timers can be disabled by setting the ``ContainerProperty``'s `micrometerEnabled` to `false`.\n\nTwo timers are maintained - one for successful calls to the listener and one for failures.\n\nThe timers are named `spring.kafka.listener` and have the following tags:\n\n* `name` : (container bean name)\n* `result` : `success` or `failure`\n* `exception` : `none` or `ListenerExecutionFailedException`\n\nYou can add additional tags using the ``ContainerProperties``'s `micrometerTags` property.\n\nStarting with versions 2.9.8, 3.0.6, you can provide a function in ``ContainerProperties``'s `micrometerTagsProvider`; the function receives the `ConsumerRecord<?, ?>` and returns tags which can be based on that record, and merged with any static tags in `micrometerTags`.\n\nNOTE: With the concurrent container, timers are created for each thread and the `name` tag is suffixed with `-n` where n is `0` to `concurrency-1`.\n\n[[monitoring-kafkatemplate-performance]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/micrometer.adoc", "title": "micrometer", "heading": "Monitoring Listener Performance", "heading_level": 2, "file_order": 36, "section_index": 1, "content_hash": "9d9acb99767269ebf3c9ce905fc3d1e929f84168e49bfc8c39c42d41916bf4e5", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/micrometer.adoc"}}
{"id": "sha256:18bf5cfd0e5211ade68532dc66c93897f52f550b43fd5bf8b2108cf537b2beac", "content": "Starting with version 2.5, the template will automatically create and update Micrometer ``Timer``s for send operations, if `Micrometer` is detected on the classpath, and a single `MeterRegistry` is present in the application context.\nThe timers can be disabled by setting the template's `micrometerEnabled` property to `false`.\n\nTwo timers are maintained - one for successful calls to the listener and one for failures.\n\nThe timers are named `spring.kafka.template` and have the following tags:\n\n* `name` : (template bean name)\n* `result` : `success` or `failure`\n* `exception` : `none` or the exception class name for failures\n\nYou can add additional tags using the template's `micrometerTags` property.\n\nStarting with versions 2.9.8, 3.0.6, you can provide a `KafkaTemplate.setMicrometerTagsProvider(Function<ProducerRecord<?, ?>, Map<String, String>>)` property; the function receives the `ProducerRecord<?, ?>` and returns tags which can be based on that record, and merged with any static tags in `micrometerTags`.\n\n[[micrometer-native]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/micrometer.adoc", "title": "micrometer", "heading": "Monitoring KafkaTemplate Performance", "heading_level": 2, "file_order": 36, "section_index": 2, "content_hash": "18bf5cfd0e5211ade68532dc66c93897f52f550b43fd5bf8b2108cf537b2beac", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/micrometer.adoc"}}
{"id": "sha256:1cad48b0f72657682dff5eb2b9ac57d9bcfc480e6ae18a793e77f77b867ad360", "content": "Starting with version 2.5, the framework provides xref:kafka/connecting.adoc#factory-listeners[Factory Listeners] to manage a Micrometer `KafkaClientMetrics` instance whenever producers and consumers are created and closed.\n\nTo enable this feature, simply add the listeners to your producer and consumer factories:\n\n[source, java]\n----\n@Bean\npublic ConsumerFactory<String, String> myConsumerFactory() {\n Map<String, Object> configs = consumerConfigs();\n ...\n DefaultKafkaConsumerFactory<String, String> cf = new DefaultKafkaConsumerFactory<>(configs);\n ...\n cf.addListener(new MicrometerConsumerListener<String, String>(meterRegistry(),\n Collections.singletonList(new ImmutableTag(\"customTag\", \"customTagValue\"))));\n ...\n return cf;\n}\n\n@Bean\npublic ProducerFactory<String, String> myProducerFactory() {\n Map<String, Object> configs = producerConfigs();\n configs.put(ProducerConfig.CLIENT_ID_CONFIG, \"myClientId\");\n ...\n DefaultKafkaProducerFactory<String, String> pf = new DefaultKafkaProducerFactory<>(configs);\n ...\n pf.addListener(new MicrometerProducerListener<String, String>(meterRegistry(),\n Collections.singletonList(new ImmutableTag(\"customTag\", \"customTagValue\"))));\n ...\n return pf;\n}\n----\n\nThe consumer/producer `id` passed to the listener is added to the meter's tags with tag name `spring.id`.\n\n.An example of obtaining one of the Kafka metrics\n[source, java]\n----\ndouble count = this.meterRegistry.get(\"kafka.producer.node.incoming.byte.total\")\n .tag(\"customTag\", \"customTagValue\")\n .tag(\"spring.id\", \"myProducerFactory.myClientId-1\")\n .functionCounter()\n .count();\n----\n\nA similar listener is provided for the `StreamsBuilderFactoryBean` - see xref:streams.adoc#streams-micrometer[KafkaStreams Micrometer Support].\n\nStarting with version 3.3, a `KafkaMetricsSupport` abstract class is introduced to manage `io.micrometer.core.instrument.binder.kafka.KafkaMetrics` binding into a `MeterRegistry` for provided Kafka client.\nThis class is a super for the mentioned above `MicrometerConsumerListener`, `MicrometerProducerListener` and `KafkaStreamsMicrometerListener`.\nHowever, it can be used for any Kafka client use-cases.\nThe class needs to be extended and its `bindClient()` and `unbindClient()` API have to be called to connect Kafka client metrics with a Micrometer collector.\n\n[[observation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/micrometer.adoc", "title": "micrometer", "heading": "Micrometer Native Metrics", "heading_level": 2, "file_order": 36, "section_index": 3, "content_hash": "1cad48b0f72657682dff5eb2b9ac57d9bcfc480e6ae18a793e77f77b867ad360", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/micrometer.adoc"}}
{"id": "sha256:8a79983b265bae895323a9db085d7ca05ac5ae6a0ae77135c0ef45adbae121e9", "content": "Using Micrometer for observation is now supported, since version 3.0, for the `KafkaTemplate` and listener containers.\n\nSet `observationEnabled` to `true` on the `KafkaTemplate` and `ContainerProperties` to enable observation; this will disable xref:kafka/micrometer.adoc[Micrometer Timers] because the timers will now be managed with each observation.\n\nIMPORTANT: Micrometer Observation does not support batch listener; this will enable Micrometer Timers\n\nRefer to {micrometer-tracing-reference-url}[Micrometer Tracing] for more information.\n\nTo add tags to timers/traces, configure a custom `KafkaTemplateObservationConvention` or `KafkaListenerObservationConvention` to the template or listener container, respectively.\n\nThe default implementations add the `bean.name` tag for template observations and `listener.id` tag for containers.\n\nYou can either subclass `DefaultKafkaTemplateObservationConvention` or `DefaultKafkaListenerObservationConvention` or provide completely new implementations.\n\nSee xref:appendix/micrometer.adoc#observation-gen[Micrometer Observation Documentation] for details of the default observations that are recorded.\n\nStarting with version 3.0.6, you can add dynamic tags to the timers and traces, based on information in the consumer or producer records.\nTo do so, add a custom `KafkaListenerObservationConvention` and/or `KafkaTemplateObservationConvention` to the listener container properties or `KafkaTemplate` respectively.\nThe `record` property in both observation contexts contains the `ConsumerRecord` or `ProducerRecord` respectively.\n\nThe sender and receiver contexts `remoteServiceName` properties are set to the Kafka `clusterId` property; this is retrieved by a `KafkaAdmin`.\nIf, for some reason - perhaps lack of admin permissions, you cannot retrieve the cluster id, starting with version 3.1, you can set a manual `clusterId` on the `KafkaAdmin` and inject it into ``KafkaTemplate``s and listener containers.\nWhen it is `null` (default), the admin will invoke the `describeCluster` admin operation to retrieve it from the broker.\n\n[[batch-listener-obs]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/micrometer.adoc", "title": "micrometer", "heading": "Micrometer Observation", "heading_level": 2, "file_order": 36, "section_index": 4, "content_hash": "8a79983b265bae895323a9db085d7ca05ac5ae6a0ae77135c0ef45adbae121e9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/micrometer.adoc"}}
{"id": "sha256:1b978640255c70ed95e4f53c4bf11aece8f26e2f8487ba692d1f4e665032ec23", "content": "When using a batch listener, by default, no observations are created, even if a `ObservationRegistry` is present.\nThis is because the scope of an observation is tied to the thread, and with a batch listener, there is no one-to-one mapping between an observation and a record.\n\nTo enable per-record observations in a batch listener, set the container factory property `recordObservationsInBatch` to `true`.\n\n[source,java]\n----\n@Bean\nConcurrentKafkaListenerContainerFactory<?, ?> kafkaListenerContainerFactory(\n ConcurrentKafkaListenerContainerFactoryConfigurer configurer,\n ConsumerFactory<Object, Object> kafkaConsumerFactory) {\n\n ConcurrentKafkaListenerContainerFactory<Object, Object> factory = new ConcurrentKafkaListenerContainerFactory<>();\n configurer.configure(factory, kafkaConsumerFactory);\n factory.getContainerProperties().setRecordObservationsInBatch(true);\n return factory;\n}\n----\n\nWhen this property is `true`, an observation will be created for each record in the batch, but the observation is not propagated to the listener method.\nThe application can then use the observation context to track the processing of each record in the batch.\nThis allows you to have visibility into the processing of each record, even within a batch context.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/micrometer.adoc", "title": "micrometer", "heading": "Batch Listener Observations", "heading_level": 3, "file_order": 36, "section_index": 5, "content_hash": "1b978640255c70ed95e4f53c4bf11aece8f26e2f8487ba692d1f4e665032ec23", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/micrometer.adoc"}}
{"id": "sha256:c95f342dc7ffd7498123afe2f464c3af44394a5f4d815f4e6c05f5fa114c948f", "content": "[[pause-resume-partitions]]\n\nSince version 2.7 you can pause and resume the consumption of specific partitions assigned to that consumer by using the `pausePartition(TopicPartition topicPartition)` and `resumePartition(TopicPartition topicPartition)` methods in the listener containers.\nThe pausing and resuming take place respectively before and after the `poll()` similar to the `pause()` and `resume()` methods.\nThe `isPartitionPauseRequested()` method returns true if pause for that partition has been requested.\nThe `isPartitionPaused()` method returns true if that partition has effectively been paused.\n\nAlso since version 2.7 `ConsumerPartitionPausedEvent` and `ConsumerPartitionResumedEvent` instances are published with the container as the `source` property and the `TopicPartition` instance.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/pause-resume-partitions.adoc", "title": "pause-resume-partitions", "heading": "pause-resume-partitions", "heading_level": 1, "file_order": 37, "section_index": 0, "content_hash": "c95f342dc7ffd7498123afe2f464c3af44394a5f4d815f4e6c05f5fa114c948f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/pause-resume-partitions.adoc"}}
{"id": "sha256:12aa38c91d1f1f27acee9bc001352f90d9a0e725020ebacbefec717b1b2f608d", "content": "[[pause-resume]]\n\nVersion 2.1.3 added `pause()` and `resume()` methods to listener containers.\nPreviously, you could pause a consumer within a `ConsumerAwareMessageListener` and resume it by listening for a `ListenerContainerIdleEvent`, which provides access to the `Consumer` object.\nWhile you could pause a consumer in an idle container by using an event listener, in some cases, this was not thread-safe, since there is no guarantee that the event listener is invoked on the consumer thread.\nTo safely pause and resume consumers, you should use the `pause` and `resume` methods on the listener containers.\nA `pause()` takes effect just before the next `poll()`; a `resume()` takes effect just after the current `poll()` returns.\nWhen a container is paused, it continues to `poll()` the consumer, avoiding a rebalance if group management is being used, but it does not retrieve any records.\nSee the Kafka documentation for more information.\n\nStarting with version 2.1.5, you can call `isPauseRequested()` to see if `pause()` has been called.\nHowever, the consumers might not have actually paused yet.\n`isConsumerPaused()` returns true if all `Consumer` instances have actually paused.\n\nIn addition(also since 2.1.5), `ConsumerPausedEvent` and `ConsumerResumedEvent` instances are published with the container as the `source` property and the `TopicPartition` instances involved in the `partitions` property.\n\nStarting with version 2.9, a new container property `pauseImmediate`, when set to true, causes the pause to take effect after the current record is processed.\nBy default, the pause takes effect when all the records from the previous poll have been processed.\nSee xref:kafka/container-props.adoc#pauseImmediate[pauseImmediate].\n\nThe following simple Spring Boot application demonstrates by using the container registry to get a reference to a `@KafkaListener` method's container and pausing or resuming its consumers as well as receiving the corresponding events:\n\n[source, java]\n----\n@SpringBootApplication\npublic class Application implements ApplicationListener<KafkaEvent> {\n\n public static void main(String[] args) {\n SpringApplication.run(Application.class, args).close();\n }\n\n @Override\n public void onApplicationEvent(KafkaEvent event) {\n System.out.println(event);\n }\n\n @Bean\n public ApplicationRunner runner(KafkaListenerEndpointRegistry registry,\n KafkaTemplate<String, String> template) {\n return args -> {\n template.send(\"pause.resume.topic\", \"thing1\");\n Thread.sleep(10_000);\n System.out.println(\"pausing\");\n registry.getListenerContainer(\"pause.resume\").pause();\n Thread.sleep(10_000);\n template.send(\"pause.resume.topic\", \"thing2\");\n Thread.sleep(10_000);\n System.out.println(\"resuming\");\n registry.getListenerContainer(\"pause.resume\").resume();\n Thread.sleep(10_000);\n };\n }\n\n @KafkaListener(id = \"pause.resume\", topics = \"pause.resume.topic\")\n public void listen(String in) {\n System.out.println(in);\n }\n\n @Bean\n public NewTopic topic() {\n return TopicBuilder.name(\"pause.resume.topic\")\n .partitions(2)\n .replicas(1)\n .build();\n }\n\n}\n----\n\nThe following listing shows the results of the preceding example:\n\n[source]\n----\npartitions assigned: [pause.resume.topic-1, pause.resume.topic-0]\nthing1\npausing\nConsumerPausedEvent [partitions=[pause.resume.topic-1, pause.resume.topic-0]]\nresuming\nConsumerResumedEvent [partitions=[pause.resume.topic-1, pause.resume.topic-0]]\nthing2\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/pause-resume.adoc", "title": "pause-resume", "heading": "pause-resume", "heading_level": 1, "file_order": 38, "section_index": 0, "content_hash": "12aa38c91d1f1f27acee9bc001352f90d9a0e725020ebacbefec717b1b2f608d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/pause-resume.adoc"}}
{"id": "sha256:6820e718ea70b3f1bd687c4db9eba4245b43fd8b1888f1a3715f84ef68b126f3", "content": "[[producer-interceptor-managed-in-spring]]\n\nStarting with version 3.0.0, when it comes to a producer interceptor, you can let Spring manage it directly as a bean instead of providing the class name of the interceptor to the Apache Kafka producer configuration.\nIf you go with this approach, then you need to set this producer interceptor on `KafkaTemplate`.\nFollowing is an example using the same `MyProducerInterceptor` from above, but changed to not use the internal config property.\n\n[source, java]\n----\npublic class MyProducerInterceptor implements ProducerInterceptor<String, String> {\n\n private final SomeBean bean;\n\n public MyProducerInterceptor(SomeBean bean) {\n this.bean = bean;\n }\n\n @Override\n public void configure(Map<String, ?> configs) {\n }\n\n @Override\n public ProducerRecord<String, String> onSend(ProducerRecord<String, String> record) {\n this.bean.someMethod(\"producer interceptor\");\n return record;\n }\n\n @Override\n public void onAcknowledgement(RecordMetadata metadata, Exception exception) {\n }\n\n @Override\n public void close() {\n }\n\n}\n----\n\n[source]\n----\n\n@Bean\npublic MyProducerInterceptor myProducerInterceptor(SomeBean someBean) {\n return new MyProducerInterceptor(someBean);\n}\n\n@Bean\npublic KafkaTemplate<String, String> kafkaTemplate(ProducerFactory<String, String> pf, MyProducerInterceptor myProducerInterceptor) {\n KafkaTemplate<String, String> kafkaTemplate = new KafkaTemplate<>(pf);\n kafkaTemplate.setProducerInterceptor(myProducerInterceptor);\n}\n----\n\nRight before the records are sent, the `onSend` method of the producer interceptor is invoked.\nOnce the server sends an acknowledgement on publishing the data, then the `onAcknowledgement` method is invoked.\nThe `onAcknowledgement` is called right before the producer invokes any user callbacks.\n\nIf you have multiple such producer interceptors managed through Spring that need to be applied on the `KafkaTemplate`, you need to use `CompositeProducerInterceptor` instead.\n`CompositeProducerInterceptor` allows individual producer interceptors to be added in order.\nThe methods from the underlying `ProducerInterceptor` implementations are invoked in the order as they were added to the `CompositeProducerInterceptor`.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/producer-interceptor-managed-in-spring.adoc", "title": "producer-interceptor-managed-in-spring", "heading": "producer-interceptor-managed-in-spring", "heading_level": 1, "file_order": 39, "section_index": 0, "content_hash": "6820e718ea70b3f1bd687c4db9eba4245b43fd8b1888f1a3715f84ef68b126f3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/producer-interceptor-managed-in-spring.adoc"}}
{"id": "sha256:d9b1436f33050facce566aad7af28c829e1357864c4fa955f5871b36fe7a6872", "content": "[[receiving-messages]]\n\nYou can receive messages by configuring a `MessageListenerContainer` and providing a message listener or by using the `@KafkaListener` annotation.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/receiving-messages.adoc", "title": "receiving-messages", "heading": "receiving-messages", "heading_level": 1, "file_order": 40, "section_index": 0, "content_hash": "d9b1436f33050facce566aad7af28c829e1357864c4fa955f5871b36fe7a6872", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/receiving-messages.adoc"}}
{"id": "sha256:f40355ef9143cda9dac402ccccceef066710d0e68d9cbb4c97d59e664274706f", "content": "[[seek]]\n\nIn order to seek, your listener must implement `ConsumerSeekAware`, which has the following methods:\n\n[source, java]\n----\nvoid registerSeekCallback(ConsumerSeekCallback callback);\n\nvoid onPartitionsAssigned(Map<TopicPartition, Long> assignments, ConsumerSeekCallback callback);\n\nvoid onPartitionsRevoked(Collection<TopicPartition> partitions);\n\nvoid onIdleContainer(Map<TopicPartition, Long> assignments, ConsumerSeekCallback callback);\n----\n\nThe `registerSeekCallback` is called when the container is started and whenever partitions are assigned.\nYou should use this callback when seeking at some arbitrary time after initialization.\nYou should save a reference to the callback.\nIf you use the same listener in multiple containers (or in a `ConcurrentMessageListenerContainer`), you should store the callback in a `ThreadLocal` or some other structure keyed by the listener `Thread`.\n\nWhen using group management, `onPartitionsAssigned` is called when partitions are assigned.\nYou can use this method, for example, for setting initial offsets for the partitions, by calling the callback.\nYou can also use this method to associate this thread's callback with the assigned partitions (see the example below).\nYou must use the callback argument, not the one passed into `registerSeekCallback`.\nStarting with version 2.5.5, this method is called, even when using xref:kafka/receiving-messages/listener-annotation.adoc#manual-assignment[manual partition assignment].\n\n`onPartitionsRevoked` is called when the container is stopped or Kafka revokes assignments.\nYou should discard this thread's callback and remove any associations to the revoked partitions.\n\nThe callback has the following methods:\n\n[source, java]\n----\nvoid seek(String topic, int partition, long offset);\n\nvoid seek(String topic, int partition, Function<Long, Long> offsetComputeFunction);\n\nvoid seekToBeginning(String topic, int partition);\n\nvoid seekToBeginning(Collection<TopicPartitions> partitions);\n\nvoid seekToEnd(String topic, int partition);\n\nvoid seekToEnd(Collection<TopicPartitions> partitions);\n\nvoid seekRelative(String topic, int partition, long offset, boolean toCurrent);\n\nvoid seekToTimestamp(String topic, int partition, long timestamp);\n\nvoid seekToTimestamp(Collection<TopicPartition> topicPartitions, long timestamp);\n\nString getGroupId();\n----\n\nThe two different variants of the `seek` methods provide a way to seek to an arbitrary offset.\nThe method that takes a `Function` as an argument to compute the offset was added in version 3.2 of the framework.\nThis function provides access to the current offset (the current position returned by the consumer, which is the next offset to be fetched).\nThe user can decide what offset to seek to based on the current offset in the consumer as part of the function definition.\n\n`seekRelative` was added in version 2.3, to perform relative seeks.\n\n* `offset` negative and `toCurrent` `false` - seek relative to the end of the partition.\n* `offset` positive and `toCurrent` `false` - seek relative to the beginning of the partition.\n* `offset` negative and `toCurrent` `true` - seek relative to the current position (rewind).\n* `offset` positive and `toCurrent` `true` - seek relative to the current position (fast forward).\n\nThe `seekToTimestamp` methods were also added in version 2.3.\n\nNOTE: When seeking to the same timestamp for multiple partitions in the `onIdleContainer` or `onPartitionsAssigned` methods, the second method is preferred because it is more efficient to find the offsets for the timestamps in a single call to the consumer's `offsetsForTimes` method.\nWhen called from other locations, the container will gather all timestamp seek requests and make one call to `offsetsForTimes`.\n\nYou can also perform seek operations from `onIdleContainer()` when an idle container is detected.\nSee xref:kafka/events.adoc#idle-containers[Detecting Idle and Non-Responsive Consumers] for how to enable idle container detection.\n\nNOTE: The `seekToBeginning` method that accepts a collection is useful, for example, when processing a compacted topic and you wish to seek to the beginning every time the application is started:\n\n[source, java]\n----\npublic class MyListener implements ConsumerSeekAware {\n\n ...\n\n @Override\n public void onPartitionsAssigned(Map<TopicPartition, Long> assignments, ConsumerSeekCallback callback) {\n callback.seekToBeginning(assignments.keySet());\n }\n\n}\n----\n\nTo arbitrarily seek at runtime, use the callback reference from the `registerSeekCallback` for the appropriate thread.\n\nHere is a trivial Spring Boot application that demonstrates how to use the callback; it sends 10 records to the topic; hitting `<Enter>` in the console causes all partitions to seek to the beginning.\n\n[source, java]\n----\n@SpringBootApplication\npublic class SeekExampleApplication {\n\n public static void main(String[] args) {\n SpringApplication.run(SeekExampleApplication.class, args);\n }\n\n @Bean\n public ApplicationRunner runner(Listener listener, KafkaTemplate<String, String> template) {\n return args -> {\n IntStream.range(0, 10).forEach(i -> template.send(\n new ProducerRecord<>(\"seekExample\", i % 3, \"foo\", \"bar\")));\n while (true) {\n System.in.read();\n listener.seekToStart();\n }\n };\n }\n\n @Bean\n public NewTopic topic() {\n return new NewTopic(\"seekExample\", 3, (short) 1);\n }\n\n}\n\n@Component\nclass Listener implements ConsumerSeekAware {\n\n private static final Logger logger = LoggerFactory.getLogger(Listener.class);\n\n private final ThreadLocal<ConsumerSeekCallback> callbackForThread = new ThreadLocal<>();\n\n private final Map<TopicPartition, ConsumerSeekCallback> callbacks = new ConcurrentHashMap<>();\n\n @Override\n public void registerSeekCallback(ConsumerSeekCallback callback) {\n this.callbackForThread.set(callback);\n }\n\n @Override\n public void onPartitionsAssigned(Map<TopicPartition, Long> assignments, ConsumerSeekCallback callback) {\n assignments.keySet().forEach(tp -> this.callbacks.put(tp, this.callbackForThread.get()));\n }\n\n @Override\n public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n partitions.forEach(tp -> this.callbacks.remove(tp));\n this.callbackForThread.remove();\n }\n\n @Override\n public void onIdleContainer(Map<TopicPartition, Long> assignments, ConsumerSeekCallback callback) {\n }\n\n @KafkaListener(id = \"seekExample\", topics = \"seekExample\", concurrency = \"3\")\n public void listen(ConsumerRecord<String, String> in) {\n logger.info(in.toString());\n }\n\n public void seekToStart() {\n this.callbacks.forEach((tp, callback) -> callback.seekToBeginning(tp.topic(), tp.partition()));\n }\n\n}\n----\n\nTo make things simpler, version 2.3 added the `AbstractConsumerSeekAware` class, which keeps track of which callback is to be used for a topic/partition.\nThe following example shows how to seek to the last record processed, in each partition, each time the container goes idle.\nIt also has methods that allow arbitrary external calls to rewind partitions by one record.\n\n[source, java]\n----\npublic class SeekToLastOnIdleListener extends AbstractConsumerSeekAware {\n\n @KafkaListener(id = \"seekOnIdle\", topics = \"seekOnIdle\")\n public void listen(String in) {\n ...\n }\n\n @Override\n public void onIdleContainer(Map<TopicPartition, Long> assignments,\n ConsumerSeekCallback callback) {\n\n assignments.keySet().forEach(tp -> callback.seekRelative(tp.topic(), tp.partition(), -1, true));\n }\n\n /**\n * Rewind all partitions one record.\n */\n public void rewindAllOneRecord() {\n getTopicsAndCallbacks()\n .forEach((tp, callbacks) ->\n callbacks.forEach(callback -> callback.seekRelative(tp.topic(), tp.partition(), -1, true))\n );\n }\n\n /**\n * Rewind one partition one record.\n */\n public void rewindOnePartitionOneRecord(String topic, int partition) {\n getSeekCallbacksFor(new TopicPartition(topic, partition))\n .forEach(callback -> callback.seekRelative(topic, partition, -1, true));\n }\n\n}\n----\n\nVersion 2.6 added convenience methods to the abstract class:\n\n* `seekToBeginning()` - seeks all assigned partitions to the beginning.\n* `seekToEnd()` - seeks all assigned partitions to the end.\n* `seekToTimestamp(long timestamp)` - seeks all assigned partitions to the offset represented by that timestamp.\n\nExample:\n\n[source, java]\n----\npublic class MyListener extends AbstractConsumerSeekAware {\n\n @KafkaListener(...)\n void listen(...) {\n ...\n }\n}\n\npublic class SomeOtherBean {\n\n MyListener listener;\n\n ...\n\n void someMethod() {\n this.listener.seekToTimestamp(System.currentTimeMillis() - 60_000);\n }\n\n}\n\n----\n\nAs of version 3.3, a new method `getGroupId()` was introduced in the `ConsumerSeekAware.ConsumerSeekCallback` interface.\nThis method is particularly useful when you need to identify the consumer group associated with a specific seek callback.\n\nNOTE: When using a class that extends `AbstractConsumerSeekAware`, a seek operation performed in one listener may impact all listeners in the same class.\nThis might not always be the desired behavior.\nTo address this, you can use the `getGroupId()` method provided by the callback.\nThis allows you to perform seek operations selectively, targeting only the consumer group of interest.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/seek.adoc", "title": "seek", "heading": "seek", "heading_level": 1, "file_order": 41, "section_index": 0, "content_hash": "f40355ef9143cda9dac402ccccceef066710d0e68d9cbb4c97d59e664274706f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/seek.adoc"}}
{"id": "sha256:174d2eb5824927568674c20218d757d1498d5dceaca822e58abe7ec2814526d0", "content": "[[sending-messages]]\n\nThis section covers how to send messages.\n\n[[kafka-template]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "sending-messages", "heading_level": 1, "file_order": 42, "section_index": 0, "content_hash": "174d2eb5824927568674c20218d757d1498d5dceaca822e58abe7ec2814526d0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:60ceac97b24a1622f6d8db5b990a7585a29b27bcca43af69ac57cdc4cb786d70", "content": "This section covers how to use `KafkaTemplate` to send messages.\n\n[[overview]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Using `KafkaTemplate`", "heading_level": 2, "file_order": 42, "section_index": 1, "content_hash": "60ceac97b24a1622f6d8db5b990a7585a29b27bcca43af69ac57cdc4cb786d70", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:0f91ae4ad512f9b7b95098c060ae7fd0dab963745ba1628eb5024b72650552b1", "content": "The `KafkaTemplate` wraps a producer and provides convenience methods to send data to Kafka topics.\nThe following listing shows the relevant methods from `KafkaTemplate`:\n\n[source, java]\n----\nCompletableFuture<SendResult<K, V>> sendDefault(V data);\n\nCompletableFuture<SendResult<K, V>> sendDefault(K key, V data);\n\nCompletableFuture<SendResult<K, V>> sendDefault(Integer partition, K key, V data);\n\nCompletableFuture<SendResult<K, V>> sendDefault(Integer partition, Long timestamp, K key, V data);\n\nCompletableFuture<SendResult<K, V>> send(String topic, V data);\n\nCompletableFuture<SendResult<K, V>> send(String topic, K key, V data);\n\nCompletableFuture<SendResult<K, V>> send(String topic, Integer partition, K key, V data);\n\nCompletableFuture<SendResult<K, V>> send(String topic, Integer partition, Long timestamp, K key, V data);\n\nCompletableFuture<SendResult<K, V>> send(ProducerRecord<K, V> record);\n\nCompletableFuture<SendResult<K, V>> send(Message<?> message);\n\nMap<MetricName, ? extends Metric> metrics();\n\nList<PartitionInfo> partitionsFor(String topic);\n\n<T> T execute(ProducerCallback<K, V, T> callback);\n\n<T> T executeInTransaction(OperationsCallback<K, V, T> callback);\n\nvoid flush();\n\ninterface ProducerCallback<K, V, T> {\n\n T doInKafka(Producer<K, V> producer);\n\n}\n\ninterface OperationsCallback<K, V, T> {\n\n T doInOperations(KafkaOperations<K, V> operations);\n\n}\n----\n\nSee the javadoc:org.springframework.kafka.core.KafkaTemplate[Javadoc] for more detail.\n\nThe `sendDefault` API requires that a default topic has been provided to the template.\n\nThe API takes in a `timestamp` as a parameter and stores this timestamp in the record.\nHow the user-provided timestamp is stored depends on the timestamp type configured on the Kafka topic.\nIf the topic is configured to use `CREATE_TIME`, the user-specified timestamp is recorded (or generated if not specified).\nIf the topic is configured to use `LOG_APPEND_TIME`, the user-specified timestamp ignored and the broker adds in the local broker time.\n\nThe `metrics` and `partitionsFor` methods delegate to the same methods on the underlying javadoc:org.apache.kafka.clients.producer.Producer[].\nThe `execute` method provides direct access to the underlying javadoc:org.apache.kafka.clients.producer.Producer[].\n\n[NOTE]\n====\nWhen sending messages from Spring components, avoid using `@PostConstruct` methods if relying on automatic topic creation via `NewTopic` beans.\n`@PostConstruct` runs before the application context is fully ready, which may cause `UnknownTopicOrPartitionException` on clean brokers.\n\nInstead, consider these alternatives:\n\n- Use an `ApplicationListener<ContextRefreshedEvent>` to ensure the context is fully refreshed before sending.\n- Implement `SmartLifecycle` to start after the `KafkaAdmin` bean has completed its initialization.\n- Pre-create topics externally.\n====\n\nTo use the template, you can configure a producer factory and provide it in the template's constructor.\nThe following example shows how to do so:\n\n[source, java, subs=\"attributes\"]\n----\n@Bean\npublic ProducerFactory<Integer, String> producerFactory() {\n return new DefaultKafkaProducerFactory<>(producerConfigs());\n}\n\n@Bean\npublic Map<String, Object> producerConfigs() {\n Map<String, Object> props = new HashMap<>();\n props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\n props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n // See {kafka-url}/documentation/#producerconfigs for more properties\n return props;\n}\n\n@Bean\npublic KafkaTemplate<Integer, String> kafkaTemplate() {\n return new KafkaTemplate<Integer, String>(producerFactory());\n}\n----\n\nStarting with version 2.5, you can now override the factory's `ProducerConfig` properties to create templates with different producer configurations from the same factory.\n\n[source, java]\n----\n@Bean\npublic KafkaTemplate<String, String> stringTemplate(ProducerFactory<String, String> pf) {\n return new KafkaTemplate<>(pf);\n}\n\n@Bean\npublic KafkaTemplate<String, byte[]> bytesTemplate(ProducerFactory<String, byte[]> pf) {\n return new KafkaTemplate<>(pf,\n Collections.singletonMap(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class));\n}\n----\n\nNote that a bean of type `ProducerFactory<?, ?>` (such as the one auto-configured by Spring Boot) can be referenced with different narrowed generic types.\n\nYou can also configure the template by using standard `<bean/>` definitions.\n\nThen, to use the template, you can invoke one of its methods.\n\nWhen you use the methods with a `Message<?>` parameter, the topic, partition, key and timestamp information is provided in a message header that includes the following items:\n\n* `KafkaHeaders.TOPIC`\n* `KafkaHeaders.PARTITION`\n* `KafkaHeaders.KEY`\n* `KafkaHeaders.TIMESTAMP`\n\nThe message payload is the data.\n\nOptionally, you can configure the `KafkaTemplate` with a `ProducerListener` to get an asynchronous callback with the results of the send (success or failure) instead of waiting for the `Future` to complete.\nThe following listing shows the definition of the `ProducerListener` interface:\n\n[source, java]\n----\npublic interface ProducerListener<K, V> {\n\n default void onSuccess(ProducerRecord<K, V> producerRecord, RecordMetadata recordMetadata) {\n\t}\n\n default void onError(ProducerRecord<K, V> producerRecord, RecordMetadata recordMetadata, Exception exception) {\n\t}\n\n}\n----\n\nBy default, the template is configured with a `LoggingProducerListener`, which logs errors and does nothing when the send is successful.\n\nFor convenience, default method implementations are provided in case you want to implement only one of the methods.\n\nNotice that the send methods return a `CompletableFuture<SendResult>`.\nYou can register a callback with the listener to receive the result of the send asynchronously.\nThe following example shows how to do so:\n\n[source, java]\n----\nCompletableFuture<SendResult<Integer, String>> future = template.send(\"myTopic\", \"something\");\nfuture.whenComplete((result, ex) -> {\n ...\n});\n----\n\n`SendResult` has two properties, a `ProducerRecord` and `RecordMetadata`.\nSee the Kafka API documentation for information about those objects.\n\nThe `Throwable` can be cast to a `KafkaProducerException`; its `producerRecord` property contains the failed record.\n\nIf you wish to block the sending thread to await the result, you can invoke the future's `get()` method; using the method with a timeout is recommended.\nIf you have set a `linger.ms`, you may wish to invoke `flush()` before waiting or, for convenience, the template has a constructor with an `autoFlush` parameter that causes the template to `flush()` on each send.\nFlushing is only needed if you have set the `linger.ms` producer property and want to immediately send a partial batch.\n\n[[examples]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Overview", "heading_level": 3, "file_order": 42, "section_index": 2, "content_hash": "0f91ae4ad512f9b7b95098c060ae7fd0dab963745ba1628eb5024b72650552b1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:51337c2b1c9de4dff5aa41403ef47293c7987d9724a72ff84e519600bbe121d1", "content": "This section shows examples of sending messages to Kafka:\n\n.Non Blocking (Async)\n====\n[source, java]\n----\npublic void sendToKafka(final MyOutputData data) {\n final ProducerRecord<String, String> record = createRecord(data);\n\n CompletableFuture<SendResult<String, String>> future = template.send(record);\n future.whenComplete((result, ex) -> {\n if (ex == null) {\n handleSuccess(data);\n }\n else {\n handleFailure(data, record, ex);\n }\n });\n}\n----\n====\n\n.Blocking (Sync)\n====\n[source, java]\n----\npublic void sendToKafka(final MyOutputData data) {\n final ProducerRecord<String, String> record = createRecord(data);\n\n try {\n template.send(record).get(10, TimeUnit.SECONDS);\n handleSuccess(data);\n }\n catch (ExecutionException e) {\n handleFailure(data, record, e.getCause());\n }\n catch (TimeoutException | InterruptedException e) {\n handleFailure(data, record, e);\n }\n}\n----\n====\n\nNote that the cause of the `ExecutionException` is `KafkaProducerException` with the `producerRecord` property.\n\n[[routing-template]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Examples", "heading_level": 3, "file_order": 42, "section_index": 3, "content_hash": "51337c2b1c9de4dff5aa41403ef47293c7987d9724a72ff84e519600bbe121d1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:0917886cdb23932da3270f20add480966c88cd54bf1a9e5a6f22f44fe9379d19", "content": "Starting with version 2.5, you can use a `RoutingKafkaTemplate` to select the producer at runtime, based on the destination `topic` name.\n\nIMPORTANT: The routing template does **not** support transactions, `execute`, `flush`, or `metrics` operations because the topic is not known for those operations.\n\nThe template requires a map of `java.util.regex.Pattern` to `ProducerFactory<Object, Object>` instances.\nThis map should be ordered (e.g. a `LinkedHashMap`) because it is traversed in order; you should add more specific patterns at the beginning.\n\nThe following simple Spring Boot application provides an example of how to use the same template to send to different topics, each using a different value serializer.\n\n[source, java]\n----\n@SpringBootApplication\npublic class Application {\n\n public static void main(String[] args) {\n SpringApplication.run(Application.class, args);\n }\n\n @Bean\n public RoutingKafkaTemplate routingTemplate(GenericApplicationContext context,\n ProducerFactory<Object, Object> pf) {\n\n // Clone the PF with a different Serializer, register with Spring for shutdown\n Map<String, Object> configs = new HashMap<>(pf.getConfigurationProperties());\n configs.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n DefaultKafkaProducerFactory<Object, Object> bytesPF = new DefaultKafkaProducerFactory<>(configs);\n context.registerBean(\"bytesPF\", DefaultKafkaProducerFactory.class, () -> bytesPF);\n\n Map<Pattern, ProducerFactory<Object, Object>> map = new LinkedHashMap<>();\n map.put(Pattern.compile(\"two\"), bytesPF);\n map.put(Pattern.compile(\".+\"), pf); // Default PF with StringSerializer\n return new RoutingKafkaTemplate(map);\n }\n\n @Bean\n public ApplicationRunner runner(RoutingKafkaTemplate routingTemplate) {\n return args -> {\n routingTemplate.send(\"one\", \"thing1\");\n routingTemplate.send(\"two\", \"thing2\".getBytes());\n };\n }\n\n}\n----\n\nThe corresponding ``@KafkaListener``s for this example are shown in xref:kafka/receiving-messages/listener-annotation.adoc#annotation-properties[Annotation Properties].\n\nFor another technique to achieve similar results, but with the additional capability of sending different types to the same topic, see xref:kafka/serdes.adoc#delegating-serialization[Delegating Serializer and Deserializer].\n\n[[producer-factory]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Using `RoutingKafkaTemplate`", "heading_level": 2, "file_order": 42, "section_index": 4, "content_hash": "0917886cdb23932da3270f20add480966c88cd54bf1a9e5a6f22f44fe9379d19", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:b3895768fe0461faa403d5a3a299e00e18fb3247bfb7fbd2c1edeb764f3b1469", "content": "As seen in xref:kafka/sending-messages.adoc#kafka-template[Using `KafkaTemplate`], a `ProducerFactory` is used to create the producer.\n\nWhen not using xref:kafka/transactions.adoc[Transactions], by default, the `DefaultKafkaProducerFactory` creates a singleton producer used by all clients, as recommended in the `KafkaProducer` JavaDocs.\nHowever, if you call `flush()` on the template, this can cause delays for other threads using the same producer.\nStarting with version 2.3, the `DefaultKafkaProducerFactory` has a new property `producerPerThread`.\nWhen set to `true`, the factory will create (and cache) a separate producer for each thread, to avoid this issue.\n\nIMPORTANT: When `producerPerThread` is `true`, user code **must** call `closeThreadBoundProducer()` on the factory when the producer is no longer needed.\nThis will physically close the producer and remove it from the `ThreadLocal`.\nCalling `reset()` or `destroy()` will not clean up these producers.\n\nAlso see xref:kafka/transactions.adoc#tx-template-mixed[`KafkaTemplate` Transactional and non-Transactional Publishing].\n\nWhen creating a `DefaultKafkaProducerFactory`, key and/or value `Serializer` classes can be picked up from configuration by calling the constructor that only takes in a Map of properties (see example in xref:kafka/sending-messages.adoc#kafka-template[Using `KafkaTemplate`]), or `Serializer` instances may be passed to the `DefaultKafkaProducerFactory` constructor (in which case all ``Producer``s share the same instances).\nAlternatively you can provide ``Supplier<Serializer>``s (starting with version 2.3) that will be used to obtain separate `Serializer` instances for each `Producer`:\n\n[source, java]\n----\n\n@Bean\npublic ProducerFactory<Integer, CustomValue> producerFactory() {\n return new DefaultKafkaProducerFactory<>(producerConfigs(), null, () -> new CustomValueSerializer());\n}\n\n@Bean\npublic KafkaTemplate<Integer, CustomValue> kafkaTemplate() {\n return new KafkaTemplate<Integer, CustomValue>(producerFactory());\n}\n\n----\n\nStarting with version 2.5.10, you can now update the producer properties after the factory is created.\nThis might be useful, for example, if you have to update SSL key/trust store locations after a credentials change.\nThe changes will not affect existing producer instances; call `reset()` to close any existing producers so that new producers will be created using the new properties.\n\nNOTE: You cannot change a transactional producer factory to non-transactional, and vice-versa.\n\nTwo new methods are now provided:\n\n[source, java]\n----\nvoid updateConfigs(Map<String, Object> updates);\n\nvoid removeConfig(String configKey);\n----\n\nStarting with version 2.8, if you provide serializers as objects (in the constructor or via the setters), the factory will invoke the `configure()` method to configure them with the configuration properties.\n\n[[replying-template]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Using `DefaultKafkaProducerFactory`", "heading_level": 2, "file_order": 42, "section_index": 5, "content_hash": "b3895768fe0461faa403d5a3a299e00e18fb3247bfb7fbd2c1edeb764f3b1469", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:596d6133248335bebb8a5e3c9a6acb235522e144f4014681435f655f6a931798", "content": "Version 2.1.3 introduced a subclass of `KafkaTemplate` to provide request/reply semantics.\nThe class is named `ReplyingKafkaTemplate` and has two additional methods; the following shows the method signatures:\n\n[source, java]\n----\nRequestReplyFuture<K, V, R> sendAndReceive(ProducerRecord<K, V> record);\n\nRequestReplyFuture<K, V, R> sendAndReceive(ProducerRecord<K, V> record,\n Duration replyTimeout);\n----\n\n(Also see xref:kafka/sending-messages.adoc#exchanging-messages[Request/Reply with ``Message<?>``s]).\n\nThe result is a `CompletableFuture` that is asynchronously populated with the result (or an exception, for a timeout).\nThe result also has a `sendFuture` property, which is the result of calling `KafkaTemplate.send()`.\nYou can use this future to determine the result of the send operation.\n\nIf the first method is used, or the `replyTimeout` argument is `null`, the template's `defaultReplyTimeout` property is used (5 seconds by default).\n\nStarting with version 2.8.8, the template has a new method `waitForAssignment`.\nThis is useful if the reply container is configured with `auto.offset.reset=latest` to avoid sending a request and a reply sent before the container is initialized.\n\nIMPORTANT: When using manual partition assignment (no group management), the duration for the wait must be greater than the container's `pollTimeout` property because the notification will not be sent until after the first poll is completed.\n\nThe following Spring Boot application shows an example of how to use the feature:\n\n[source, java]\n----\n@SpringBootApplication\npublic class KRequestingApplication {\n\n public static void main(String[] args) {\n SpringApplication.run(KRequestingApplication.class, args).close();\n }\n\n @Bean\n public ApplicationRunner runner(ReplyingKafkaTemplate<String, String, String> template) {\n return args -> {\n if (!template.waitForAssignment(Duration.ofSeconds(10))) {\n throw new IllegalStateException(\"Reply container did not initialize\");\n }\n ProducerRecord<String, String> record = new ProducerRecord<>(\"kRequests\", \"foo\");\n RequestReplyFuture<String, String, String> replyFuture = template.sendAndReceive(record);\n SendResult<String, String> sendResult = replyFuture.getSendFuture().get(10, TimeUnit.SECONDS);\n System.out.println(\"Sent ok: \" + sendResult.getRecordMetadata());\n ConsumerRecord<String, String> consumerRecord = replyFuture.get(10, TimeUnit.SECONDS);\n System.out.println(\"Return value: \" + consumerRecord.value());\n };\n }\n\n @Bean\n public ReplyingKafkaTemplate<String, String, String> replyingTemplate(\n ProducerFactory<String, String> pf,\n ConcurrentMessageListenerContainer<String, String> repliesContainer) {\n\n return new ReplyingKafkaTemplate<>(pf, repliesContainer);\n }\n\n @Bean\n public ConcurrentMessageListenerContainer<String, String> repliesContainer(\n ConcurrentKafkaListenerContainerFactory<String, String> containerFactory) {\n\n ConcurrentMessageListenerContainer<String, String> repliesContainer =\n containerFactory.createContainer(\"kReplies\");\n repliesContainer.getContainerProperties().setGroupId(\"repliesGroup\");\n repliesContainer.setAutoStartup(false);\n return repliesContainer;\n }\n\n @Bean\n public NewTopic kRequests() {\n return TopicBuilder.name(\"kRequests\")\n .partitions(10)\n .replicas(2)\n .build();\n }\n\n @Bean\n public NewTopic kReplies() {\n return TopicBuilder.name(\"kReplies\")\n .partitions(10)\n .replicas(2)\n .build();\n }\n\n}\n----\n\nNote that we can use Boot's auto-configured container factory to create the reply container.\n\nIf a non-trivial deserializer is being used for replies, consider using an xref:kafka/serdes.adoc#error-handling-deserializer[`ErrorHandlingDeserializer`] that delegates to your configured deserializer.\nWhen so configured, the `RequestReplyFuture` will be completed exceptionally and you can catch the `ExecutionException`, with the `DeserializationException` in its `cause` property.\n\nStarting with version 2.6.7, in addition to detecting ``DeserializationException``s, the template will call the `replyErrorChecker` function, if provided.\nIf it returns an exception, the future will be completed exceptionally.\n\nHere is an example:\n\n[source, java]\n----\ntemplate.setReplyErrorChecker(record -> {\n Header error = record.headers().lastHeader(\"serverSentAnError\");\n if (error != null) {\n return new MyException(new String(error.value()));\n }\n else {\n return null;\n }\n});\n\n...\n\nRequestReplyFuture<Integer, String, String> future = template.sendAndReceive(record);\ntry {\n future.getSendFuture().get(10, TimeUnit.SECONDS); // send ok\n ConsumerRecord<Integer, String> consumerRecord = future.get(10, TimeUnit.SECONDS);\n ...\n}\ncatch (InterruptedException e) {\n ...\n}\ncatch (ExecutionException e) {\n if (e.getCause() instanceof MyException) {\n ...\n }\n}\ncatch (TimeoutException e) {\n ...\n}\n----\n\nThe template sets a header (named `KafkaHeaders.CORRELATION_ID` by default), which must be echoed back by the server side.\n\nIn this case, the following `@KafkaListener` application responds:\n\n[source, java]\n----\n@SpringBootApplication\npublic class KReplyingApplication {\n\n public static void main(String[] args) {\n SpringApplication.run(KReplyingApplication.class, args);\n }\n\n @KafkaListener(id=\"server\", topics = \"kRequests\")\n @SendTo // use default replyTo expression\n public String listen(String in) {\n System.out.println(\"Server received: \" + in);\n return in.toUpperCase();\n }\n\n @Bean\n public NewTopic kRequests() {\n return TopicBuilder.name(\"kRequests\")\n .partitions(10)\n .replicas(2)\n .build();\n }\n\n @Bean // not required if Jackson is on the classpath\n public MessagingMessageConverter simpleMapperConverter() {\n MessagingMessageConverter messagingMessageConverter = new MessagingMessageConverter();\n messagingMessageConverter.setHeaderMapper(new SimpleKafkaHeaderMapper());\n return messagingMessageConverter;\n }\n\n}\n----\n\nThe `@KafkaListener` infrastructure echoes the correlation ID and determines the reply topic.\n\nSee xref:kafka/receiving-messages/annotation-send-to.adoc[Forwarding Listener Results using `@SendTo`] for more information about sending replies.\nThe template uses the default header `KafKaHeaders.REPLY_TOPIC` to indicate the topic to which the reply goes.\n\nStarting with version 2.2, the template tries to detect the reply topic or partition from the configured reply container.\nIf the container is configured to listen to a single topic or a single `TopicPartitionOffset`, it is used to set the reply headers.\nIf the container is configured otherwise, the user must set up the reply headers.\nIn this case, an `INFO` log message is written during initialization.\nThe following example uses `KafkaHeaders.REPLY_TOPIC`:\n\n[source, java]\n----\nrecord.headers().add(new RecordHeader(KafkaHeaders.REPLY_TOPIC, \"kReplies\".getBytes()));\n----\n\nWhen you configure with a single reply `TopicPartitionOffset`, you can use the same reply topic for multiple templates, as long as each instance listens on a different partition.\nWhen configuring with a single reply topic, each instance must use a different `group.id`.\nIn this case, all instances receive each reply, but only the instance that sent the request finds the correlation ID.\nThis may be useful for auto-scaling, but with the overhead of additional network traffic and the small cost of discarding each unwanted reply.\nWhen you use this setting, we recommend that you set the template's `sharedReplyTopic` to `true`, which reduces the logging level of unexpected replies to DEBUG instead of the default ERROR.\n\nThe following is an example of configuring the reply container to use the same shared reply topic:\n\n[source, java]\n----\n@Bean\npublic ConcurrentMessageListenerContainer<String, String> replyContainer(\n ConcurrentKafkaListenerContainerFactory<String, String> containerFactory) {\n\n ConcurrentMessageListenerContainer<String, String> container = containerFactory.createContainer(\"topic2\");\n container.getContainerProperties().setGroupId(UUID.randomUUID().toString()); // unique\n Properties props = new Properties();\n props.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"latest\"); // so the new group doesn't get old replies\n container.getContainerProperties().setKafkaConsumerProperties(props);\n return container;\n}\n----\n\nIMPORTANT: If you have multiple client instances and you do not configure them as discussed in the preceding paragraph, each instance needs a dedicated reply topic.\nAn alternative is to set the `KafkaHeaders.REPLY_PARTITION` and use a dedicated partition for each instance.\nThe `Header` contains a four-byte int (big-endian).\nThe server must use this header to route the reply to the correct partition (`@KafkaListener` does this).\nIn this case, though, the reply container must not use Kafka's group management feature and must be configured to listen on a fixed partition (by using a `TopicPartitionOffset` in its `ContainerProperties` constructor).\n\nNOTE: The `JsonKafkaHeaderMapper` requires Jackson to be on the classpath (for the `@KafkaListener`).\nIf it is not available, the message converter has no header mapper, so you must configure a `MessagingMessageConverter` with a `SimpleKafkaHeaderMapper`, as shown earlier.\n\nBy default, 3 headers are used:\n\n* `KafkaHeaders.CORRELATION_ID` - used to correlate the reply to a request\n* `KafkaHeaders.REPLY_TOPIC` - used to tell the server where to reply\n* `KafkaHeaders.REPLY_PARTITION` - (optional) used to tell the server which partition to reply to\n\nThese header names are used by the `@KafkaListener` infrastructure to route the reply.\n\nStarting with version 2.3, you can customize the header names - the template has 3 properties `correlationHeaderName`, `replyTopicHeaderName`, and `replyPartitionHeaderName`.\nThis is useful if your server is not a Spring application (or does not use the `@KafkaListener`).\n\nNOTE: Conversely, if the requesting application is not a spring application and puts correlation information in a different header, starting with version 3.0, you can configure a custom `correlationHeaderName` on the listener container factory and that header will be echoed back.\nPreviously, the listener had to echo custom correlation headers.\n\n[[exchanging-messages]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Using `ReplyingKafkaTemplate`", "heading_level": 2, "file_order": 42, "section_index": 6, "content_hash": "596d6133248335bebb8a5e3c9a6acb235522e144f4014681435f655f6a931798", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:fe31a63598ae830ff1a0c604e2fc432a663cecf1ff5f3289e4f62ff890ef4789", "content": "Version 2.7 added methods to the `ReplyingKafkaTemplate` to send and receive ``spring-messaging``'s `Message<?>` abstraction:\n\n[source, java]\n----\nRequestReplyMessageFuture<K, V> sendAndReceive(Message<?> message);\n\n<P> RequestReplyTypedMessageFuture<K, V, P> sendAndReceive(Message<?> message,\n ParameterizedTypeReference<P> returnType);\n----\n\nThese will use the template's default `replyTimeout`, there are also overloaded versions that can take a timeout in the method call.\n\nUse the first method if the consumer's `Deserializer` or the template's `MessageConverter` can convert the payload without any additional information, either via configuration or type metadata in the reply message.\n\nUse the second method if you need to provide type information for the return type, to assist the message converter.\nThis also allows the same template to receive different types, even if there is no type metadata in the replies, such as when the server side is not a Spring application.\nThe following is an example of the latter:\n\n.Template Bean\n[tabs]\n======\nJava::\n+\n[source, java, role=\"primary\", indent=0]\n----\ninclude::{java-examples}/requestreply/Application.java[tag=beans]\n----\n\nKotlin::\n+\n[source, kotlin, role=\"secondary\",indent=0]\n----\ninclude::{kotlin-examples}/requestreply/Application.kt[tag=beans]\n----\n======\n\n.Using the template\n[tabs]\n======\nJava::\n+\n[source, java, role=\"primary\", indent=0]\n----\ninclude::{java-examples}/requestreply/Application.java[tag=sendReceive]\n----\n\nKotlin::\n+\n[source, kotlin, role=\"secondary\", indent=0]\n----\ninclude::{kotlin-examples}/requestreply/Application.kt[tag=sendReceive]\n----\n======\n\n[[reply-message]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Request/Reply with ``Message<?>``s", "heading_level": 3, "file_order": 42, "section_index": 7, "content_hash": "fe31a63598ae830ff1a0c604e2fc432a663cecf1ff5f3289e4f62ff890ef4789", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:6c29c8a2a5d67dd83ce858caad244e9ae85f5ec3620e0090773df83909d0daeb", "content": "When the `@KafkaListener` returns a `Message<?>`, with versions before 2.5, it was necessary to populate the reply topic and correlation id headers.\nIn this example, we use the reply topic header from the request:\n\n[source, java]\n----\n@KafkaListener(id = \"requestor\", topics = \"request\")\n@SendTo\npublic Message<?> messageReturn(String in) {\n return MessageBuilder.withPayload(in.toUpperCase())\n .setHeader(KafkaHeaders.TOPIC, replyTo)\n .setHeader(KafkaHeaders.KEY, 42)\n .setHeader(KafkaHeaders.CORRELATION_ID, correlation)\n .build();\n}\n----\n\nThis also shows how to set a key on the reply record.\n\nStarting with version 2.5, the framework will detect if these headers are missing and populate them with the topic - either the topic determined from the `@SendTo` value or the incoming `KafkaHeaders.REPLY_TOPIC` header (if present).\nIt will also echo the incoming `KafkaHeaders.CORRELATION_ID` and `KafkaHeaders.REPLY_PARTITION`, if present.\n\n[source, java]\n----\n@KafkaListener(id = \"requestor\", topics = \"request\")\n@SendTo // default REPLY_TOPIC header\npublic Message<?> messageReturn(String in) {\n return MessageBuilder.withPayload(in.toUpperCase())\n .setHeader(KafkaHeaders.KEY, 42)\n .build();\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Reply Type Message<?>", "heading_level": 2, "file_order": 42, "section_index": 8, "content_hash": "6c29c8a2a5d67dd83ce858caad244e9ae85f5ec3620e0090773df83909d0daeb", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:1082c53c1a93ece9e97569b19e65d07201beef5dd32482867f996a15f3d5bf93", "content": "Starting with version 3.3, the Kafka record key from the incoming request (if it exists) will be preserved in the reply record.\nThis is only applicable for single record request/reply scenario.\nWhen the listener is batch or when the return type is a collection, it is up to the application to specify which keys to use by wrapping the reply record in a `Message` type.\n\n[[aggregating-request-reply]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Original Record Key in Reply", "heading_level": 3, "file_order": 42, "section_index": 9, "content_hash": "1082c53c1a93ece9e97569b19e65d07201beef5dd32482867f996a15f3d5bf93", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:18268f0253e48663b52f1f7ceb5afdc3e6a89ebccedc17405b7383fcd513a9bb", "content": "The template in xref:kafka/sending-messages.adoc#replying-template[Using `ReplyingKafkaTemplate`] is strictly for a single request/reply scenario.\nFor cases where multiple receivers of a single message return a reply, you can use the `AggregatingReplyingKafkaTemplate`.\nThis is an implementation of the client-side of the https://www.enterpriseintegrationpatterns.com/patterns/messaging/BroadcastAggregate.html[Scatter-Gather Enterprise Integration Pattern].\n\nLike the `ReplyingKafkaTemplate`, the `AggregatingReplyingKafkaTemplate` constructor takes a producer factory and a listener container to receive the replies; it has a third parameter `BiPredicate<List<ConsumerRecord<K, R>>, Boolean> releaseStrategy` which is consulted each time a reply is received; when the predicate returns `true`, the collection of ``ConsumerRecord``s is used to complete the `Future` returned by the `sendAndReceive` method.\n\nThere is an additional property `returnPartialOnTimeout` (default false).\nWhen this is set to `true`, instead of completing the future with a `KafkaReplyTimeoutException`, a partial result completes the future normally (as long as at least one reply record has been received).\n\nStarting with version 2.3.5, the predicate is also called after a timeout (if `returnPartialOnTimeout` is `true`).\nThe first argument is the current list of records; the second is `true` if this call is due to a timeout.\nThe predicate can modify the list of records.\n\n[source, java]\n----\nAggregatingReplyingKafkaTemplate<Integer, String, String> template =\n new AggregatingReplyingKafkaTemplate<>(producerFactory, container,\n coll -> coll.size() == releaseSize);\n...\nRequestReplyFuture<Integer, String, Collection<ConsumerRecord<Integer, String>>> future =\n template.sendAndReceive(record);\nfuture.getSendFuture().get(10, TimeUnit.SECONDS); // send ok\nConsumerRecord<Integer, Collection<ConsumerRecord<Integer, String>>> consumerRecord =\n future.get(30, TimeUnit.SECONDS);\n----\n\nNotice that the return type is a `ConsumerRecord` with a value that is a collection of ``ConsumerRecord``s.\nThe \"outer\" `ConsumerRecord` is not a \"real\" record, it is synthesized by the template, as a holder for the actual reply records received for the request.\nWhen a normal release occurs (release strategy returns true), the topic is set to `aggregatedResults`; if `returnPartialOnTimeout` is true, and timeout occurs (and at least one reply record has been received), the topic is set to `partialResultsAfterTimeout`.\nThe template provides constant static variables for these \"topic\" names:\n\n[source, java]\n----\n/**\n * Pseudo topic name for the \"outer\" {@link ConsumerRecords} that has the aggregated\n * results in its value after a normal release by the release strategy.\n */\npublic static final String AGGREGATED_RESULTS_TOPIC = \"aggregatedResults\";\n\n/**\n * Pseudo topic name for the \"outer\" {@link ConsumerRecords} that has the aggregated\n * results in its value after a timeout.\n */\npublic static final String PARTIAL_RESULTS_AFTER_TIMEOUT_TOPIC = \"partialResultsAfterTimeout\";\n----\n\nThe real ``ConsumerRecord``s in the `Collection` contain the actual topic(s) from which the replies are received.\n\nIMPORTANT: The listener container for the replies **must** be configured with `AckMode.MANUAL` or `AckMode.MANUAL_IMMEDIATE`; the consumer property `enable.auto.commit` must be `false` (the default since version 2.3).\nTo avoid any possibility of losing messages, the template only commits offsets when there are zero requests outstanding, i.e. when the last outstanding request is released by the release strategy.\nAfter a rebalance, it is possible for duplicate reply deliveries; these will be ignored for any in-flight requests; you may see error log messages when duplicate replies are received for already released replies.\n\nNOTE: If you use an xref:kafka/serdes.adoc#error-handling-deserializer[`ErrorHandlingDeserializer`] with this aggregating template, the framework will not automatically detect ``DeserializationException``s.\nInstead, the record (with a `null` value) will be returned intact, with the deserialization exception(s) in headers.\nIt is recommended that applications call the utility method `ReplyingKafkaTemplate.checkDeserialization()` method to determine if a deserialization exception occurred.\nSee its JavaDocs for more information.\nThe `replyErrorChecker` is also not called for this aggregating template; you should perform the checks on each element of the reply.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/sending-messages.adoc", "title": "sending-messages", "heading": "Aggregating Multiple Replies", "heading_level": 2, "file_order": 42, "section_index": 10, "content_hash": "18268f0253e48663b52f1f7ceb5afdc3e6a89ebccedc17405b7383fcd513a9bb", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/sending-messages.adoc"}}
{"id": "sha256:e2d71761009c86fbe586df42c77267535997618d332a926be760d9533265135b", "content": "[[serdes]]\n\n[[overview]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "serdes", "heading_level": 1, "file_order": 43, "section_index": 0, "content_hash": "e2d71761009c86fbe586df42c77267535997618d332a926be760d9533265135b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:b07146be22a7f5070ffd9ce30623b6fcaf13f321e3ec0afb678d9b26b11b22bb", "content": "Apache Kafka provides a high-level API for serializing and deserializing record values as well as their keys.\nIt is present with the `org.apache.kafka.common.serialization.Serializer<T>` and\n`org.apache.kafka.common.serialization.Deserializer<T>` abstractions with some built-in implementations.\nMeanwhile, we can specify serializer and deserializer classes by using `Producer` or `Consumer` configuration properties.\nThe following example shows how to do so:\n\n[source, java]\n----\nprops.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, IntegerDeserializer.class);\nprops.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n...\nprops.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, IntegerSerializer.class);\nprops.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);\n----\n\nFor more complex or particular cases, the `KafkaConsumer` (and, therefore, `KafkaProducer`) provides overloaded\nconstructors to accept `Serializer` and `Deserializer` instances for `keys` and `values`, respectively.\n\nWhen you use this API, the `DefaultKafkaProducerFactory` and `DefaultKafkaConsumerFactory` also provide properties (through constructors or setter methods) to inject custom `Serializer` and `Deserializer` instances into the target `Producer` or `Consumer`.\nAlso, you can pass in `Supplier<Serializer>` or `Supplier<Deserializer>` instances through constructors - these ``Supplier``s are called on creation of each `Producer` or `Consumer`.\n\n[[string-serde]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Overview", "heading_level": 2, "file_order": 43, "section_index": 1, "content_hash": "b07146be22a7f5070ffd9ce30623b6fcaf13f321e3ec0afb678d9b26b11b22bb", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:e0234c5e570b2d7633378a8cc6ef00225db401b337aa3c8b403e9f9e597e1fe8", "content": "Since version 2.5, Spring for Apache Kafka provides `ToStringSerializer` and `ParseStringDeserializer` classes that use String representation of entities.\nThey rely on methods `toString` and some `Function<String>` or `BiFunction<String, Headers>` to parse the String and populate properties of an instance.\nUsually, this would invoke some static method on the class, such as `parse`:\n\n[source, java]\n----\nToStringSerializer<Thing> thingSerializer = new ToStringSerializer<>();\nParseStringDeserializer<Thing> deserializer = new ParseStringDeserializer<>(Thing::parse);\n----\n\nBy default, the `ToStringSerializer` is configured to convey type information about the serialized entity in the record `Headers`.\nYou can disable this by setting the `addTypeInfo` property to `false`.\nThis information can be used by `ParseStringDeserializer` on the receiving side.\n\n* `ToStringSerializer.ADD_TYPE_INFO_HEADERS` (default `true`): You can set it to `false` to disable this feature on the `ToStringSerializer` (sets the `addTypeInfo` property).\n\n[source, java]\n----\nParseStringDeserializer<Object> deserializer = new ParseStringDeserializer<>((str, headers) -> {\n byte[] header = headers.lastHeader(ToStringSerializer.VALUE_TYPE).value();\n String entityType = new String(header);\n\n if (entityType.contains(\"Thing\")) {\n return Thing.parse(str);\n }\n else {\n // ...parsing logic\n }\n});\n----\n\nYou can configure the `Charset` used to convert `String` to/from `byte[]` with the default being `UTF-8`.\n\nYou can configure the deserializer with the name of the parser method using `ConsumerConfig` properties:\n\n* `ParseStringDeserializer.KEY_PARSER`\n* `ParseStringDeserializer.VALUE_PARSER`\n\nThe properties must contain the fully qualified name of the class followed by the method name, separated by a period `.`.\nThe method must be static and have a signature of either `(String, Headers)` or `(String)`.\n\nA `ToFromStringSerde` is also provided, for use with Kafka Streams.\n\n[[json-serde]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "String serialization", "heading_level": 2, "file_order": 43, "section_index": 2, "content_hash": "e0234c5e570b2d7633378a8cc6ef00225db401b337aa3c8b403e9f9e597e1fe8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:88b41c37a1a300c6e2a997ecb72e5c74266961619509591c6328452098efd8fc", "content": "Spring for Apache Kafka also provides `JacksonJsonSerializer` and `JacksonJsonDeserializer` implementations that are based on the\nJackson JSON object mapper.\nThe `JacksonJsonSerializer` allows writing any Java object as a JSON `byte[]`.\nThe `JacksonJsonDeserializer` requires an additional `Class<?> targetType` argument to allow the deserialization of a consumed `byte[]` to the proper target object.\nThe following example shows how to create a `JacksonJsonDeserializer`:\n\n[source, java]\n----\nJacksonJsonDeserializer<Thing> thingDeserializer = new JacksonJsonDeserializer<>(Thing.class);\n----\n\nYou can customize both `JacksonJsonSerializer` and `JacksonJsonDeserializer` with an `ObjectMapper`.\nYou can also extend them to implement some particular configuration logic in the `configure(Map<String, ?> configs, boolean isKey)` method.\n\nStarting with version 2.3, all the JSON-aware components are configured by default with a `JacksonUtils.enhancedObjectMapper()` instance, which comes with the `MapperFeature.DEFAULT_VIEW_INCLUSION` and `DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES` features disabled.\nAlso such an instance is supplied with well-known modules for custom data types, such a Java time and Kotlin support.\nSee `JacksonUtils.enhancedObjectMapper()` JavaDocs for more information.\nThis method also registers a `org.springframework.kafka.support.JacksonMimeTypeModule` for `org.springframework.util.MimeType` objects serialization into the plain string for inter-platform compatibility over the network.\nA `JacksonMimeTypeModule` can be registered as a bean in the application context and it will be auto-configured into the {spring-boot-url}/how-to/spring-mvc.html#howto.spring-mvc.customize-jackson-objectmapper[Spring Boot `ObjectMapper` instance].\n\nAlso starting with version 2.3, the `JsonDeserializer` provides `TypeReference`-based constructors for better handling of target generic container types.\n\nStarting with version 2.1, you can convey type information in record `Headers`, allowing the handling of multiple types.\nIn addition, you can configure the serializer and deserializer by using the following Kafka properties.\nThey have no effect if you have provided `Serializer` and `Deserializer` instances for `KafkaConsumer` and `KafkaProducer`, respectively.\n\n[[serdes-json-config]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "JSON", "heading_level": 2, "file_order": 43, "section_index": 3, "content_hash": "88b41c37a1a300c6e2a997ecb72e5c74266961619509591c6328452098efd8fc", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:72bb4213195eff9db3fc16fcec98cebedc88b6be141c93363fafa13ea409fe3d", "content": "* `JacksonJsonSerializer.ADD_TYPE_INFO_HEADERS` (default `true`): You can set it to `false` to disable this feature on the `JacksonJsonSerializer` (sets the `addTypeInfo` property).\n* `JacksonJsonSerializer.TYPE_MAPPINGS` (default `empty`): See xref:kafka/serdes.adoc#serdes-mapping-types[Mapping Types].\n* `JacksonJsonDeserializer.USE_TYPE_INFO_HEADERS` (default `true`): You can set it to `false` to ignore headers set by the serializer.\n* `JacksonJsonDeserializer.REMOVE_TYPE_INFO_HEADERS` (default `true`): You can set it to `false` to retain headers set by the serializer.\n* `JacksonJsonDeserializer.KEY_DEFAULT_TYPE`: Fallback type for deserialization of keys if no header information is present.\n* `JacksonJsonDeserializer.VALUE_DEFAULT_TYPE`: Fallback type for deserialization of values if no header information is present.\n* `JacksonJsonDeserializer.TRUSTED_PACKAGES` (default `java.util`, `java.lang`): Comma-delimited list of package patterns allowed for deserialization.\n`*` means deserializing all.\n* `JacksonJsonDeserializer.TYPE_MAPPINGS` (default `empty`): See xref:kafka/serdes.adoc#serdes-mapping-types[Mapping Types].\n* `JacksonJsonDeserializer.KEY_TYPE_METHOD` (default `empty`): See xref:kafka/serdes.adoc#serdes-type-methods[Using Methods to Determine Types].\n* `JacksonJsonDeserializer.VALUE_TYPE_METHOD` (default `empty`): See xref:kafka/serdes.adoc#serdes-type-methods[Using Methods to Determine Types].\n\nStarting with version 2.2, the type information headers (if added by the serializer) are removed by the deserializer.\nYou can revert to the previous behavior by setting the `removeTypeHeaders` property to `false`, either directly on the deserializer or with the configuration property described earlier.\n\nSee also xref:tips.adoc#tip-json[Customizing the JacksonJsonSerializer and JacksonJsonDeserializer].\n\nIMPORTANT: Starting with version 2.8, if you construct the serializer or deserializer programmatically as shown in xref:kafka/serdes.adoc#prog-json[Programmatic Construction], the above properties will be applied by the factories, as long as you have not set any properties explicitly (using `set*()` methods or using the fluent API).\nPreviously, when creating programmatically, the configuration properties were never applied; this is still the case if you explicitly set properties on the object directly.\n\n[[serdes-mapping-types]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Configuration Properties", "heading_level": 3, "file_order": 43, "section_index": 4, "content_hash": "72bb4213195eff9db3fc16fcec98cebedc88b6be141c93363fafa13ea409fe3d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:fff9bacdd5427bf0b1a329ee13e9cf77d7841644dfe3eb411bf1c463f3b7e4b5", "content": "Starting with version 2.2, when using JSON, you can now provide type mappings by using the properties in the preceding list.\nPreviously, you had to customize the type mapper within the serializer and deserializer.\nMappings consist of a comma-delimited list of `token:className` pairs.\nOn outbound, the payload's class name is mapped to the corresponding token.\nOn inbound, the token in the type header is mapped to the corresponding class name.\n\nThe following example creates a set of mappings:\n\n[source, java]\n----\nsenderProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class);\nsenderProps.put(JsonSerializer.TYPE_MAPPINGS, \"cat:com.mycat.Cat, hat:com.myhat.Hat\");\n...\nconsumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, JsonDeserializer.class);\nconsumerProps.put(JsonDeserializer.TYPE_MAPPINGS, \"cat:com.yourcat.Cat, hat:com.yourhat.Hat\");\n----\n\nIMPORTANT: The corresponding objects must be compatible.\n\nIf you use {spring-boot-url}/reference/messaging/kafka.html[Spring Boot], you can provide these properties in the `application.properties` (or yaml) file.\nThe following example shows how to do so:\n\n[source]\n----\nspring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer\nspring.kafka.producer.properties.spring.json.type.mapping=cat:com.mycat.Cat,hat:com.myhat.Hat\n----\n\n[IMPORTANT]\n====\nYou can perform only simple configuration with properties.\nFor more advanced configuration (such as using a custom `ObjectMapper` in the serializer and deserializer), you should use the producer and consumer factory constructors that accept a pre-built serializer and deserializer.\nThe following Spring Boot example overrides the default factories:\n\n=====\n[source, java]\n----\n@Bean\npublic ConsumerFactory<?, ?> kafkaConsumerFactory(JsonDeserializer customValueDeserializer) {\n Map<String, Object> properties = new HashMap<>();\n // properties.put(..., ...)\n // ...\n return new DefaultKafkaConsumerFactory<>(properties,\n new StringDeserializer(), customValueDeserializer);\n}\n\n@Bean\npublic ProducerFactory<?, ?> kafkaProducerFactory(JsonSerializer customValueSerializer) {\n return new DefaultKafkaProducerFactory<>(properties.buildProducerProperties(),\n new StringSerializer(), customValueSerializer);\n}\n----\n=====\n\nSetters are also provided, as an alternative to using these constructors.\n====\n\nNOTE: When using Spring Boot and overriding the `ConsumerFactory` and `ProducerFactory` as shown above, wild card generic types need to be used with the bean method return type.\nIf concrete generic types are provided instead, then Spring Boot will ignore these beans and still use the default ones.\n\nStarting with version 2.2, you can explicitly configure the deserializer to use the supplied target type and ignore type information in headers by using one of the overloaded constructors that have a boolean `useHeadersIfPresent` argument (which is `true` by default).\nThe following example shows how to do so:\n\n[source, java]\n----\nDefaultKafkaConsumerFactory<Integer, Cat1> cf = new DefaultKafkaConsumerFactory<>(props,\n new IntegerDeserializer(), new JacksonJsonDeserializer<>(Cat1.class, false));\n----\n\n[[serdes-type-methods]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Mapping Types", "heading_level": 3, "file_order": 43, "section_index": 5, "content_hash": "fff9bacdd5427bf0b1a329ee13e9cf77d7841644dfe3eb411bf1c463f3b7e4b5", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:2a89608683ab80d2387ea613aeb8be5c0585d624146b78d6b0bde8b5cf586d48", "content": "Starting with version 2.5, you can now configure the deserializer, via properties, to invoke a method to determine the target type.\nIf present, this will override any of the other techniques discussed above.\nThis can be useful if the data is published by an application that does not use the Spring serializer and you need to deserialize to different types depending on the data, or other headers.\nSet these properties to the method name - a fully qualified class name followed by the method name, separated by a period `.`.\nThe method must be declared as `public static`, have one of three signatures `(String topic, byte[] data, Headers headers)`, `(byte[] data, Headers headers)` or `(byte[] data)` and return a Jackson `JavaType`.\n\n* `JsonDeserializer.KEY_TYPE_METHOD` : `spring.json.key.type.method`\n* `JsonDeserializer.VALUE_TYPE_METHOD` : `spring.json.value.type.method`\n\nYou can use arbitrary headers or inspect the data to determine the type.\n\n[source, java]\n----\nJavaType thing1Type = TypeFactory.defaultInstance().constructType(Thing1.class);\n\nJavaType thing2Type = TypeFactory.defaultInstance().constructType(Thing2.class);\n\npublic static JavaType thingOneOrThingTwo(byte[] data, Headers headers) {\n // {\"thisIsAFieldInThing1\":\"value\", ...\n if (data[21] == '1') {\n return thing1Type;\n }\n else {\n return thing2Type;\n }\n}\n----\n\nFor more sophisticated data inspection consider using `JsonPath` or similar but, the simpler the test to determine the type, the more efficient the process will be.\n\nThe following is an example of creating the deserializer programmatically (when providing the consumer factory with the deserializer in the constructor):\n\n[source, java]\n----\nJacksonJsonDeserializer<Object> deser = new JacksonJsonDeserializer<>()\n .trustedPackages(\"*\")\n .typeResolver(SomeClass::thing1Thing2JavaTypeForTopic);\n\n...\n\npublic static JavaType thing1Thing2JavaTypeForTopic(String topic, byte[] data, Headers headers) {\n ...\n}\n----\n\n[[prog-json]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Using Methods to Determine Types", "heading_level": 3, "file_order": 43, "section_index": 6, "content_hash": "2a89608683ab80d2387ea613aeb8be5c0585d624146b78d6b0bde8b5cf586d48", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:37c40a13fbde8d6783a9a921437c806e803438a8ef40898c0163fbda135766b8", "content": "When constructing the serializer/deserializer programmatically for use in the producer/consumer factory, since version 2.3, you can use the fluent API, which simplifies configuration.\n\n[source, java]\n----\n@Bean\npublic ProducerFactory<MyKeyType, MyValueType> pf() {\n Map<String, Object> props = new HashMap<>();\n // props.put(..., ...)\n // ...\n DefaultKafkaProducerFactory<MyKeyType, MyValueType> pf = new DefaultKafkaProducerFactory<>(props,\n new JacksonJsonSerializer<MyKeyType>()\n .forKeys()\n .noTypeInfo(),\n new JacksonJsonSerializer<MyValueType>()\n .noTypeInfo());\n return pf;\n}\n\n@Bean\npublic ConsumerFactory<MyKeyType, MyValueType> cf() {\n Map<String, Object> props = new HashMap<>();\n // props.put(..., ...)\n // ...\n DefaultKafkaConsumerFactory<MyKeyType, MyValueType> cf = new DefaultKafkaConsumerFactory<>(props,\n new JacksonJsonDeserializer<>(MyKeyType.class)\n .forKeys()\n .ignoreTypeHeaders(),\n new JacksonJsonDeserializer<>(MyValueType.class)\n .ignoreTypeHeaders());\n return cf;\n}\n----\n\nTo provide type mapping programmatically, similar to xref:kafka/serdes.adoc#serdes-type-methods[Using Methods to Determine Types], use the `typeFunction` property.\n\n[source, java]\n----\nJacksonJsonDeserializer<Object> deser = new JacksonJsonDeserializer<>()\n .trustedPackages(\"*\")\n .typeFunction(MyUtils::thingOneOrThingTwo);\n----\n\nAlternatively, as long as you don't use the fluent API to configure properties, or set them using `set*()` methods, the factories will configure the serializer/deserializer using the configuration properties; see xref:kafka/serdes.adoc#serdes-json-config[Configuration Properties].\n\n[[delegating-serialization]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Programmatic Construction", "heading_level": 3, "file_order": 43, "section_index": 7, "content_hash": "37c40a13fbde8d6783a9a921437c806e803438a8ef40898c0163fbda135766b8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:787d17f2b906857a105e745bc142dd4a0ff9432b5ace48587f6bb7704418afc8", "content": "[[using-headers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Delegating Serializer and Deserializer", "heading_level": 2, "file_order": 43, "section_index": 8, "content_hash": "787d17f2b906857a105e745bc142dd4a0ff9432b5ace48587f6bb7704418afc8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:bd446966748574dc51e4bde82fca05f1d40d983e729f505ccc3e4d5e797f5a08", "content": "Version 2.3 introduced the `DelegatingSerializer` and `DelegatingDeserializer`, which allow producing and consuming records with different key and/or value types.\nProducers must set a header `DelegatingSerializer.VALUE_SERIALIZATION_SELECTOR` to a selector value that is used to select which serializer to use for the value and `DelegatingSerializer.KEY_SERIALIZATION_SELECTOR` for the key; if a match is not found, an `IllegalStateException` is thrown.\n\nFor incoming records, the deserializer uses the same headers to select the deserializer to use; if a match is not found or the header is not present, the raw `byte[]` is returned.\n\nYou can configure the map of selector to `Serializer` / `Deserializer` via a constructor, or you can configure it via Kafka producer/consumer properties with the keys `DelegatingSerializer.VALUE_SERIALIZATION_SELECTOR_CONFIG` and `DelegatingSerializer.KEY_SERIALIZATION_SELECTOR_CONFIG`.\nFor the serializer, the producer property can be a `Map<String, Object>` where the key is the selector and the value is a `Serializer` instance, a serializer `Class` or the class name.\nThe property can also be a String of comma-delimited map entries, as shown below.\n\nFor the deserializer, the consumer property can be a `Map<String, Object>` where the key is the selector and the value is a `Deserializer` instance, a deserializer `Class` or the class name.\nThe property can also be a String of comma-delimited map entries, as shown below.\n\nTo configure using properties, use the following syntax:\n\n[source, java]\n----\nproducerProps.put(DelegatingSerializer.VALUE_SERIALIZATION_SELECTOR_CONFIG,\n \"thing1:com.example.MyThing1Serializer, thing2:com.example.MyThing2Serializer\")\n\nconsumerProps.put(DelegatingDeserializer.VALUE_SERIALIZATION_SELECTOR_CONFIG,\n \"thing1:com.example.MyThing1Deserializer, thing2:com.example.MyThing2Deserializer\")\n----\n\nProducers would then set the `DelegatingSerializer.VALUE_SERIALIZATION_SELECTOR` header to `thing1` or `thing2`.\n\nThis technique supports sending different types to the same topic (or different topics).\n\nNOTE: Starting with version 2.5.1, it is not necessary to set the selector header, if the type (key or value) is one of the standard types supported by `Serdes` (`Long`, `Integer`, etc).\nInstead, the serializer will set the header to the class name of the type.\nIt is not necessary to configure serializers or deserializers for these types, they will be created (once) dynamically.\n\nFor another technique to send different types to different topics, see xref:kafka/sending-messages.adoc#routing-template[Using `RoutingKafkaTemplate`].\n\n[[by-type]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Using Headers", "heading_level": 3, "file_order": 43, "section_index": 9, "content_hash": "bd446966748574dc51e4bde82fca05f1d40d983e729f505ccc3e4d5e797f5a08", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:78d41dcaf2d6a81523b65a362bfdcc59285edf61a9c9817087f9c9e5c4f3877d", "content": "Version 2.8 introduced the `DelegatingByTypeSerializer`.\n\n[source, java]\n----\n@Bean\npublic ProducerFactory<Integer, Object> producerFactory(Map<String, Object> config) {\n return new DefaultKafkaProducerFactory<>(config,\n null, new DelegatingByTypeSerializer(Map.of(\n byte[].class, new ByteArraySerializer(),\n Bytes.class, new BytesSerializer(),\n String.class, new StringSerializer())));\n}\n----\n\nStarting with version 2.8.3, you can configure the serializer to check if the map key is assignable from the target object, useful when a delegate serializer can serialize sub classes.\nIn this case, if there are ambiguous matches, an ordered `Map`, such as a `LinkedHashMap` should be provided.\n\n[[by-topic]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "By Type", "heading_level": 3, "file_order": 43, "section_index": 10, "content_hash": "78d41dcaf2d6a81523b65a362bfdcc59285edf61a9c9817087f9c9e5c4f3877d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:db9255129feb49b328b6e058524cb9c94b4f1ccd5081d8ebf64396d892b4fab8", "content": "Starting with version 2.8, the `DelegatingByTopicSerializer` and `DelegatingByTopicDeserializer` allow selection of a serializer/deserializer based on the topic name.\nRegex ``Pattern``s are used to lookup the instance to use.\nThe map can be configured using a constructor, or via properties (a comma delimited list of `pattern:serializer`).\n\n[source, java]\n----\nproducerConfigs.put(DelegatingByTopicSerializer.VALUE_SERIALIZATION_TOPIC_CONFIG,\n \"topic[0-4]:\" + ByteArraySerializer.class.getName()\n + \", topic[5-9]:\" + StringSerializer.class.getName());\n...\nconsumerConfigs.put(DelegatingByTopicDeserializer.VALUE_SERIALIZATION_TOPIC_CONFIG,\n \"topic[0-4]:\" + ByteArrayDeserializer.class.getName()\n + \", topic[5-9]:\" + StringDeserializer.class.getName());\n----\n\nUse `KEY_SERIALIZATION_TOPIC_CONFIG` when using this for keys.\n\n[source, java]\n----\n@Bean\npublic ProducerFactory<Integer, Object> producerFactory(Map<String, Object> config) {\n return new DefaultKafkaProducerFactory<>(config,\n new IntegerSerializer(),\n new DelegatingByTopicSerializer(Map.of(\n Pattern.compile(\"topic[0-4]\"), new ByteArraySerializer(),\n Pattern.compile(\"topic[5-9]\"), new StringSerializer())),\n new JacksonJsonSerializer<Object>()); // default\n}\n----\n\nYou can specify a default serializer/deserializer to use when there is no pattern match using `DelegatingByTopicSerialization.KEY_SERIALIZATION_TOPIC_DEFAULT` and `DelegatingByTopicSerialization.VALUE_SERIALIZATION_TOPIC_DEFAULT`.\n\nAn additional property `DelegatingByTopicSerialization.CASE_SENSITIVE` (default `true`), when set to `false` makes the topic lookup case insensitive.\n\n[[retrying-deserialization]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "By Topic", "heading_level": 3, "file_order": 43, "section_index": 11, "content_hash": "db9255129feb49b328b6e058524cb9c94b4f1ccd5081d8ebf64396d892b4fab8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:e3e482c348b5fc028eb6ee9276db2746aa820d885513cce9ed4e77fcd5923010", "content": "The `RetryingDeserializer` uses a delegate `Deserializer` and `RetryTemplate` to retry deserialization when the delegate might have transient errors, such as network issues, during deserialization.\n\n[source, java]\n----\nConsumerFactory cf = new DefaultKafkaConsumerFactory(myConsumerConfigs,\n new RetryingDeserializer(myUnreliableKeyDeserializer, retryTemplate),\n new RetryingDeserializer(myUnreliableValueDeserializer, retryTemplate));\n----\n\nA recovery callback be set on the `RetryingDeserializer`, to return a fallback object if all retries are exhausted.\n\nRefer to the https://github.com/spring-projects/spring-framework[Spring Framework] project for configuration of the `RetryTemplate` with a retry policy, back off, etc.\n\n[[messaging-message-conversion]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Retrying Deserializer", "heading_level": 2, "file_order": 43, "section_index": 12, "content_hash": "e3e482c348b5fc028eb6ee9276db2746aa820d885513cce9ed4e77fcd5923010", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:47e80f5bec0296d9a8ad91ba177da7e77b98e53825b053e9500ac660185bac56", "content": "Although the `Serializer` and `Deserializer` API is quite simple and flexible from the low-level Kafka `Consumer` and `Producer` perspective, you might need more flexibility at the Spring Messaging level, when using either `@KafkaListener` or {spring-integration-url}/kafka.html[Spring Integration's Apache Kafka Support].\nTo let you easily convert to and from `org.springframework.messaging.Message`, Spring for Apache Kafka provides a `MessageConverter` abstraction with the `MessagingMessageConverter` implementation and its `JacksonJsonMessageConverter` (and subclasses) customization.\nYou can inject the `MessageConverter` into a `KafkaTemplate` instance directly and by using `AbstractKafkaListenerContainerFactory` bean definition for the `@KafkaListener.containerFactory()` property.\nThe following example shows how to do so:\n\n[source, java]\n----\n@Bean\npublic KafkaListenerContainerFactory<?> kafkaJsonListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory<Integer, String> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n factory.setConsumerFactory(consumerFactory());\n factory.setRecordMessageConverter(new JacksonJsonMessageConverter());\n return factory;\n}\n...\n@KafkaListener(topics = \"jsonData\",\n containerFactory = \"kafkaJsonListenerContainerFactory\")\npublic void jsonListener(Cat cat) {\n...\n}\n----\n\nWhen using Spring Boot, simply define the converter as a `@Bean` and Spring Boot auto configuration will wire it into the auto-configured template and container factory.\n\nWhen you use a `@KafkaListener`, the parameter type is provided to the message converter to assist with the conversion.\n\n[NOTE]\n====\nThis type inference can be achieved only when the `@KafkaListener` annotation is declared at the method level.\nWith a class-level `@KafkaListener`, the payload type is used to select which `@KafkaHandler` method to invoke, so it must already have been converted before the method can be chosen.\n====\n\n[NOTE]\n====\nOn the consumer side, you can configure a `JacksonJsonMessageConverter`; it can handle `ConsumerRecord` values of type `byte[]`, `Bytes` and `String` so should be used in conjunction with a `ByteArrayDeserializer`, `BytesDeserializer` or `StringDeserializer`.\n(`byte[]` and `Bytes` are more efficient because they avoid an unnecessary `byte[]` to `String` conversion).\nYou can also configure the specific subclass of `JacksonJsonMessageConverter` corresponding to the deserializer, if you so wish.\n\nOn the producer side, when you use Spring Integration or the `KafkaTemplate.send(Message<?> message)` method (see xref:kafka/sending-messages.adoc#kafka-template[Using `KafkaTemplate`]), you must configure a message converter that is compatible with the configured Kafka `Serializer`.\n\n* `StringJacksonJsonMessageConverter` with `StringSerializer`\n* `BytesJacksonJsonMessageConverter` with `BytesSerializer`\n* `ByteArrayJacksonJsonMessageConverter` with `ByteArraySerializer`\n\nAgain, using `byte[]` or `Bytes` is more efficient because they avoid a `String` to `byte[]` conversion.\n\nFor convenience, starting with version 2.3, the framework also provides a `StringOrBytesSerializer` which can serialize all three value types so it can be used with any of the message converters.\n====\n\nStarting with version 2.7.1, message payload conversion can be delegated to a `spring-messaging` `SmartMessageConverter`; this enables conversion, for example, to be based on the `MessageHeaders.CONTENT_TYPE` header.\n\nIMPORTANT: The `KafkaMessageConverter.fromMessage()` method is called for outbound conversion to a `ProducerRecord` with the message payload in the `ProducerRecord.value()` property.\nThe `KafkaMessageConverter.toMessage()` method is called for inbound conversion from `ConsumerRecord` with the payload being the `ConsumerRecord.value()` property.\nThe `SmartMessageConverter.toMessage()` method is called to create a new outbound `Message<?>` from the `Message` passed to `fromMessage()` (usually by `KafkaTemplate.send(Message<?> msg)`).\nSimilarly, in the `KafkaMessageConverter.toMessage()` method, after the converter has created a new `Message<?>` from the `ConsumerRecord`, the `SmartMessageConverter.fromMessage()` method is called and then the final inbound message is created with the newly converted payload.\nIn either case, if the `SmartMessageConverter` returns `null`, the original message is used.\n\nWhen the default converter is used in the `KafkaTemplate` and listener container factory, you configure the `SmartMessageConverter` by calling `setMessagingConverter()` on the template and via the `contentTypeConverter` property on `@KafkaListener` methods.\n\nExamples:\n\n[source, java]\n----\ntemplate.setMessagingConverter(mySmartConverter);\n----\n\n[source, java]\n----\n@KafkaListener(id = \"withSmartConverter\", topics = \"someTopic\",\n contentTypeConverter = \"mySmartConverter\")\npublic void smart(Thing thing) {\n ...\n}\n----\n\n[[data-projection]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Spring Messaging Message Conversion", "heading_level": 2, "file_order": 43, "section_index": 13, "content_hash": "47e80f5bec0296d9a8ad91ba177da7e77b98e53825b053e9500ac660185bac56", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:1eea0b4051f05510edb69ab95e49661a6c90e606be7d001e85abcbf2f029f76a", "content": "Starting with version 2.1.1, you can convert JSON to a Spring Data Projection interface instead of a concrete type.\nThis allows very selective, and low-coupled bindings to data, including the lookup of values from multiple places inside the JSON document.\nFor example the following interface can be defined as message payload type:\n\n[source, java]\n----\ninterface SomeSample {\n\n @JsonPath({ \"$.username\", \"$.user.name\" })\n String getUsername();\n\n}\n----\n\n[source, java]\n----\n@KafkaListener(id=\"projection.listener\", topics = \"projection\")\npublic void projection(SomeSample in) {\n String username = in.getUsername();\n ...\n}\n----\n\nAccessor methods will be used to lookup the property name as field in the received JSON document by default.\nThe `@JsonPath` expression allows customization of the value lookup, and even to define multiple JSON Path expressions, to look up values from multiple places until an expression returns an actual value.\n\nTo enable this feature, use a `JacksonProjectingMessageConverter` configured with an appropriate delegate converter (used for outbound conversion and converting non-projection interfaces).\nYou must also add `spring-data:spring-data-commons` and `com.jayway.jsonpath:json-path` to the classpath.\n\nWhen used as the parameter to a `@KafkaListener` method, the interface type is automatically passed to the converter as normal.\n\n[[error-handling-deserializer]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Using Spring Data Projection Interfaces", "heading_level": 3, "file_order": 43, "section_index": 14, "content_hash": "1eea0b4051f05510edb69ab95e49661a6c90e606be7d001e85abcbf2f029f76a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:0c97e755ed6dea76068848824d077fa0a8ff5edfb58ed824128e0eef5a373718", "content": "When a deserializer fails to deserialize a message, Spring has no way to handle the problem, because it occurs before the `poll()` returns.\nTo solve this problem, the `ErrorHandlingDeserializer` has been introduced.\nThis deserializer delegates to a real deserializer (key or value).\nIf the delegate fails to deserialize the record content, the `ErrorHandlingDeserializer` returns a `null` value and a `DeserializationException` in a header that contains the cause and the raw bytes.\nWhen you use a record-level `MessageListener`, if the `ConsumerRecord` contains a `DeserializationException` header for either the key or value, the container's `ErrorHandler` is called with the failed `ConsumerRecord`.\nThe record is not passed to the listener.\n\nAlternatively, you can configure the `ErrorHandlingDeserializer` to create a custom value by providing a `failedDeserializationFunction`, which is a `Function<FailedDeserializationInfo, T>`.\nThis function is invoked to create an instance of `T`, which is passed to the listener in the usual fashion.\nAn object of type `FailedDeserializationInfo`, which contains all the contextual information is provided to the function.\nYou can find the `DeserializationException` (as a serialized Java object) in headers.\nSee the javadoc:org.springframework.kafka.support.serializer.ErrorHandlingDeserializer[Javadoc] for the `ErrorHandlingDeserializer` for more information.\n\nYou can use the `DefaultKafkaConsumerFactory` constructor that takes key and value `Deserializer` objects and wire in appropriate `ErrorHandlingDeserializer` instances that you have configured with the proper delegates.\nAlternatively, you can use consumer configuration properties (which are used by the `ErrorHandlingDeserializer`) to instantiate the delegates.\nThe property names are `ErrorHandlingDeserializer.KEY_DESERIALIZER_CLASS` and `ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS`.\nThe property value can be a class or class name.\nThe following example shows how to set these properties:\n\n[source, java]\n----\n... // other props\nprops.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class);\nprops.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class);\nprops.put(ErrorHandlingDeserializer.KEY_DESERIALIZER_CLASS, JsonDeserializer.class);\nprops.put(JsonDeserializer.KEY_DEFAULT_TYPE, \"com.example.MyKey\")\nprops.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS, JsonDeserializer.class.getName());\nprops.put(JsonDeserializer.VALUE_DEFAULT_TYPE, \"com.example.MyValue\")\nprops.put(JsonDeserializer.TRUSTED_PACKAGES, \"com.example\")\nreturn new DefaultKafkaConsumerFactory<>(props);\n----\n\nThe following example uses a `failedDeserializationFunction`.\n\n[source, java]\n----\npublic class BadThing extends Thing {\n\n private final FailedDeserializationInfo failedDeserializationInfo;\n\n public BadThing(FailedDeserializationInfo failedDeserializationInfo) {\n this.failedDeserializationInfo = failedDeserializationInfo;\n }\n\n public FailedDeserializationInfo getFailedDeserializationInfo() {\n return this.failedDeserializationInfo;\n }\n\n}\n\npublic class FailedThingProvider implements Function<FailedDeserializationInfo, Thing> {\n\n @Override\n public Thing apply(FailedDeserializationInfo info) {\n return new BadThing(info);\n }\n\n}\n----\n\nThe preceding example uses the following configuration:\n\n[source, java]\n----\n...\nconsumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class);\nconsumerProps.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS, JsonDeserializer.class);\nconsumerProps.put(ErrorHandlingDeserializer.VALUE_FUNCTION, FailedThingProvider.class);\n...\n----\n\nIMPORTANT: If the consumer is configured with an `ErrorHandlingDeserializer`, it is important to configure the `KafkaTemplate` and its producer with a serializer that can handle normal objects as well as raw `byte[]` values, which result from deserialization exceptions.\nThe generic value type of the template should be `Object`.\nOne technique is to use the `DelegatingByTypeSerializer`; an example follows:\n\n[source, java]\n----\n@Bean\npublic ProducerFactory<String, Object> producerFactory() {\n return new DefaultKafkaProducerFactory<>(producerConfiguration(), new StringSerializer(),\n new DelegatingByTypeSerializer(Map.of(byte[].class, new ByteArraySerializer(),\n MyNormalObject.class, new JacksonJsonSerializer<Object>())));\n}\n\n@Bean\npublic KafkaTemplate<String, Object> kafkaTemplate() {\n return new KafkaTemplate<>(producerFactory());\n}\n----\nWhen using an `ErrorHandlingDeserializer` with a batch listener, you must check for the deserialization exceptions in message headers.\nWhen used with a `DefaultBatchErrorHandler`, you can use that header to determine which record the exception failed on and communicate to the error handler via a `BatchListenerFailedException`.\n\n[source, java]\n----\n@KafkaListener(id = \"test\", topics = \"test\")\nvoid listen(List<Thing> in, @Header(KafkaHeaders.BATCH_CONVERTED_HEADERS) List<Map<String, Object>> headers) {\n for (int i = 0; i < in.size(); i++) {\n Thing thing = in.get(i);\n if (thing == null\n && headers.get(i).get(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER) != null) {\n try {\n DeserializationException deserEx = SerializationUtils.byteArrayToDeserializationException(this.logger,\n headers.get(i).get(SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER));\n if (deserEx != null) {\n logger.error(deserEx, \"Record at index \" + i + \" could not be deserialized\");\n }\n }\n catch (Exception ex) {\n logger.error(ex, \"Record at index \" + i + \" could not be deserialized\");\n }\n throw new BatchListenerFailedException(\"Deserialization\", deserEx, i);\n }\n process(thing);\n }\n}\n----\n\n`SerializationUtils.byteArrayToDeserializationException()` can be used to convert the header to a `DeserializationException`.\n\nWhen consuming `List<ConsumerRecord<?, ?>`, `SerializationUtils.getExceptionFromHeader()` is used instead:\n\n[source, java]\n----\n@KafkaListener(id = \"kgh2036\", topics = \"kgh2036\")\nvoid listen(List<ConsumerRecord<String, Thing>> in) {\n for (int i = 0; i < in.size(); i++) {\n ConsumerRecord<String, Thing> rec = in.get(i);\n if (rec.value() == null) {\n DeserializationException deserEx = SerializationUtils.getExceptionFromHeader(rec,\n SerializationUtils.VALUE_DESERIALIZER_EXCEPTION_HEADER, this.logger);\n if (deserEx != null) {\n logger.error(deserEx, \"Record at offset \" + rec.offset() + \" could not be deserialized\");\n throw new BatchListenerFailedException(\"Deserialization\", deserEx, i);\n }\n }\n process(rec.value());\n }\n}\n----\n\nIMPORTANT: If you are also using a `DeadLetterPublishingRecoverer`, the record published for a `DeserializationException` will have a `record.value()` of type `byte[]`; this should not be serialized.\nConsider using a `DelegatingByTypeSerializer` configured to use a `ByteArraySerializer` for `byte[]` and the normal serializer (Json, Avro, etc) for all other types.\n\nStarting with version 3.1, you can add a `Validator` to the `ErrorHandlingDeserializer`.\nIf the delegate `Deserializer` successfully deserializes the object, but that object fails validation, an exception is thrown similar to a deserialization exception occurring.\nThis allows the original raw data to be passed to the error handler.\nWhen creating the deserializer yourself, simply call `setValidator`; if you configure the serializer using properties, set the consumer configuration property `ErrorHandlingDeserializer.VALIDATOR_CLASS` to the class or fully qualified class name for your `Validator`.\nWhen using Spring Boot, this property name is `spring.kafka.consumer.properties.spring.deserializer.validator.class`.\n\n[[payload-conversion-with-batch]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Using `ErrorHandlingDeserializer`", "heading_level": 2, "file_order": 43, "section_index": 15, "content_hash": "0c97e755ed6dea76068848824d077fa0a8ff5edfb58ed824128e0eef5a373718", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:5fb8500c5c1b04f7433084a3efecd5f1be440f14798f3d340b7ae389dd78fafa", "content": "You can also use a `JacksonJsonMessageConverter` within a `BatchMessagingMessageConverter` to convert batch messages when you use a batch listener container factory.\nSee xref:kafka/serdes.adoc[Serialization, Deserialization, and Message Conversion] and xref:kafka/serdes.adoc#messaging-message-conversion[Spring Messaging Message Conversion] for more information.\n\nBy default, the type for the conversion is inferred from the listener argument.\nIf you configure the `JacksonJsonMessageConverter` with a `DefaultJackson2TypeMapper` that has its `TypePrecedence` set to `TYPE_ID` (instead of the default `INFERRED`), the converter uses the type information in headers (if present) instead.\nThis allows, for example, listener methods to be declared with interfaces instead of concrete classes.\nAlso, the type converter supports mapping, so the deserialization can be to a different type than the source (as long as the data is compatible).\nThis is also useful when you use xref:kafka/receiving-messages/class-level-kafkalistener.adoc[class-level `@KafkaListener` instances] where the payload must have already been converted to determine which method to invoke.\nThe following example creates beans that use this method:\n\n[source, java]\n----\n@Bean\npublic KafkaListenerContainerFactory<?> kafkaListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory<Integer, String> factory =\n new ConcurrentKafkaListenerContainerFactory<>();\n factory.setConsumerFactory(consumerFactory());\n factory.setBatchListener(true);\n factory.setBatchMessageConverter(new BatchMessagingMessageConverter(converter()));\n return factory;\n}\n\n@Bean\npublic JacksonJsonMessageConverter converter() {\n return new JacksonJsonMessageConverter();\n}\n----\n\nNote that, for this to work, the method signature for the conversion target must be a container object with a single generic parameter type, such as the following:\n\n[source, java]\n----\n@KafkaListener(topics = \"blc1\")\npublic void listen(List<Foo> foos, @Header(KafkaHeaders.OFFSET) List<Long> offsets) {\n ...\n}\n----\n\nNote that you can still access the batch headers.\n\nIf the batch converter has a record converter that supports it, you can also receive a list of messages where the payloads are converted according to the generic type.\nThe following example shows how to do so:\n\n[source, java]\n----\n@KafkaListener(topics = \"blc3\", groupId = \"blc3\")\npublic void listen(List<Message<Foo>> fooMessages) {\n ...\n}\n----\n\nIf record in the batch cannot be converted, its payload is set as `null` into the target `payloads` list.\nThe conversion exception is logged as warning for this record and also stored into a `KafkaHeaders.CONVERSION_FAILURES` header as an item of the `List<ConversionException>`.\nThe target `@KafkaListener` method may perform Java `Stream` API to filter out those `null` values from the payload list or do something with the conversion exceptions header:\n\n[source, java]\n----\n@KafkaListener(id = \"foo\", topics = \"foo\", autoStartup = \"false\")\npublic void listen(List<Foo> list,\n @Header(KafkaHeaders.CONVERSION_FAILURES) List<ConversionException> conversionFailures) {\n\n for (int i = 0; i < list.size(); i++) {\n if (conversionFailures.get(i) != null) {\n throw new BatchListenerFailedException(\"Conversion Failed\", conversionFailures.get(i), i);\n }\n }\n}\n----\n\n[[conversionservice-customization]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Payload Conversion with Batch Listeners", "heading_level": 2, "file_order": 43, "section_index": 16, "content_hash": "5fb8500c5c1b04f7433084a3efecd5f1be440f14798f3d340b7ae389dd78fafa", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:ce6dd4398965b084a59b551d26a6e53fb49ba7575f65b5ecfc8ac7d4d7ae9356", "content": "Starting with version 2.1.1, the `org.springframework.core.convert.ConversionService` used by the default `org.springframework.messaging.handler.annotation.support.MessageHandlerMethodFactory` to resolve parameters for the invocation of a listener method is supplied with all beans that implement any of the following interfaces:\n\n* `org.springframework.core.convert.converter.Converter`\n* `org.springframework.core.convert.converter.GenericConverter`\n* `org.springframework.format.Formatter`\n\nThis lets you further customize listener deserialization without changing the default configuration for `ConsumerFactory` and `KafkaListenerContainerFactory`.\n\nIMPORTANT: Setting a custom `MessageHandlerMethodFactory` on the `KafkaListenerEndpointRegistrar` through a `KafkaListenerConfigurer` bean disables this feature.\n\n[[custom-arg-resolve]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "`ConversionService` Customization", "heading_level": 2, "file_order": 43, "section_index": 17, "content_hash": "ce6dd4398965b084a59b551d26a6e53fb49ba7575f65b5ecfc8ac7d4d7ae9356", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:d7d9f16d555e4db6ff8d5e80cb0444f91f7203b7ef330340146b930a5dd71cb0", "content": "Starting with version 2.4.2 you are able to add your own `HandlerMethodArgumentResolver` and resolve custom method parameters.\nAll you need is to implement `KafkaListenerConfigurer` and use method `setCustomMethodArgumentResolvers()` from class `KafkaListenerEndpointRegistrar`.\n\n[source, java]\n----\n@Configuration\nclass CustomKafkaConfig implements KafkaListenerConfigurer {\n\n @Override\n public void configureKafkaListeners(KafkaListenerEndpointRegistrar registrar) {\n registrar.setCustomMethodArgumentResolvers(\n new HandlerMethodArgumentResolver() {\n\n @Override\n public boolean supportsParameter(MethodParameter parameter) {\n return CustomMethodArgument.class.isAssignableFrom(parameter.getParameterType());\n }\n\n @Override\n public Object resolveArgument(MethodParameter parameter, Message<?> message) {\n return new CustomMethodArgument(\n message.getHeaders().get(KafkaHeaders.RECEIVED_TOPIC, String.class)\n );\n }\n }\n );\n }\n\n}\n----\n\nYou can also completely replace the framework's argument resolution by adding a custom `MessageHandlerMethodFactory` to the `KafkaListenerEndpointRegistrar` bean.\nIf you do this, and your application needs to handle tombstone records, with a `null` `value()` (e.g. from a compacted topic), you should add a `KafkaNullAwarePayloadArgumentResolver` to the factory; it must be the last resolver because it supports all types and can match arguments without a `@Payload` annotation.\nIf you are using a `DefaultMessageHandlerMethodFactory`, set this resolver as the last custom resolver; the factory will ensure that this resolver will be used before the standard `PayloadMethodArgumentResolver`, which has no knowledge of `KafkaNull` payloads.\n\nSee also xref:kafka/tombstones.adoc[Null Payloads and Log Compaction of `Tombstone` Records].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/serdes.adoc", "title": "serdes", "heading": "Adding Custom `HandlerMethodArgumentResolver` to `@KafkaListener`", "heading_level": 2, "file_order": 43, "section_index": 18, "content_hash": "d7d9f16d555e4db6ff8d5e80cb0444f91f7203b7ef330340146b930a5dd71cb0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/serdes.adoc"}}
{"id": "sha256:85d36edf837e53e05da62e11d9be457614c51646cd51d2efc65b52cef8e96fef", "content": "[[thread-safety]]\n\nWhen using a concurrent message listener container, a single listener instance is invoked on all consumer threads.\nListeners, therefore, need to be thread-safe, and it is preferable to use stateless listeners.\nIf it is not possible to make your listener thread-safe or adding synchronization would significantly reduce the benefit of adding concurrency, you can use one of a few techniques:\n\n* Use `n` containers with `concurrency=1` with a prototype scoped `MessageListener` bean so that each container gets its own instance (this is not possible when using `@KafkaListener`).\n* Keep the state in `ThreadLocal<?>` instances.\n* Have the singleton listener delegate to a bean that is declared in `SimpleThreadScope` (or a similar scope).\n\nTo facilitate cleaning up thread state (for the second and third items in the preceding list), starting with version 2.2, the listener container publishes a `ConsumerStoppedEvent` when each thread exits.\nYou can consume these events with an `ApplicationListener` or `@EventListener` method to remove `ThreadLocal<?>` instances or `remove()` thread-scoped beans from the scope.\nNote that `SimpleThreadScope` does not destroy beans that have a destruction interface (such as `DisposableBean`), so you should `destroy()` the instance yourself.\n\nIMPORTANT: By default, the application context's event multicaster invokes event listeners on the calling thread.\nIf you change the multicaster to use an async executor, thread cleanup is not effective.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/thread-safety.adoc", "title": "thread-safety", "heading": "thread-safety", "heading_level": 1, "file_order": 44, "section_index": 0, "content_hash": "85d36edf837e53e05da62e11d9be457614c51646cd51d2efc65b52cef8e96fef", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/thread-safety.adoc"}}
{"id": "sha256:8dedd39bc6fa6d9a2b5ebdefa059576883c9d2ceb05b1968bc7e7c0d0a68a796", "content": "Because of certain limitations in the underlying library classes still using `synchronized` blocks for thread coordination, applications need to be cautious when using virtual threads with concurrent message listener containers.\nWhen virtual threads are enabled, if the concurrency exceeds the available number of platform threads, it is very likely for the virtual threads to be pinned on the platform threads and possible race conditions.\nTherefore, as the 3rd party libraries that Spring for Apache Kafka uses evolves to fully support virtual threads, it is recommended to keep the concurrency on the message listener container to be equal to or less than the number of platform threads.\nThis way, the applications avoid any race conditions between the threads and the virtual threads being pinned on platform threads.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/thread-safety.adoc", "title": "thread-safety", "heading": "Special Note on Virtual Threads and Concurrent Message Listener Containers", "heading_level": 2, "file_order": 44, "section_index": 1, "content_hash": "8dedd39bc6fa6d9a2b5ebdefa059576883c9d2ceb05b1968bc7e7c0d0a68a796", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/thread-safety.adoc"}}
{"id": "sha256:7517c952a52433601b99cec09a099e89dbdf3e77489ce2c76e8c6603cb2f8021", "content": "[[tombstones]]\n\nWhen you use {kafka-url}/documentation/#compaction[Log Compaction], you can send and receive messages with `null` payloads to identify the deletion of a key.\n\nYou can also receive `null` values for other reasons, such as a `Deserializer` that might return `null` when it cannot deserialize a value.\n\nTo send a `null` payload by using the `KafkaTemplate`, you can pass null into the value argument of the `send()` methods.\nOne exception to this is the `send(Message<?> message)` variant.\nSince `spring-messaging` `Message<?>` cannot have a `null` payload, you can use a special payload type called `KafkaNull`, and the framework sends `null`.\nFor convenience, the static `KafkaNull.INSTANCE` is provided.\n\nWhen you use a message listener container, the received `ConsumerRecord` has a `null` `value()`.\n\nTo configure the `@KafkaListener` to handle `null` payloads, you must use the `@Payload` annotation with `required = false`.\nIf it is a tombstone message for a compacted log, you usually also need the key so that your application can determine which key was +++\"+++`deleted`+++\"+++.\nThe following example shows such a configuration:\n\n[source, java]\n----\n@KafkaListener(id = \"deletableListener\", topics = \"myTopic\")\npublic void listen(@Payload(required = false) String value, @Header(KafkaHeaders.RECEIVED_KEY) String key) {\n // value == null represents key deletion\n}\n----\n\nWhen you use a class-level `@KafkaListener` with multiple `@KafkaHandler` methods, some additional configuration is needed.\nSpecifically, you need a `@KafkaHandler` method with a `KafkaNull` payload.\nThe following example shows how to configure one:\n\n[source, java]\n----\n@KafkaListener(id = \"multi\", topics = \"myTopic\")\nstatic class MultiListenerBean {\n\n @KafkaHandler\n public void listen(String cat) {\n ...\n }\n\n @KafkaHandler\n public void listen(Integer hat) {\n ...\n }\n\n @KafkaHandler\n public void delete(@Payload(required = false) KafkaNull nul, @Header(KafkaHeaders.RECEIVED_KEY) int key) {\n ...\n }\n\n}\n----\n\nNote that the argument is `null`, not `KafkaNull`.\n\nTIP: See xref:tips.adoc[Manually Assigning All Partitions].\n\nIMPORTANT: This feature requires the use of a `KafkaNullAwarePayloadArgumentResolver` which the framework will configure when using the default `MessageHandlerMethodFactory`.\nWhen using a custom `MessageHandlerMethodFactory`, see xref:kafka/serdes.adoc#custom-arg-resolve[Adding custom `HandlerMethodArgumentResolver` to `@KafkaListener`].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/tombstones.adoc", "title": "tombstones", "heading": "tombstones", "heading_level": 1, "file_order": 45, "section_index": 0, "content_hash": "7517c952a52433601b99cec09a099e89dbdf3e77489ce2c76e8c6603cb2f8021", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/tombstones.adoc"}}
{"id": "sha256:e35c0f8c3ab1d41f3662e4bf030b9f4c5dd5450e7455625b170e5907f5b9089d", "content": "[[transactions]]\n\nThis section describes how Spring for Apache Kafka supports transactions.\n\n[[overview]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "transactions", "heading_level": 1, "file_order": 46, "section_index": 0, "content_hash": "e35c0f8c3ab1d41f3662e4bf030b9f4c5dd5450e7455625b170e5907f5b9089d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:83e32e4b483e545b672c70525f138b8b9e88e384a38587cf704943678dcd8815", "content": "The 0.11.0.0 client library added support for transactions.\nSpring for Apache Kafka adds support in the following ways:\n\n* `KafkaTransactionManager`: Used with normal Spring transaction support (`@Transactional`, `TransactionTemplate`, etc)\n* Transactional `KafkaMessageListenerContainer`\n* Local transactions with `KafkaTemplate`\n* Transaction synchronization with other transaction managers\n\nTransactions are enabled by providing the `DefaultKafkaProducerFactory` with a `transactionIdPrefix`.\nIn that case, instead of managing a single shared `Producer`, the factory maintains a cache of transactional producers.\nWhen the user calls `close()` on a producer, it is returned to the cache for reuse instead of actually being closed.\nThe `transactional.id` property of each producer is `transactionIdPrefix` + `n`, where `n` starts with `0` and is incremented for each new producer.\nIn previous versions of Spring for Apache Kafka, the `transactional.id` was generated differently for transactions started by a listener container with a record-based listener, to support fencing zombies, which is not necessary any more, with `EOSMode.V2` being the only option starting with 3.0.\nFor applications running with multiple instances, the `transactionIdPrefix` must be unique per instance.\n\nAlso see xref:kafka/exactly-once.adoc[Exactly Once Semantics].\n\nAlso see xref:kafka/transactions.adoc#transaction-id-prefix[`transactionIdPrefix`].\n\nWith Spring Boot, it is only necessary to set the `spring.kafka.producer.transaction-id-prefix` property - Spring Boot will automatically configure a `KafkaTransactionManager` bean and wire it into the listener container.\n\nIMPORTANT: Starting with version 2.5.8, you can now configure the `maxAge` property on the producer factory.\nThis is useful when using transactional producers that might lay idle for the broker's `transactional.id.expiration.ms`.\nWith current `kafka-clients`, this can cause a `ProducerFencedException` without a rebalance.\nBy setting the `maxAge` to less than `transactional.id.expiration.ms`, the factory will refresh the producer if it is past its max age.\n\n[[using-kafkatransactionmanager]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "Overview", "heading_level": 2, "file_order": 46, "section_index": 1, "content_hash": "83e32e4b483e545b672c70525f138b8b9e88e384a38587cf704943678dcd8815", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:f1810a57a2aaedc4d7ad5e876f7e23712f5cb9ff69fea17f37113d01adad0daa", "content": "The `KafkaTransactionManager` is an implementation of Spring Framework's `PlatformTransactionManager`.\nIt is provided with a reference to the producer factory in its constructor.\nIf you provide a custom producer factory, it must support transactions.\nSee `ProducerFactory.transactionCapable()`.\n\nYou can use the `KafkaTransactionManager` with normal Spring transaction support (`@Transactional`, `TransactionTemplate`, and others).\nIf a transaction is active, any `KafkaTemplate` operations performed within the scope of the transaction use the transaction's `Producer`.\nThe manager commits or rolls back the transaction, depending on success or failure.\nYou must configure the `KafkaTemplate` to use the same `ProducerFactory` as the transaction manager.\n\n[[transaction-synchronization]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "Using `KafkaTransactionManager`", "heading_level": 2, "file_order": 46, "section_index": 2, "content_hash": "f1810a57a2aaedc4d7ad5e876f7e23712f5cb9ff69fea17f37113d01adad0daa", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:7c7b721a06691df33cebacaa25bcb7f87e71ddbfb1c78125902eb6220ad59906", "content": "This section refers to producer-only transactions (transactions not started by a listener container); see xref:kafka/transactions.adoc#container-transaction-manager[Using Consumer-Initiated Transactions] for information about chaining transactions when the container starts the transaction.\n\nIf you want to send records to kafka and perform some database updates, you can use normal Spring transaction management with, say, a `DataSourceTransactionManager`.\n\n[source, java]\n----\n@Transactional\npublic void process(List<Thing> things) {\n things.forEach(thing -> this.kafkaTemplate.send(\"topic\", thing));\n updateDb(things);\n}\n----\n\nThe interceptor for the `@Transactional` annotation starts the transaction and the `KafkaTemplate` will synchronize a transaction with that transaction manager; each send will participate in that transaction.\nWhen the method exits, the database transaction will commit followed by the Kafka transaction.\nIf you wish the commits to be performed in the reverse order (Kafka first), use nested `@Transactional` methods, with the outer method configured to use the `DataSourceTransactionManager`, and the inner method configured to use the `KafkaTransactionManager`.\n\nSee xref:tips.adoc#ex-jdbc-sync[Examples of Kafka Transactions with Other Transaction Managers] for examples of an application that synchronizes JDBC and Kafka transactions in Kafka-first or DB-first configurations.\n\nNOTE: Starting with versions 2.5.17, 2.6.12, 2.7.9 and 2.8.0, if the commit fails on the synchronized transaction (after the primary transaction has committed), the exception will be thrown to the caller.\nPreviously, this was silently ignored (logged at debug level).\nApplications should take remedial action, if necessary, to compensate for the committed primary transaction.\n\n[[container-transaction-manager]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "Transaction Synchronization", "heading_level": 2, "file_order": 46, "section_index": 3, "content_hash": "7c7b721a06691df33cebacaa25bcb7f87e71ddbfb1c78125902eb6220ad59906", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:d73101cb2239127fb5bd031e197b82b2a61302bb0c650e0053ddc5610f17e81f", "content": "The `ChainedKafkaTransactionManager` is now deprecated, since version 2.7; see the JavaDocs for its super class `ChainedTransactionManager` for more information.\nInstead, use a `KafkaTransactionManager` in the container to start the Kafka transaction and annotate the listener method with `@Transactional` to start the other transaction.\n\nSee xref:tips.adoc#ex-jdbc-sync[Examples of Kafka Transactions with Other Transaction Managers] for an example application that chains JDBC and Kafka transactions.\n\nIMPORTANT: xref:retrytopic.adoc[Non-Blocking Retries] cannot combine with xref:kafka/transactions.adoc#container-transaction-manager[Container Transactions].\nWhen the listener code throws an exception, container transaction commit succeeds, and the record is sent to the retryable topic.\n\n[[kafkatemplate-local-transactions]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "Using Consumer-Initiated Transactions", "heading_level": 2, "file_order": 46, "section_index": 4, "content_hash": "d73101cb2239127fb5bd031e197b82b2a61302bb0c650e0053ddc5610f17e81f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:eddb6e166e12bebd8b932ad4fd46a229b7b8e45abf248c79c55c7a46fefae274", "content": "You can use the `KafkaTemplate` to execute a series of operations within a local transaction.\nThe following example shows how to do so:\n\n[source, java]\n----\nboolean result = template.executeInTransaction(t -> {\n t.sendDefault(\"thing1\", \"thing2\");\n t.sendDefault(\"cat\", \"hat\");\n return true;\n});\n----\n\nThe argument in the callback is the template itself (`this`).\nIf the callback exits normally, the transaction is committed.\nIf an exception is thrown, the transaction is rolled back.\n\nNOTE: If there is a `KafkaTransactionManager` (or synchronized) transaction in process, it is not used.\nInstead, a new \"nested\" transaction is used.\n\n[[transaction-id-prefix]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "`KafkaTemplate` Local Transactions", "heading_level": 2, "file_order": 46, "section_index": 5, "content_hash": "eddb6e166e12bebd8b932ad4fd46a229b7b8e45abf248c79c55c7a46fefae274", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:53690219699f12899c62bcaa2ba0069ff09ea7aa09b883b63ac33b4f7760078f", "content": "With `EOSMode.V2` (aka `BETA`), the only supported mode, it is no longer necessary to use the same `transactional.id`, even for consumer-initiated transactions; in fact, it must be unique on each instance the same as for producer-initiated transactions.\nThis property must have a different value on each application instance.\n\n[[transaction-id-suffix-fixed]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "`TransactionIdPrefix`", "heading_level": 2, "file_order": 46, "section_index": 6, "content_hash": "53690219699f12899c62bcaa2ba0069ff09ea7aa09b883b63ac33b4f7760078f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:e0f92bdb82e2ae802367f269957809728591d993c88e4fbd710a8aef2f8fab2f", "content": "Since 3.2, a new `TransactionIdSuffixStrategy` interface was introduced to manage `transactional.id` suffix.\nThe default implementation is `DefaultTransactionIdSuffixStrategy` when setting `maxCache` greater than zero can reuse `transactional.id` within a specific range, otherwise suffixes will be generated on the fly by incrementing a counter.\nWhen a transaction producer is requested and `transactional.id` all in use, throw a `NoProducerAvailableException`.\nUser can then use a `RetryTemplate` configured to retry that exception, with a suitably configured back off.\n\n[source,java]\n----\npublic static class Config {\n\n @Bean\n public ProducerFactory<String, String> myProducerFactory() {\n Map<String, Object> configs = producerConfigs();\n configs.put(ProducerConfig.CLIENT_ID_CONFIG, \"myClientId\");\n ...\n DefaultKafkaProducerFactory<String, String> pf = new DefaultKafkaProducerFactory<>(configs);\n ...\n TransactionIdSuffixStrategy ss = new DefaultTransactionIdSuffixStrategy(5);\n pf.setTransactionIdSuffixStrategy(ss);\n return pf;\n }\n\n}\n----\nWhen setting `maxCache` to 5, `transactional.id` is `my.txid.`+`{0-4}`.\n\nIMPORTANT: When using `KafkaTransactionManager` with the `ConcurrentMessageListenerContainer` and enabling `maxCache`, it is necessary to set `maxCache` to a value greater than or equal to `concurrency`.\nIf a `MessageListenerContainer` is unable to acquire a `transactional.id` suffix, it will throw a `NoProducerAvailableException`.\nWhen using nested transactions in the `ConcurrentMessageListenerContainer`, it is necessary to adjust the maxCache setting to handle the increased number of nested transactions.\n\n[[tx-template-mixed]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "`TransactionIdSuffix Fixed`", "heading_level": 2, "file_order": 46, "section_index": 7, "content_hash": "e0f92bdb82e2ae802367f269957809728591d993c88e4fbd710a8aef2f8fab2f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:3b84554a8ae0dbe9a6e5c0dfc2baeeff398bfca39adb7f61b364e0912aa592a9", "content": "Normally, when a `KafkaTemplate` is transactional (configured with a transaction-capable producer factory), transactions are required.\nThe transaction can be started by a `TransactionTemplate`, a `@Transactional` method, calling `executeInTransaction`, or by a listener container, when configured with a `KafkaTransactionManager`.\nAny attempt to use the template outside the scope of a transaction results in the template throwing an `IllegalStateException`.\nStarting with version 2.4.3, you can set the template's `allowNonTransactional` property to `true`.\nIn that case, the template will allow the operation to run without a transaction, by calling the ``ProducerFactory``'s `createNonTransactionalProducer()` method; the producer will be cached, or thread-bound, as normal for reuse.\nSee xref:kafka/sending-messages.adoc#producer-factory[Using `DefaultKafkaProducerFactory`].\n\n[[transactions-batch]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "`KafkaTemplate` Transactional and non-Transactional Publishing", "heading_level": 2, "file_order": 46, "section_index": 8, "content_hash": "3b84554a8ae0dbe9a6e5c0dfc2baeeff398bfca39adb7f61b364e0912aa592a9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:92965fb45111c5c3351fcdbf22c2c47fd417add8d3d24887e4e68c99a44ee96a", "content": "When a listener fails while transactions are being used, the `AfterRollbackProcessor` is invoked to take some action after the rollback occurs.\nWhen using the default `AfterRollbackProcessor` with a record listener, seeks are performed so that the failed record will be redelivered.\nWith a batch listener, however, the whole batch will be redelivered because the framework doesn't know which record in the batch failed.\nSee xref:kafka/annotation-error-handling.adoc#after-rollback[After-rollback Processor] for more information.\n\nWhen using a batch listener, version 2.4.2 introduced an alternative mechanism to deal with failures while processing a batch: `BatchToRecordAdapter`.\nWhen a container factory with `batchListener` set to true is configured with a `BatchToRecordAdapter`, the listener is invoked with one record at a time.\nThis enables error handling within the batch, while still making it possible to stop processing the entire batch, depending on the exception type.\nA default `BatchToRecordAdapter` is provided, that can be configured with a standard `ConsumerRecordRecoverer` such as the `DeadLetterPublishingRecoverer`.\nThe following test case configuration snippet illustrates how to use this feature:\n\n[source, java]\n----\npublic static class TestListener {\n\n final List<String> values = new ArrayList<>();\n\n @KafkaListener(id = \"batchRecordAdapter\", topics = \"test\")\n public void listen(String data) {\n values.add(data);\n if (\"bar\".equals(data)) {\n throw new RuntimeException(\"reject partial\");\n }\n }\n\n}\n\n@Configuration\n@EnableKafka\npublic static class Config {\n\n ConsumerRecord<?, ?> failed;\n\n @Bean\n public TestListener test() {\n return new TestListener();\n }\n\n @Bean\n public ConsumerFactory<?, ?> consumerFactory() {\n return mock(ConsumerFactory.class);\n }\n\n @Bean\n public ConcurrentKafkaListenerContainerFactory<String, String> kafkaListenerContainerFactory() {\n ConcurrentKafkaListenerContainerFactory factory = new ConcurrentKafkaListenerContainerFactory();\n factory.setConsumerFactory(consumerFactory());\n factory.setBatchListener(true);\n factory.setBatchToRecordAdapter(new DefaultBatchToRecordAdapter<>((record, ex) -> {\n this.failed = record;\n }));\n return factory;\n }\n\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka/transactions.adoc", "title": "transactions", "heading": "Transactions with Batch Listeners", "heading_level": 2, "file_order": 46, "section_index": 9, "content_hash": "92965fb45111c5c3351fcdbf22c2c47fd417add8d3d24887e4e68c99a44ee96a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka/transactions.adoc"}}
{"id": "sha256:5060f4c89ab0d28ef902a707496cc4d4973ed7ce4e45d6defa8a1312be233843", "content": "[[access-topic-info-runtime]]\n\nSince 2.9, you can access information regarding the topic chain at runtime by injecting the provided `DestinationTopicContainer` bean.\nThis interface provides methods to look up the next topic in the chain or the DLT for a topic if configured, as well as useful properties such as the topic's name, delay and type.\n\nAs a real-world use-case example, you can use such information so a console application can resend a record from the DLT to the first retry topic in the chain after the cause of the failed processing, e.g. bug / inconsistent state, has been resolved.\n\nIMPORTANT: The `DestinationTopic` provided by the `DestinationTopicContainer#getNextDestinationTopicFor()` method corresponds to the next topic registered in the chain for the input topic.\nThe actual topic the message will be forwarded to may differ due to different factors such as exception classification, number of attempts or single-topic fixed-delay strategies.\nUse the `DestinationTopicResolver` interface if you need to weigh in these factors.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/access-topic-info-runtime.adoc", "title": "access-topic-info-runtime", "heading": "access-topic-info-runtime", "heading_level": 1, "file_order": 47, "section_index": 0, "content_hash": "5060f4c89ab0d28ef902a707496cc4d4973ed7ce4e45d6defa8a1312be233843", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/access-topic-info-runtime.adoc"}}
{"id": "sha256:3f334c2c8bf1289b9c8d2dd9a42cfde0609d181910858d7c396f55a23628c150", "content": "[[accessing-delivery-attempts]]\n\nTo access blocking and non-blocking delivery attempts, add these headers to your `@KafkaListener` method signature:\n\n[source, java]\n----\n@Header(KafkaHeaders.DELIVERY_ATTEMPT) int blockingAttempts,\n@Header(name = RetryTopicHeaders.DEFAULT_HEADER_ATTEMPTS, required = false) Integer nonBlockingAttempts\n----\n\nBlocking delivery attempts are only provided if you set ``ContainerProperties``'s xref:kafka/container-props.adoc#deliveryAttemptHeader[deliveryAttemptHeader] to `true`.\n\nNote that the non blocking attempts will be `null` for the initial delivery.\n\nStarting with version 3.0.10, a convenient `KafkaMessageHeaderAccessor` is provided to allow simpler access to these headers; the accessor can be provided as a parameter for the listener method:\n\n[souce, java]\n----\n@RetryableTopic(backOff = @BackOff(...))\n@KafkaListener(id = \"dh1\", topics = \"dh1\")\nvoid listen(Thing thing, KafkaMessageHeaderAccessor accessor) {\n ...\n}\n----\n\nUse `accessor.getBlockingRetryDeliveryAttempt()` and `accessor.getNonBlockingRetryDeliveryAttempt()` to get the values.\nThe accessor will throw an `IllegalStateException` if blocking retries are not enabled; for non-blocking retries, the accessor returns `1` for the initial delivery.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/accessing-delivery-attempts.adoc", "title": "accessing-delivery-attempts", "heading": "accessing-delivery-attempts", "heading_level": 1, "file_order": 48, "section_index": 0, "content_hash": "3f334c2c8bf1289b9c8d2dd9a42cfde0609d181910858d7c396f55a23628c150", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/accessing-delivery-attempts.adoc"}}
{"id": "sha256:a443ab294c82a929d6eb0e3b9f4666c3996d839c7e98cbce68988b54f2198776", "content": "[[back-off-delay-precision]]\n\n[[overview-and-guarantees]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/back-off-delay-precision.adoc", "title": "back-off-delay-precision", "heading": "back-off-delay-precision", "heading_level": 1, "file_order": 49, "section_index": 0, "content_hash": "a443ab294c82a929d6eb0e3b9f4666c3996d839c7e98cbce68988b54f2198776", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/back-off-delay-precision.adoc"}}
{"id": "sha256:5ba348691519e8bc543c7b9fb5db378d9440bcbb72bdee4ecdb44de8e4541905", "content": "All message processing and backing off is handled by the consumer thread, and, as such, delay precision is guaranteed on a best-effort basis.\nIf one message's processing takes longer than the next message's back off period for that consumer, the next message's delay will be higher than expected.\nAlso, for short delays (about 1s or less), the maintenance work the thread has to do, such as committing offsets, may delay the message processing execution.\nThe precision can also be affected if the retry topic's consumer is handling more than one partition, because we rely on waking up the consumer from polling and having full pollTimeouts to make timing adjustments.\n\nThat being said, for consumers handling a single partition the message's processing should occur approximately at its exact due time for most situations.\n\nIMPORTANT: It is guaranteed that a message will never be processed before its due time.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/back-off-delay-precision.adoc", "title": "back-off-delay-precision", "heading": "Overview and Guarantees", "heading_level": 2, "file_order": 49, "section_index": 1, "content_hash": "5ba348691519e8bc543c7b9fb5db378d9440bcbb72bdee4ecdb44de8e4541905", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/back-off-delay-precision.adoc"}}
{"id": "sha256:70ae8832cf1f5cae2fa717e17d10b4a4adb8a552bc94cc61819a28fdf1066959", "content": "[[change-kboe-logging-level]]\n\nWhen a message in the retry topic is not due for consumption, a `KafkaBackOffException` is thrown.\nSuch exceptions are logged by default at `DEBUG` level, but you can change this behavior by setting an error handler customizer in the `ListenerContainerFactoryConfigurer` in a `@Configuration` class.\n\nFor example, to change the logging level to WARN you might add:\n\n[source, java]\n----\n@Override\nprotected void configureCustomizers(CustomizersConfigurer customizersConfigurer) {\n customizersConfigurer.customizeErrorHandler(defaultErrorHandler ->\n defaultErrorHandler.setLogLevel(KafkaException.Level.WARN))\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/change-kboe-logging-level.adoc", "title": "change-kboe-logging-level", "heading": "change-kboe-logging-level", "heading_level": 1, "file_order": 50, "section_index": 0, "content_hash": "70ae8832cf1f5cae2fa717e17d10b4a4adb8a552bc94cc61819a28fdf1066959", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/change-kboe-logging-level.adoc"}}
{"id": "sha256:b44f68c85b8c0e3da03623bcb1667c1bf12e61a75d84bfa6de640fb9743b7534", "content": "[[dlt-strategies]]\n\nThe framework provides a few strategies for working with DLTs.\nYou can provide a method for DLT processing, use the default logging method, or have no DLT at all.\nAlso you can choose what happens if DLT processing fails.\n\n[[dlt-processing-method]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/dlt-strategies.adoc", "title": "dlt-strategies", "heading": "dlt-strategies", "heading_level": 1, "file_order": 51, "section_index": 0, "content_hash": "b44f68c85b8c0e3da03623bcb1667c1bf12e61a75d84bfa6de640fb9743b7534", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/dlt-strategies.adoc"}}
{"id": "sha256:7e69f1b5ee46e29520ddb0978d49d5d79ba8d37ee6ced19e79b7f166a362ff79", "content": "You can specify the method used to process the DLT for the topic, as well as the behavior if that processing fails.\n\nTo do that you can use the `@DltHandler` annotation in a method of the class with the `@RetryableTopic` annotation(s).\nNote that the same method will be used for all the `@RetryableTopic` annotated methods within that class.\n\n[source, java]\n----\n@RetryableTopic\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n\n@DltHandler\npublic void processDltMessage(MyPojo message) {\n // ... message processing, persistence, etc\n}\n----\n\nThe DLT handler method can also be provided through the `RetryTopicConfigurationBuilder.dltHandlerMethod(String, String)` method, passing as arguments the bean name and method name that should process the DLT's messages.\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<Integer, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .dltHandlerMethod(\"myCustomDltProcessor\", \"processDltMessage\")\n .create(template);\n}\n\n@Component\npublic class MyCustomDltProcessor {\n\n private final MyDependency myDependency;\n\n public MyCustomDltProcessor(MyDependency myDependency) {\n this.myDependency = myDependency;\n }\n\n public void processDltMessage(MyPojo message) {\n // ... message processing, persistence, etc\n }\n}\n----\n\nNOTE: If no DLT handler is provided, the default `RetryTopicConfigurer.LoggingDltListenerHandlerMethod` is used.\n\nStarting with version 2.8, if you don't want to consume from the DLT in this application at all, including by the default handler (or you wish to defer consumption), you can control whether or not the DLT container starts, independent of the container factory's `autoStartup` property.\n\nWhen using the `@RetryableTopic` annotation, set the `autoStartDltHandler` property to `false`; when using the configuration builder, use `autoStartDltHandler(false)` .\n\nYou can later start the DLT handler via the `KafkaListenerEndpointRegistry`.\n\n[[dlt-failure-behavior]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/dlt-strategies.adoc", "title": "dlt-strategies", "heading": "DLT Processing Method", "heading_level": 2, "file_order": 51, "section_index": 1, "content_hash": "7e69f1b5ee46e29520ddb0978d49d5d79ba8d37ee6ced19e79b7f166a362ff79", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/dlt-strategies.adoc"}}
{"id": "sha256:114ed7d560af90af043d54f6561f1112164f66e551ade233e536113d24ba8274", "content": "Should the DLT processing fail, there are two possible behaviors available: `ALWAYS_RETRY_ON_ERROR` and `FAIL_ON_ERROR`.\n\nIn the former the record is forwarded back to the DLT topic so it doesn't block other DLT records' processing.\nIn the latter the consumer ends the execution without forwarding the message.\n\n[source,java]\n----\n\n@RetryableTopic(dltProcessingFailureStrategy =\n DltStrategy.FAIL_ON_ERROR)\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<Integer, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .dltHandlerMethod(\"myCustomDltProcessor\", \"processDltMessage\")\n .doNotRetryOnDltFailure()\n .create(template);\n}\n----\n\nNOTE: The default behavior is to `ALWAYS_RETRY_ON_ERROR`.\n\nIMPORTANT: Starting with version 2.8.3, `ALWAYS_RETRY_ON_ERROR` will NOT route a record back to the DLT if the record causes a fatal exception to be thrown,\nsuch as a `DeserializationException`, because, generally, such exceptions will always be thrown.\n\nExceptions that are considered fatal are:\n\n* `DeserializationException`\n* `MessageConversionException`\n* `ConversionException`\n* `MethodArgumentResolutionException`\n* `NoSuchMethodException`\n* `ClassCastException`\n\nYou can add exceptions to and remove exceptions from this list using methods on the `DestinationTopicResolver` bean.\n\nSee xref:retrytopic/features.adoc#retry-topic-ex-classifier[Exception Classifier] for more information.\n\n[[configuring-no-dlt]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/dlt-strategies.adoc", "title": "dlt-strategies", "heading": "DLT Failure Behavior", "heading_level": 2, "file_order": 51, "section_index": 2, "content_hash": "114ed7d560af90af043d54f6561f1112164f66e551ade233e536113d24ba8274", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/dlt-strategies.adoc"}}
{"id": "sha256:2c7e6c7ea838fcbd09bd92bd91119032067ba471b93e57813a412ac2b0fe158b", "content": "The framework also provides the possibility of not configuring a DLT for the topic.\nIn this case after retrials are exhausted the processing simply ends.\n\n[source, java]\n----\n\n@RetryableTopic(dltProcessingFailureStrategy =\n DltStrategy.NO_DLT)\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<Integer, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .doNotConfigureDlt()\n .create(template);\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/dlt-strategies.adoc", "title": "dlt-strategies", "heading": "Configuring No DLT", "heading_level": 2, "file_order": 51, "section_index": 3, "content_hash": "2c7e6c7ea838fcbd09bd92bd91119032067ba471b93e57813a412ac2b0fe158b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/dlt-strategies.adoc"}}
{"id": "sha256:fe5ececc8361411108c68631a31a3e1d0f00af176b6893dd146561748396513c", "content": "[[features]]\n\nMost of the features are available both for the `@RetryableTopic` annotation and the `RetryTopicConfiguration` beans.\n\n[[backoff-configuration]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/features.adoc", "title": "features", "heading": "features", "heading_level": 1, "file_order": 52, "section_index": 0, "content_hash": "fe5ececc8361411108c68631a31a3e1d0f00af176b6893dd146561748396513c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/features.adoc"}}
{"id": "sha256:a5770f0b8134e7464aa7e7028e295bba814b81521774d979c017efd80c3793f6", "content": "The BackOff configuration relies on the `BackOffPolicy` interface from the `Spring Retry` project.\n\nIt includes:\n\n* Fixed Back Off\n* Exponential Back Off\n* Random Exponential Back Off\n* Uniform Random Back Off\n* No Back Off\n* Custom Back Off\n\n[source, java]\n----\n@RetryableTopic(attempts = 5,\n backOff = @BackOff(delay = 1000, multiplier = 2, maxDelay = 5000))\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .fixedBackOff(3_000)\n .maxAttempts(4)\n .create(template);\n}\n----\n\nYou can also provide a custom implementation of Spring Retry's `SleepingBackOffPolicy` interface:\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .customBackoff(new MyCustomBackOffPolicy())\n .maxAttempts(5)\n .create(template);\n}\n----\n\nNOTE: The default back off policy is `FixedBackOffPolicy` with a maximum of 3 attempts and 1000ms intervals.\n\nNOTE: There is a 30-second default maximum delay for the `ExponentialBackOffPolicy`.\nIf your back off policy requires delays with values bigger than that, adjust the `maxDelay` property accordingly.\n\nIMPORTANT: The first attempt counts against `maxAttempts`, so if you provide a `maxAttempts` value of 4 there'll be the original attempt plus 3 retries.\n\n[[global-timeout]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/features.adoc", "title": "features", "heading": "BackOff Configuration", "heading_level": 2, "file_order": 52, "section_index": 1, "content_hash": "a5770f0b8134e7464aa7e7028e295bba814b81521774d979c017efd80c3793f6", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/features.adoc"}}
{"id": "sha256:d95e5add82795bbb9daa16f9be7eaa2c9602d03665a7401c024b5cab258804c9", "content": "You can set the global timeout for the retrying process.\nIf that time is reached, the next time the consumer throws an exception the message goes straight to the DLT, or just ends the processing if no DLT is available.\n\n[source, java]\n----\n@RetryableTopic(backOff = @BackOff(2_000), timeout = 5_000)\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .fixedBackOff(2_000)\n .timeoutAfter(5_000)\n .create(template);\n}\n----\n\nNOTE: The default is having no timeout set, which can also be achieved by providing -1 as the timeout value.\n\n[[retry-topic-ex-classifier]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/features.adoc", "title": "features", "heading": "Global Timeout", "heading_level": 2, "file_order": 52, "section_index": 2, "content_hash": "d95e5add82795bbb9daa16f9be7eaa2c9602d03665a7401c024b5cab258804c9", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/features.adoc"}}
{"id": "sha256:15c0b450cbc38b5eb4dd366947e808bb8b136b5abeda7be76dcc29698bcd30da", "content": "You can specify which exceptions you want to retry on and which not to.\nYou can also set it to traverse the causes to lookup nested exceptions.\n\n[source, java]\n----\n@RetryableTopic(include = {MyRetryException.class, MyOtherRetryException.class}, traversingCauses = \"true\")\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n throw new RuntimeException(new MyRetryException()); // will retry\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyOtherPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .notRetryOn(MyDontRetryException.class)\n .create(template);\n}\n----\n\nNOTE: The default behavior is retrying on all exceptions and not traversing causes.\n\nSince 2.8.3 there's a global list of fatal exceptions which will cause the record to be sent to the DLT without any retries.\nSee xref:kafka/annotation-error-handling.adoc#default-eh[DefaultErrorHandler] for the default list of fatal exceptions.\nYou can add or remove exceptions to and from this list by overriding the `configureNonBlockingRetries` method in a `@Configuration` class that extends `RetryTopicConfigurationSupport`.\nSee xref:retrytopic/retry-config.adoc#retry-topic-global-settings[Configuring Global Settings and Features] for more information.\n\n[source, java]\n----\n\n@Override\nprotected void manageNonBlockingFatalExceptions(List<Class<? extends Throwable>> nonBlockingFatalExceptions) {\n nonBlockingFatalExceptions.add(MyNonBlockingException.class);\n}\n\n----\n\nNOTE: To disable fatal exceptions' classification, just clear the provided list.\n\n[[include-and-exclude-topics]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/features.adoc", "title": "features", "heading": "Exception Classifier", "heading_level": 2, "file_order": 52, "section_index": 3, "content_hash": "15c0b450cbc38b5eb4dd366947e808bb8b136b5abeda7be76dcc29698bcd30da", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/features.adoc"}}
{"id": "sha256:c326933ab9dd80c7039291db038e0d67dc4cf9b40802a4bdddf930e3031e26c7", "content": "You can decide which topics will and will not be handled by a `RetryTopicConfiguration` bean via the .includeTopic(String topic), .includeTopics(Collection<String> topics) .excludeTopic(String topic) and .excludeTopics(Collection<String> topics) methods.\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<Integer, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .includeTopics(List.of(\"my-included-topic\", \"my-other-included-topic\"))\n .create(template);\n}\n\n@Bean\npublic RetryTopicConfiguration myOtherRetryTopic(KafkaTemplate<Integer, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .excludeTopic(\"my-excluded-topic\")\n .create(template);\n}\n----\n\nNOTE: The default behavior is to include all topics.\n\n[[topics-autocreation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/features.adoc", "title": "features", "heading": "Include and Exclude Topics", "heading_level": 2, "file_order": 52, "section_index": 4, "content_hash": "c326933ab9dd80c7039291db038e0d67dc4cf9b40802a4bdddf930e3031e26c7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/features.adoc"}}
{"id": "sha256:dd3cde20f78cf8458052c6bb18623c37ea234c15fb4d2843700de04676a4b15c", "content": "Unless otherwise specified the framework will auto create the required topics using `NewTopic` beans that are consumed by the `KafkaAdmin` bean.\nYou can specify the number of partitions and the replication factor with which the topics will be created, and you can turn this feature off.\nStarting with version 3.0, the default replication factor is `-1`, meaning using the broker default.\nIf your broker version is earlier than 2.4, you will need to set an explicit value.\n\nIMPORTANT: Note that if you're not using Spring Boot you'll have to provide a KafkaAdmin bean in order to use this feature.\n\n[source, java]\n----\n@RetryableTopic(numPartitions = \"2\", replicationFactor = \"3\")\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n\n@RetryableTopic(autoCreateTopics = \"false\")\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<Integer, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .autoCreateTopicsWith(2, 3)\n .create(template);\n}\n\n@Bean\npublic RetryTopicConfiguration myOtherRetryTopic(KafkaTemplate<Integer, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .doNotAutoCreateRetryTopics()\n .create(template);\n}\n----\n\nNOTE: By default the topics are autocreated with one partition and a replication factor of -1 (meaning using the broker default).\nIf your broker version is earlier than 2.4, you will need to set an explicit value.\n\n[[retry-headers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/features.adoc", "title": "features", "heading": "Topics AutoCreation", "heading_level": 2, "file_order": 52, "section_index": 5, "content_hash": "dd3cde20f78cf8458052c6bb18623c37ea234c15fb4d2843700de04676a4b15c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/features.adoc"}}
{"id": "sha256:1ab0488d7c26f6f27d7066fc77f5701d542b8e24d6f96829c644b715873510ad", "content": "When considering how to manage failure headers (original headers and exception headers), the framework delegates to the `DeadLetterPublishingRecoverer` to decide whether to append or replace the headers.\n\nBy default, it explicitly sets `appendOriginalHeaders` to `false` and leaves `stripPreviousExceptionHeaders` to the default used by the `DeadLetterPublishingRecover`.\n\nThis means that only the first \"original\" and last exception headers are retained with the default configuration.\nThis is to avoid creation of excessively large messages (due to the stack trace header, for example) when many retry steps are involved.\n\nSee xref:kafka/annotation-error-handling.adoc#dlpr-headers[Managing Dead Letter Record Headers] for more information.\n\nTo reconfigure the framework to use different settings for these properties, configure a `DeadLetterPublishingRecoverer` customizer by overriding the `configureCustomizers` method in a `@Configuration` class that extends `RetryTopicConfigurationSupport`.\nSee xref:retrytopic/retry-config.adoc#retry-topic-global-settings[Configuring Global Settings and Features] for more details.\n\n[source, java]\n----\n@Override\nprotected void configureCustomizers(CustomizersConfigurer customizersConfigurer) {\n customizersConfigurer.customizeDeadLetterPublishingRecoverer(dlpr -> {\n dlpr.setAppendOriginalHeaders(true);\n dlpr.setStripPreviousExceptionHeaders(false);\n });\n}\n----\n\nStarting with version 2.8.4, if you wish to add custom headers (in addition to the retry information headers added by the factory, you can add a `headersFunction` to the factory - `factory.setHeadersFunction((rec, ex) +++->+++ { +++...+++ })`.\n\nBy default, any headers added will be cumulative - Kafka headers can contain multiple values.\nStarting with version 2.9.5, if the `Headers` returned by the function contains a header of type `DeadLetterPublishingRecoverer.SingleRecordHeader`, then any existing values for that header will be removed and only the new single value will remain.\n\n[[custom-dlpr]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/features.adoc", "title": "features", "heading": "Failure Header Management", "heading_level": 2, "file_order": 52, "section_index": 6, "content_hash": "1ab0488d7c26f6f27d7066fc77f5701d542b8e24d6f96829c644b715873510ad", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/features.adoc"}}
{"id": "sha256:d603b7d5bb71fe58a0449e9f30fd217e59eabb0f6c0773ba1f71655c88152e0c", "content": "As can be seen in xref:retrytopic/features.adoc#retry-headers[Failure Header Management] it is possible to customize the default `DeadLetterPublishingRecoverer` instances created by the framework.\nHowever, for some use cases, it is necessary to subclass the `DeadLetterPublishingRecoverer`, for example to override `createProducerRecord()` to modify the contents sent to the retry (or dead-letter) topics.\nStarting with version 3.0.9, you can override the `RetryTopicConfigurationSupport.configureDeadLetterPublishingContainerFactory()` method to provide a `DeadLetterPublisherCreator` instance, for example:\n\n[source, java]\n----\n@Override\nprotected Consumer<DeadLetterPublishingRecovererFactory>\n configureDeadLetterPublishingContainerFactory() {\n\n return (factory) -> factory.setDeadLetterPublisherCreator(\n (templateResolver, destinationResolver) ->\n new CustomDLPR(templateResolver, destinationResolver));\n}\n----\n\nIt is recommended that you use the provided resolvers when constructing the custom instance.\n\n[[exc-based-custom-dlt-routing]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/features.adoc", "title": "features", "heading": "Custom DeadLetterPublishingRecoverer", "heading_level": 2, "file_order": 52, "section_index": 7, "content_hash": "d603b7d5bb71fe58a0449e9f30fd217e59eabb0f6c0773ba1f71655c88152e0c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/features.adoc"}}
{"id": "sha256:bc830a761725c2c5b75925bc8ffcb86eb174e54f7e27920283a9ef0d9cc559ef", "content": "Starting with version 3.2.0, it's possible to route messages to custom DLTs based on the type of the exception, which has been thrown during their processing.\nIn order to do that, there's a need to specify the routing.\nRouting customization consists of the specification of the additional destinations.\nDestinations in turn consist of two settings: the `suffix` and `exceptions`.\nWhen the exception type specified in `exceptions` has been thrown, the DLT containing the `suffix` will be considered as the target topic for the message before the general purpose DLT is considered.\nExamples of configuration using either annotations or `RetryTopicConfiguration` beans:\n\n[source, java]\n----\n@RetryableTopic(exceptionBasedDltRouting = {\n @ExceptionBasedDltDestination(\n suffix = \"-deserialization\", exceptions = {DeserializationException.class}\n )}\n)\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .dltRoutingRules(Map.of(\"-deserialization\", Set.of(DeserializationException.class)))\n .create(template);\n}\n----\n\n`suffix` takes place before the general `dltTopicSuffix` in the custom DLT name.\nConsidering presented examples, the message, which caused the `DeserializationException` will be routed to the `my-annotated-topic-deserialization-dlt` instead of the `my-annotated-topic-dlt`.\nCustom DLTs will be created following the same rules as stated in the xref:retrytopic/features.adoc#topics-autocreation[Topics AutoCreation].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/features.adoc", "title": "features", "heading": "Routing of messages to custom DLTs based on thrown exceptions", "heading_level": 2, "file_order": 52, "section_index": 8, "content_hash": "bc830a761725c2c5b75925bc8ffcb86eb174e54f7e27920283a9ef0d9cc559ef", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/features.adoc"}}
{"id": "sha256:5c5c1179a0ca5b6eef1e978dd186eca5a85916076bf79d685da0e47aa4812840", "content": "[[how-the-pattern-works]]\n\nIf message processing fails, the message is forwarded to a retry topic with a back off timestamp.\nThe retry topic consumer then checks the timestamp and if it's not due it pauses the consumption for that topic's partition.\nWhen it is due the partition consumption is resumed, and the message is consumed again.\nIf the message processing fails again the message will be forwarded to the next retry topic, and the pattern is repeated until a successful processing occurs, or the attempts are exhausted, and the message is sent to the Dead Letter Topic (if configured).\n\nTo illustrate, if you have a \"main-topic\" topic, and want to set up non-blocking retry with an exponential backoff of 1000ms with a multiplier of 2 and 4 max attempts, it will create the main-topic-retry-1000, main-topic-retry-2000, main-topic-retry-4000 and main-topic-dlt topics and configure the respective consumers.\nThe framework also takes care of creating the topics and setting up and configuring the listeners.\n\nIMPORTANT: By using this strategy you lose Kafka's ordering guarantees for that topic.\n\nIMPORTANT: You can set the `AckMode` mode you prefer, but `RECORD` is suggested.\n\nWhen using a manual `AckMode` with `asyncAcks` set to true, the `DefaultErrorHandler` must be configured with `seekAfterError` set to `false`.\nStarting with versions 2.9.10, 3.0.8, this will be set to `false` unconditionally for such configurations.\nWith earlier versions, it was necessary to override the `RetryTopicConfigurationSupport.configureCustomizers()` method to set the property to `false`.\n\n[source, java]\n----\n@Override\nprotected void configureCustomizers(CustomizersConfigurer customizersConfigurer) {\n customizersConfigurer.customizeErrorHandler(eh -> eh.setSeekAfterError(false));\n}\n----\n\nIn addition, before those versions, using the default (logging) DLT handler was not compatible with any kind of manual `AckMode`, regardless of the `asyncAcks` property.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/how-the-pattern-works.adoc", "title": "how-the-pattern-works", "heading": "how-the-pattern-works", "heading_level": 1, "file_order": 53, "section_index": 0, "content_hash": "5c5c1179a0ca5b6eef1e978dd186eca5a85916076bf79d685da0e47aa4812840", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/how-the-pattern-works.adoc"}}
{"id": "sha256:094372c226ab8ea29673a8baee0d2bf480ca10940a8f35453bb971af0de3143b", "content": "[[multi-retry]]\n\nStarting with version 3.0, it is now possible to configure multiple listeners on the same topic(s).\nIn order to do this, you must use custom topic naming to isolate the retry topics from each other.\nThis is best shown with an example:\n\n[source, java]\n----\n@RetryableTopic(...\n retryTopicSuffix = \"-listener1\", dltTopicSuffix = \"-listener1-dlt\",\n topicSuffixingStrategy = TopicSuffixingStrategy.SUFFIX_WITH_INDEX_VALUE)\n@KafkaListener(id = \"listener1\", groupId = \"group1\", topics = TWO_LISTENERS_TOPIC, ...)\nvoid listen1(String message, @Header(KafkaHeaders.RECEIVED_TOPIC) String receivedTopic) {\n ...\n}\n\n@RetryableTopic(...\n retryTopicSuffix = \"-listener2\", dltTopicSuffix = \"-listener2-dlt\",\n topicSuffixingStrategy = TopicSuffixingStrategy.SUFFIX_WITH_INDEX_VALUE)\n@KafkaListener(id = \"listener2\", groupId = \"group2\", topics = TWO_LISTENERS_TOPIC, ...)\nvoid listen2(String message, @Header(KafkaHeaders.RECEIVED_TOPIC) String receivedTopic) {\n ...\n}\n----\n\nThe `topicSuffixingStrategy` is optional.\nThe framework will configure and use a separate set of retry topics for each listener.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/multi-retry.adoc", "title": "multi-retry", "heading": "multi-retry", "heading_level": 1, "file_order": 54, "section_index": 0, "content_hash": "094372c226ab8ea29673a8baee0d2bf480ca10940a8f35453bb971af0de3143b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/multi-retry.adoc"}}
{"id": "sha256:ae2abe238d1fe0535e1d7d53971ca7b5503a35c8e610a4b0f15adc911171cb3b", "content": "[[programmatic-construction]]\n\nThe feature is designed to be used with `@KafkaListener`; however, several users have requested information on how to configure non-blocking retries programmatically.\nThe following Spring Boot application provides an example of how to do so.\n\n[source, java]\n----\n@SpringBootApplication\npublic class Application extends RetryTopicConfigurationSupport {\n\n public static void main(String[] args) {\n SpringApplication.run(Application.class, args);\n }\n\n @Bean\n RetryTopicConfiguration retryConfig(KafkaTemplate<String, String> template) {\n return RetryTopicConfigurationBuilder.newInstance()\n .maxAttempts(4)\n .autoCreateTopicsWith(2, (short) 1)\n .create(template);\n }\n\n @Bean\n TaskScheduler scheduler() {\n return new ThreadPoolTaskScheduler();\n }\n\n @Bean\n @Order(0)\n SmartInitializingSingleton dynamicRetry(RetryTopicConfigurer configurer, RetryTopicConfiguration config,\n KafkaListenerAnnotationBeanPostProcessor<?, ?> bpp, KafkaListenerContainerFactory<?> factory,\n Listener listener, KafkaListenerEndpointRegistry registry) {\n\n return () -> {\n KafkaListenerEndpointRegistrar registrar = bpp.getEndpointRegistrar();\n MethodKafkaListenerEndpoint<String, String> mainEndpoint = new MethodKafkaListenerEndpoint<>();\n EndpointProcessor endpointProcessor = endpoint -> {\n // customize as needed (e.g. apply attributes to retry endpoints).\n if (!endpoint.equals(mainEndpoint)) {\n endpoint.setConcurrency(1);\n }\n // these are required\n endpoint.setMessageHandlerMethodFactory(bpp.getMessageHandlerMethodFactory());\n endpoint.setTopics(\"topic\");\n endpoint.setId(\"id\");\n endpoint.setGroupId(\"group\");\n };\n mainEndpoint.setBean(listener);\n try {\n mainEndpoint.setMethod(Listener.class.getDeclaredMethod(\"onMessage\", ConsumerRecord.class));\n }\n catch (NoSuchMethodException | SecurityException ex) {\n throw new IllegalStateException(ex);\n }\n mainEndpoint.setConcurrency(2);\n mainEndpoint.setTopics(\"topic\");\n mainEndpoint.setId(\"id\");\n mainEndpoint.setGroupId(\"group\");\n configurer.processMainAndRetryListeners(endpointProcessor, mainEndpoint, config, registrar, factory,\n \"kafkaListenerContainerFactory\");\n };\n }\n\n @Bean\n ApplicationRunner runner(KafkaTemplate<String, String> template) {\n return args -> {\n template.send(\"topic\", \"test\");\n };\n }\n\n}\n\n@Component\nclass Listener implements MessageListener<String, String> {\n\n @Override\n public void onMessage(ConsumerRecord<String, String> record) {\n System.out.println(KafkaUtils.format(record));\n throw new RuntimeException(\"test\");\n }\n\n}\n----\n\nIMPORTANT: Auto creation of topics will only occur if the configuration is processed before the application context is refreshed, as in the above example.\nTo configure containers at runtime, the topics will need to be created using some other technique.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/programmatic-construction.adoc", "title": "programmatic-construction", "heading": "programmatic-construction", "heading_level": 1, "file_order": 55, "section_index": 0, "content_hash": "ae2abe238d1fe0535e1d7d53971ca7b5503a35c8e610a4b0f15adc911171cb3b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/programmatic-construction.adoc"}}
{"id": "sha256:9d43b31334cb50b905b28d7d24e2c94167d140593a385aa89763e96ba737cd4b", "content": "[[retry-config]]\n\nFor the default setup, enable non-blocking retries by adding the `@RetryableTopic` annotation to a `@KafkaListener` method.\nThis is the recommended and simplest approach, as it automatically configures the required retry infrastructure and creates the retry and DLT topics with default settings.\n\nTo import the non-blocking retry infrastructure and expose its components as beans, annotate a `@Configuration` class with `@EnableKafkaRetryTopic`.\nThis enables injection and runtime lookup of the featureâ€™s components and serves as a foundation for advanced and global configuration.\n\nNOTE: It is not necessary to also add `@EnableKafka`, if you add this annotation, because `@EnableKafkaRetryTopic` is meta-annotated with `@EnableKafka`.\n\nFor advanced and global customization, extend `RetryTopicConfigurationSupport` in a single `@Configuration` class and override the relevant methods.\nFor more details refer to xref:retrytopic/retry-config.adoc#retry-topic-global-settings[Configuring Global Settings and Features].\n\nBy default, the containers for the retry topics will have the same concurrency as the main container.\nStarting with version 3.0, you can set a different `concurrency` for the retry containers (either on the annotation, or in `RetryTopicConfigurationBuilder`).\n\n[IMPORTANT]\n====\nUse only one of the two global configuration approaches above (`@EnableKafkaRetryTopic` or extending `RetryTopicConfigurationSupport`).\nIn addition, only one `@Configuration` class should extend `RetryTopicConfigurationSupport`.\n====\n\n[[using-the-retryabletopic-annotation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/retry-config.adoc", "title": "retry-config", "heading": "retry-config", "heading_level": 1, "file_order": 56, "section_index": 0, "content_hash": "9d43b31334cb50b905b28d7d24e2c94167d140593a385aa89763e96ba737cd4b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/retry-config.adoc"}}
{"id": "sha256:9d163b01f29730311e41e385ff6359652181231f7b21762a80d1afd20668b619", "content": "To configure the retry topic and dlt for a `@KafkaListener` annotated method, you just have to add the `@RetryableTopic` annotation to it and Spring for Apache Kafka will bootstrap all the necessary topics and consumers with the default configurations.\n\n[source, java]\n----\n@RetryableTopic(kafkaTemplate = \"myRetryableTopicKafkaTemplate\")\n@KafkaListener(topics = \"my-annotated-topic\", groupId = \"myGroupId\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\nSince 3.2, `@RetryableTopic` support for @KafkaListener on a class would be:\n[source,java]\n----\n@RetryableTopic(listenerContainerFactory = \"my-retry-topic-factory\")\n@KafkaListener(topics = \"my-annotated-topic\")\npublic class ClassLevelRetryListener {\n\n @KafkaHandler\n public void processMessage(MyPojo message) {\n // ... message processing\n }\n\n}\n----\n\nYou can specify a method in the same class to process the dlt messages by annotating it with the `@DltHandler` annotation.\nIf no DltHandler method is provided a default consumer is created which only logs the consumption.\n\n[source, java]\n----\n@DltHandler\npublic void processMessage(MyPojo message) {\n // ... message processing, persistence, etc\n}\n----\n\nNOTE: If you don't specify a kafkaTemplate name a bean with name `defaultRetryTopicKafkaTemplate` will be looked up.\nIf no bean is found an exception is thrown.\n\nStarting with version 3.0, the `@RetryableTopic` annotation can be used as a meta-annotation on custom annotations; for example:\n\n[source, java]\n----\n@Target({ElementType.METHOD})\n@Retention(RetentionPolicy.RUNTIME)\n@RetryableTopic\nstatic @interface MetaAnnotatedRetryableTopic {\n\n @AliasFor(attribute = \"concurrency\", annotation = RetryableTopic.class)\n String parallelism() default \"3\";\n\n}\n----\n\n[[using-retrytopicconfiguration-beans]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/retry-config.adoc", "title": "retry-config", "heading": "Using the `@RetryableTopic` annotation", "heading_level": 2, "file_order": 56, "section_index": 1, "content_hash": "9d163b01f29730311e41e385ff6359652181231f7b21762a80d1afd20668b619", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/retry-config.adoc"}}
{"id": "sha256:c4aab689a41a3f4569b364bf3ad83cf4ddd240e1c4b18248f8c64370f4981d06", "content": "You can also configure the non-blocking retry support by creating `RetryTopicConfiguration` beans in a `@Configuration` annotated class.\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, Object> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .create(template);\n}\n----\n\nThis will create retry topics and a dlt, as well as the corresponding consumers, for all topics in methods annotated with `+++@+++KafkaListener` using the default configurations. The `KafkaTemplate` instance is required for message forwarding.\n\nTo achieve more fine-grained control over how to handle non-blocking retrials for each topic, more than one `RetryTopicConfiguration` bean can be provided.\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .fixedBackOff(3000)\n .maxAttempts(5)\n .concurrency(1)\n .includeTopics(List.of(\"my-topic\", \"my-other-topic\"))\n .create(template);\n}\n\n@Bean\npublic RetryTopicConfiguration myOtherRetryTopic(KafkaTemplate<String, MyOtherPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .exponentialBackoff(1000, 2, 5000)\n .maxAttempts(4)\n .excludeTopics(List.of(\"my-topic\", \"my-other-topic\"))\n .retryOn(MyException.class)\n .create(template);\n}\n----\n\nNOTE: The retry topics' and dlt's consumers will be assigned to a consumer group with a group id that is the combination of the one which you provide in the `groupId` parameter of the `@KafkaListener` annotation with the topic's suffix.\nIf you don't provide any they'll all belong to the same group, and rebalance on a retry topic will cause an unnecessary rebalance on the main topic.\n\nIMPORTANT: If the consumer is configured with an xref:kafka/serdes.adoc#error-handling-deserializer[`ErrorHandlingDeserializer`], to handle deserialization exceptions, it is important to configure the `KafkaTemplate` and its producer with a serializer that can handle normal objects as well as raw `byte[]` values, which result from deserialization exceptions.\nThe generic value type of the template should be `Object`.\nOne technique is to use the `DelegatingByTypeSerializer`; an example follows:\n\n[source, java]\n----\n@Bean\npublic ProducerFactory<String, Object> producerFactory() {\n return new DefaultKafkaProducerFactory<>(producerConfiguration(), new StringSerializer(),\n new DelegatingByTypeSerializer(Map.of(byte[].class, new ByteArraySerializer(),\n MyNormalObject.class, new JacksonJsonSerializer<Object>())));\n}\n\n@Bean\npublic KafkaTemplate<String, Object> kafkaTemplate() {\n return new KafkaTemplate<>(producerFactory());\n}\n----\n\nIMPORTANT: Multiple `@KafkaListener` annotations can be used for the same topic with or without manual partition assignment along with non-blocking retries, but only one configuration will be used for a given topic.\nIt's best to use a single `RetryTopicConfiguration` bean for configuration of such topics; if multiple `@RetryableTopic` annotations are being used for the same topic, all of them should have the same values, otherwise one of them will be applied to all of that topic's listeners and the other annotations' values will be ignored.\n\n[[retry-topic-global-settings]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/retry-config.adoc", "title": "retry-config", "heading": "Using `RetryTopicConfiguration` beans", "heading_level": 2, "file_order": 56, "section_index": 2, "content_hash": "c4aab689a41a3f4569b364bf3ad83cf4ddd240e1c4b18248f8c64370f4981d06", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/retry-config.adoc"}}
{"id": "sha256:43d5bc9160279a953dacc939078ad1edeb59919c2eeea3a0c9843fd2104baad3", "content": "Since 2.9, the previous bean overriding approach for configuring components has been removed (without deprecation, due to the aforementioned experimental nature of the API).\nThis does not change the `RetryTopicConfiguration` beans approach - only infrastructure components' configurations.\nNow the `RetryTopicConfigurationSupport` class should be extended in a (single) `@Configuration` class, and the proper methods overridden.\nAn example follows:\n\n[source, java]\n----\n\n@EnableKafka\n@Configuration\npublic class MyRetryTopicConfiguration extends RetryTopicConfigurationSupport {\n\n @Override\n protected void configureBlockingRetries(BlockingRetriesConfigurer blockingRetries) {\n blockingRetries\n .retryOn(MyBlockingRetriesException.class, MyOtherBlockingRetriesException.class)\n .backOff(new FixedBackOff(3000, 3));\n }\n\n @Override\n protected void manageNonBlockingFatalExceptions(List<Class<? extends Throwable>> nonBlockingFatalExceptions) {\n nonBlockingFatalExceptions.add(MyNonBlockingException.class);\n }\n\n @Override\n protected void configureCustomizers(CustomizersConfigurer customizersConfigurer) {\n // Use the new 2.9 mechanism to avoid re-fetching the same records after a pause\n customizersConfigurer.customizeErrorHandler(eh -> {\n eh.setSeekAfterError(false);\n });\n }\n\n}\n----\n\nIMPORTANT: When using this configuration approach, the `@EnableKafkaRetryTopic` annotation should not be used to prevent context failing to start due to duplicated beans.\nUse the simple `@EnableKafka` annotation instead.\n\nWhen `autoCreateTopics` is true, the main and retry topics will be created with the specified number of partitions and replication factor.\nStarting with version 3.0, the default replication factor is `-1`, meaning using the broker default.\nIf your broker version is earlier than 2.4, you will need to set an explicit value.\nTo override these values for a particular topic (e.g. the main topic or DLT), simply add a `NewTopic` `@Bean` with the required properties; that will override the auto creation properties.\n\nIMPORTANT: By default, records are published to the retry topic(s) using the original partition of the received record.\nIf the retry topics have fewer partitions than the main topic, you should configure the framework appropriately; an example follows.\n\n[source, java]\n----\n@EnableKafka\n@Configuration\npublic class Config extends RetryTopicConfigurationSupport {\n\n @Override\n protected Consumer<DeadLetterPublishingRecovererFactory> configureDeadLetterPublishingContainerFactory() {\n return dlprf -> dlprf.setPartitionResolver((cr, nextTopic) -> null);\n }\n\n ...\n\n}\n----\n\nThe parameters to the function are the consumer record and the name of the next topic.\nYou can return a specific partition number, or `null` to indicate that the `KafkaProducer` should determine the partition.\n\nBy default, all values of retry headers (number of attempts, timestamps) are retained when a record transitions through the retry topics.\nStarting with version 2.9.6, if you want to retain just the last value of these headers, use the `configureDeadLetterPublishingContainerFactory()` method shown above to set the factory's `retainAllRetryHeaderValues` property to `false`.\n\n[[find-retry-topic-config]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/retry-config.adoc", "title": "retry-config", "heading": "Configuring Global Settings and Features", "heading_level": 2, "file_order": 56, "section_index": 3, "content_hash": "43d5bc9160279a953dacc939078ad1edeb59919c2eeea3a0c9843fd2104baad3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/retry-config.adoc"}}
{"id": "sha256:8637076cf20a0a9b795d974be908baa20a38484b85f58fd1cefc0eb4d5ad22f3", "content": "Attempts to provide an instance of `RetryTopicConfiguration` by either creating one from a `@RetryableTopic` annotation, or from the bean container if no annotation is available.\n\nIf beans are found in the container, there's a check to determine whether the provided topics should be handled by any of such instances.\n\nIf `@RetryableTopic` annotation is provided, a `DltHandler` annotated method is looked up.\n\nsince 3.2, provide new API to Create `RetryTopicConfiguration` when `@RetryableTopic` annotated on a class:\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic() {\n RetryTopicConfigurationProvider provider = new RetryTopicConfigurationProvider(beanFactory);\n return provider.findRetryConfigurationFor(topics, null, AnnotatedClass.class, bean);\n}\n\n@RetryableTopic\npublic static class AnnotatedClass {\n // NoOps\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/retry-config.adoc", "title": "retry-config", "heading": "Find RetryTopicConfiguration", "heading_level": 2, "file_order": 56, "section_index": 4, "content_hash": "8637076cf20a0a9b795d974be908baa20a38484b85f58fd1cefc0eb4d5ad22f3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/retry-config.adoc"}}
{"id": "sha256:7ff9b8454b342baa5c70fdb9ec3e641a8f08974587a4bac5c2b7c212347733ff", "content": "[[retry-topic-combine-blocking]]\n\nStarting in 2.8.4 you can configure the framework to use both blocking and non-blocking retries in conjunction.\nFor example, you can have a set of exceptions that would likely trigger errors on the next records as well, such as `DatabaseAccessException`, so you can retry the same record a few times before sending it to the retry topic, or straight to the DLT.\n\nTo configure blocking retries, override the `configureBlockingRetries` method in a `@Configuration` class that extends `RetryTopicConfigurationSupport` and add the exceptions you want to retry, along with the `BackOff` to be used.\nThe default `BackOff` is a `FixedBackOff` with no delay and 9 attempts.\nSee xref:retrytopic/retry-config.adoc#retry-topic-global-settings[Configuring Global Settings and Features] for more information.\n\n[source, java]\n----\n\n@Override\nprotected void configureBlockingRetries(BlockingRetriesConfigurer blockingRetries) {\n blockingRetries\n .retryOn(MyBlockingRetryException.class, MyOtherBlockingRetryException.class)\n .backOff(new FixedBackOff(3_000, 5));\n}\n\n----\n\nNOTE: In combination with the global retryable topic's fatal exceptions classification, you can configure the framework for any behavior you'd like, such as having some exceptions trigger both blocking and non-blocking retries, trigger only one kind or the other, or go straight to the DLT without retries of any kind.\n\nHere's an example with both configurations working together:\n\n[source, java]\n----\n@Override\nprotected void configureBlockingRetries(BlockingRetriesConfigurer blockingRetries) {\n blockingRetries\n .retryOn(ShouldRetryOnlyBlockingException.class, ShouldRetryViaBothException.class)\n .backOff(new FixedBackOff(50, 3));\n}\n\n@Override\nprotected void manageNonBlockingFatalExceptions(List<Class<? extends Throwable>> nonBlockingFatalExceptions) {\n nonBlockingFatalExceptions.add(ShouldSkipBothRetriesException.class);\n}\n\n----\n\nIn this example:\n\n* `ShouldRetryOnlyBlockingException.class` would retry only via blocking and, if all retries fail, would go straight to the DLT.\n* `ShouldRetryViaBothException.class` would retry via blocking, and if all blocking retries fail would be forwarded to the next retry topic for another set of attempts.\n* `ShouldSkipBothRetriesException.class` would never be retried in any way and would go straight to the DLT if the first processing attempt failed.\n\nIMPORTANT: Note that the blocking retries behavior is allowlist - you add the exceptions you do want to retry that way; while the non-blocking retries classification is geared towards FATAL exceptions and as such is denylist - you add the exceptions you don't want to do non-blocking retries, but to send directly to the DLT instead.\n\nIMPORTANT: The non-blocking exception classification behavior also depends on the specific topic's configuration.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/retry-topic-combine-blocking.adoc", "title": "retry-topic-combine-blocking", "heading": "retry-topic-combine-blocking", "heading_level": 1, "file_order": 57, "section_index": 0, "content_hash": "7ff9b8454b342baa5c70fdb9ec3e641a8f08974587a4bac5c2b7c212347733ff", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/retry-topic-combine-blocking.adoc"}}
{"id": "sha256:9f5fc97cabb4d012bdf2fc6b3a6d2ea8fcf59a791037104a3b10a982ec2eed36", "content": "[[retry-topic-lcf]]\n\nBy default the RetryTopic configuration will use the provided factory from the `@KafkaListener` annotation, but you can specify a different one to be used to create the retry topic and dlt listener containers.\n\nFor the `@RetryableTopic` annotation you can provide the factory's bean name, and using the `RetryTopicConfiguration` bean you can either provide the bean name or the instance itself.\n\n[source, java]\n----\n@RetryableTopic(listenerContainerFactory = \"my-retry-topic-factory\")\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<Integer, MyPojo> template,\n ConcurrentKafkaListenerContainerFactory<Integer, MyPojo> factory) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .listenerFactory(factory)\n .create(template);\n}\n\n@Bean\npublic RetryTopicConfiguration myOtherRetryTopic(KafkaTemplate<Integer, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .listenerFactory(\"my-retry-topic-factory\")\n .create(template);\n}\n----\n\nIMPORTANT: Since 2.8.3 you can use the same factory for retryable and non-retryable topics.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/retry-topic-lcf.adoc", "title": "retry-topic-lcf", "heading": "retry-topic-lcf", "heading_level": 1, "file_order": 58, "section_index": 0, "content_hash": "9f5fc97cabb4d012bdf2fc6b3a6d2ea8fcf59a791037104a3b10a982ec2eed36", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/retry-topic-lcf.adoc"}}
{"id": "sha256:88c9d1be4d3db5abb889e56bdd560feda078a214b79d110d3028b03b35d1372c", "content": "[[topic-naming]]\n\nRetry topics and DLT are named by suffixing the main topic with a provided or default value, appended by either the delay or index for that topic.\n\nExamples:\n\n\"my-topic\" -> \"my-topic-retry-0\", \"my-topic-retry-1\", ..., \"my-topic-dlt\"\n\n\"my-other-topic\" -> \"my-topic-myRetrySuffix-1000\", \"my-topic-myRetrySuffix-2000\", ..., \"my-topic-myDltSuffix\"\n\nNOTE: The default behavior is to create separate retry topics for each attempt, appended with an index value: retry-0, retry-1, ..., retry-n.\nTherefore, by default the number of retry topics is the configured `maxAttempts` minus 1.\n\nYou can xref:retrytopic/topic-naming.adoc#retry-topics-and-dlt-suffixes[configure the suffixes], choose whether to append xref:retrytopic/topic-naming.adoc#append-index-or-delay[the attempt index or delay], use a xref:retrytopic/topic-naming.adoc#single-topic-fixed-delay[single retry topic when using fixed backoff], and use a xref:retrytopic/topic-naming.adoc#single-topic-maxinterval-delay[single retry topic for the attempts with the maxInterval] when using exponential backoffs.\n\n[[retry-topics-and-dlt-suffixes]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/topic-naming.adoc", "title": "topic-naming", "heading": "topic-naming", "heading_level": 1, "file_order": 59, "section_index": 0, "content_hash": "88c9d1be4d3db5abb889e56bdd560feda078a214b79d110d3028b03b35d1372c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/topic-naming.adoc"}}
{"id": "sha256:95b14363e7a507ac1c6f2588cf93e0cd1960f2ccdd6e8d4bad1153f1682dec8a", "content": "You can specify the suffixes that will be used by the retry and DLT topics.\n\n[source, java]\n----\n@RetryableTopic(retryTopicSuffix = \"-my-retry-suffix\", dltTopicSuffix = \"-my-dlt-suffix\")\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyOtherPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .retryTopicSuffix(\"-my-retry-suffix\")\n .dltTopicSuffix(\"-my-dlt-suffix\")\n .create(template);\n}\n----\n\nNOTE: The default suffixes are \"-retry\" and \"-dlt\", for retry topics and dlt respectively.\n\n[[append-index-or-delay]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/topic-naming.adoc", "title": "topic-naming", "heading": "Retry Topics and DLT Suffixes", "heading_level": 2, "file_order": 59, "section_index": 1, "content_hash": "95b14363e7a507ac1c6f2588cf93e0cd1960f2ccdd6e8d4bad1153f1682dec8a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/topic-naming.adoc"}}
{"id": "sha256:6190ebd57ebe10e85270d9d38e82c45ac238a721a892178da3980b53c3138def", "content": "You can either append the topic's index or delay values after the suffix.\n\n[source, java]\n----\n@RetryableTopic(topicSuffixingStrategy = TopicSuffixingStrategy.SUFFIX_WITH_INDEX_VALUE)\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .suffixTopicsWithIndexValues()\n .create(template);\n }\n----\n\nNOTE: The default behavior is to suffix with the delay values, except for fixed delay configurations with multiple topics, in which case the topics are suffixed with the topic's index.\n\n[[single-topic-fixed-delay]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/topic-naming.adoc", "title": "topic-naming", "heading": "Appending the Topic's Index or Delay", "heading_level": 2, "file_order": 59, "section_index": 2, "content_hash": "6190ebd57ebe10e85270d9d38e82c45ac238a721a892178da3980b53c3138def", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/topic-naming.adoc"}}
{"id": "sha256:7290078f95d29b535158a13ebd1d87e69105b68cfe28415b52af496a43486dc6", "content": "If you're using fixed delay policies such as `FixedBackOffPolicy` or `NoBackOffPolicy` you can use a single topic to accomplish the non-blocking retries.\nThis topic will be suffixed with the provided or default suffix, and will not have either the index or the delay values appended.\n\nNOTE: The previous `FixedDelayStrategy` is now deprecated, and can be replaced by `SameIntervalTopicReuseStrategy`.\n\n[source, java]\n----\n@RetryableTopic(backOff = @BackOff(2_000), sameIntervalTopicReuseStrategy = SameIntervalTopicReuseStrategy.SINGLE_TOPIC)\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .fixedBackOff(3_000)\n .maxAttempts(5)\n .useSingleTopicForSameIntervals()\n .create(template);\n}\n----\n\nNOTE: The default behavior is creating separate retry topics for each attempt, appended with their index values: retry-0, retry-1, ...\n\n[[single-topic-maxinterval-delay]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/topic-naming.adoc", "title": "topic-naming", "heading": "Single Topic for Fixed Delay Retries", "heading_level": 2, "file_order": 59, "section_index": 3, "content_hash": "7290078f95d29b535158a13ebd1d87e69105b68cfe28415b52af496a43486dc6", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/topic-naming.adoc"}}
{"id": "sha256:fc2b695d3e4feda7d8c9cdd77690af0cfb380672297d6b0e7118f0ce5198cc46", "content": "If you're using exponential backoff policy (`ExponentialBackOffPolicy`), you can use a single retry topic to accomplish the non-blocking retries of the attempts whose delays are the configured `maxInterval`.\n\nThis \"final\" retry topic will be suffixed with the provided or default suffix, and will have either the index or the `maxInterval` value appended.\n\nNOTE: By opting to use a single topic for the retries with the `maxInterval` delay, it may become more viable to configure an exponential retry policy that keeps retrying for a long time, because in this approach you do not need a large amount of topics.\n\nStarting 3.2, the default behavior is reuses the retry topic for the same intervals, when using exponential backoff, the retry topics are suffixed with the delay values, with the last retry topic reuses for the same intervals(corresponding to the `maxInterval` delay).\n\nFor instance, when configuring the exponential backoff with `initialInterval=1_000`, `multiplier=2`, and `maxInterval=16_000`, in order to keep trying for one hour, one would need to configure `maxAttempts` as 229, and by default the needed retry topics would be:\n\n* -retry-1000\n* -retry-2000\n* -retry-4000\n* -retry-8000\n* -retry-16000\n\nWhen using the strategy that work with the number of retry topics equal to the configured `maxAttempts` minus 1, the last retry topic (corresponding to the `maxInterval` delay) being suffixed with an additional index would be:\n\n* -retry-1000\n* -retry-2000\n* -retry-4000\n* -retry-8000\n* -retry-16000-0\n* -retry-16000-1\n* -retry-16000-2\n* ...\n* -retry-16000-224\n\nIf multiple topics are required, then that can be done using the following configuration.\n\n[source, java]\n----\n@RetryableTopic(attempts = 230,\n backOff = @BackOff(delay = 1_000, multiplier = 2, maxDelay = 16_000),\n sameIntervalTopicReuseStrategy = SameIntervalTopicReuseStrategy.MULTIPLE_TOPICS)\n@KafkaListener(topics = \"my-annotated-topic\")\npublic void processMessage(MyPojo message) {\n // ... message processing\n}\n----\n\n[source, java]\n----\n@Bean\npublic RetryTopicConfiguration myRetryTopic(KafkaTemplate<String, MyPojo> template) {\n return RetryTopicConfigurationBuilder\n .newInstance()\n .exponentialBackoff(1_000, 2, 16_000)\n .maxAttempts(230)\n .useSingleTopicForSameIntervals()\n .create(template);\n}\n----\n\n[[custom-naming-strategies]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/topic-naming.adoc", "title": "topic-naming", "heading": "Single Topic for maxInterval Exponential Delay", "heading_level": 2, "file_order": 59, "section_index": 4, "content_hash": "fc2b695d3e4feda7d8c9cdd77690af0cfb380672297d6b0e7118f0ce5198cc46", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/topic-naming.adoc"}}
{"id": "sha256:d845f7321070c9d9ada4d32916d33fa7ea39eeb9e26c1741b25d87cc99586140", "content": "More complex naming strategies can be accomplished by registering a bean that implements `RetryTopicNamesProviderFactory`.\nThe default implementation is `SuffixingRetryTopicNamesProviderFactory` and a different implementation can be registered in the following way:\n\n[source, java]\n----\n@Override\nprotected RetryTopicComponentFactory createComponentFactory() {\n return new RetryTopicComponentFactory() {\n @Override\n public RetryTopicNamesProviderFactory retryTopicNamesProviderFactory() {\n return new CustomRetryTopicNamesProviderFactory();\n }\n };\n}\n----\n\nAs an example, the following implementation, in addition to the standard suffix, adds a prefix to retry/dlt topics names:\n\n[source, java]\n----\npublic class CustomRetryTopicNamesProviderFactory implements RetryTopicNamesProviderFactory {\n\n @Override\n public RetryTopicNamesProvider createRetryTopicNamesProvider(\n DestinationTopic.Properties properties) {\n\n if (properties.isMainEndpoint()) {\n return new SuffixingRetryTopicNamesProvider(properties);\n }\n else {\n return new SuffixingRetryTopicNamesProvider(properties) {\n\n @Override\n public String getTopicName(String topic) {\n return \"my-prefix-\" + super.getTopicName(topic);\n }\n\n };\n }\n }\n\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic/topic-naming.adoc", "title": "topic-naming", "heading": "Custom Naming Strategies", "heading_level": 2, "file_order": 59, "section_index": 5, "content_hash": "d845f7321070c9d9ada4d32916d33fa7ea39eeb9e26c1741b25d87cc99586140", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic/topic-naming.adoc"}}
{"id": "sha256:988775e2cb9091a027415779c35713a22d52a403c0941f2e0e6255433394bc83", "content": "[appendix]\n\n[appendix]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/appendix.adoc", "title": "Appendix", "heading": "Appendix", "heading_level": 1, "file_order": 60, "section_index": 0, "content_hash": "988775e2cb9091a027415779c35713a22d52a403c0941f2e0e6255433394bc83", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/appendix.adoc"}}
{"id": "sha256:d46213cd290faa6e6225af7a3233019137dd055f76d70f3db8ba8e9cae0b9912", "content": "[[spring-kafka-reference]]\nGary Russell; Artem Bilan; Biju Kunjummen; Jay Bryant; Soby Chacko; Tomaz Fernandes\n\n*{project-version}*\n\nThe Spring for Apache Kafka project applies core Spring concepts to the development of Kafka-based messaging solutions.\nWe provide a \"`template`\" as a high-level abstraction for sending messages.\nWe also provide support for Message-driven POJOs.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/index.adoc", "title": "index", "heading": "index", "heading_level": 1, "file_order": 61, "section_index": 0, "content_hash": "d46213cd290faa6e6225af7a3233019137dd055f76d70f3db8ba8e9cae0b9912", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/index.adoc"}}
{"id": "sha256:edbdd01482e77a28117c288d5381e2d2bfa3d454fe9099e4226b55852b336025", "content": "[[introduction]]\n\nThis first part of the reference documentation is a high-level overview of Spring for Apache Kafka and the underlying concepts and some code snippets that can help you get up and running as quickly as possible.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/introduction.adoc", "title": "introduction", "heading": "introduction", "heading_level": 1, "file_order": 62, "section_index": 0, "content_hash": "edbdd01482e77a28117c288d5381e2d2bfa3d454fe9099e4226b55852b336025", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/introduction.adoc"}}
{"id": "sha256:ce85b992016f9de2670e2c0933ab09385f16303a2c21608fe2eea51dcda8d458", "content": "[[kafka]]\n\nThis section offers detailed explanations of the various concerns that impact using Spring for Apache Kafka.\nFor a quick but less detailed introduction, see xref:quick-tour.adoc[Quick Tour].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/kafka.adoc", "title": "kafka", "heading": "kafka", "heading_level": 1, "file_order": 63, "section_index": 0, "content_hash": "ce85b992016f9de2670e2c0933ab09385f16303a2c21608fe2eea51dcda8d458", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/kafka.adoc"}}
{"id": "sha256:9e80eacdeaeb3c4523e9577b8b950cab5633a41e5ff5a8b5eb3a1e3ea924055b", "content": "[[other-resources]]\n\nIn addition to this reference documentation, we recommend a number of other resources that may help you learn about Spring and Apache Kafka.\n\n- {kafka-page}[Apache Kafka Project Home Page]\n- https://projects.spring.io/spring-kafka/[Spring for Apache Kafka Home Page]\n- https://github.com/spring-projects/spring-kafka[Spring for Apache Kafka GitHub Repository]\n- https://github.com/spring-projects/spring-integration[Spring Integration GitHub Repository (Apache Kafka Module)]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/other-resources.adoc", "title": "other-resources", "heading": "other-resources", "heading_level": 1, "file_order": 64, "section_index": 0, "content_hash": "9e80eacdeaeb3c4523e9577b8b950cab5633a41e5ff5a8b5eb3a1e3ea924055b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/other-resources.adoc"}}
{"id": "sha256:758edd2d1de89b1beb1cf97284663b76effc7ca2ac9f528fc4c459a47d1902f4", "content": "[[quick-tour]]\n\nPrerequisites: You must install and run Apache Kafka.\nThen you must put the Spring for Apache Kafka (`spring-kafka`) JAR and all of its dependencies on your classpath.\nThe easiest way to do that is to declare a dependency in your build tool.\n\nIf you are not using Spring Boot, declare the `spring-kafka` jar as a dependency in your project.\n\n[tabs]\n======\nMaven::\n+\n[source,xml,subs=\"+attributes\",role=\"primary\"]\n----\n<dependency>\n <groupId>org.springframework.kafka</groupId>\n <artifactId>spring-kafka</artifactId>\n <version>{project-version}</version>\n</dependency>\n----\n\nGradle::\n+\n[source,groovy,subs=\"+attributes\",role=\"secondary\"]\n----\nimplementation 'org.springframework.kafka:spring-kafka:{project-version}'\n----\n======\n\nIMPORTANT: When using Spring Boot, (and you haven't used start.spring.io to create your project), use the `spring-boot-starter-kafka` dependency and omit the version; Boot will automatically bring in the correct version that is compatible with your Boot version:\n\n[tabs]\n======\nMaven::\n+\n[source,xml,subs=\"+attributes\",role=\"primary\"]\n----\n<dependency>\n <groupId>org.springframework.boot</groupId>\n <artifactId>spring-boot-starter-kafka</artifactId>\n</dependency>\n----\n\nGradle::\n+\n[source,groovy,subs=\"+attributes\",role=\"secondary\"]\n----\nimplementation 'org.springframework.boot:spring-boot-starter-kafka'\n----\n======\n\nHowever, the quickest way to get started is to use https://start.spring.io[start.spring.io] (or the wizards in Spring Tool Suits and Intellij IDEA) and create a project, selecting 'Spring for Apache Kafka' as a dependency.\n\n[[compatibility]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/quick-tour.adoc", "title": "quick-tour", "heading": "quick-tour", "heading_level": 1, "file_order": 65, "section_index": 0, "content_hash": "758edd2d1de89b1beb1cf97284663b76effc7ca2ac9f528fc4c459a47d1902f4", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/quick-tour.adoc"}}
{"id": "sha256:4a62066fedd6389392250496bf1be0b2e9d5830729e1380242240b712281a902", "content": "This quick tour works with the following versions:\n\n* Apache Kafka Clients 4.0.x\n* Spring Framework 7.0.0\n* Minimum Java version: 17\n\n[[getting-started]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/quick-tour.adoc", "title": "quick-tour", "heading": "Compatibility", "heading_level": 2, "file_order": 65, "section_index": 1, "content_hash": "4a62066fedd6389392250496bf1be0b2e9d5830729e1380242240b712281a902", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/quick-tour.adoc"}}
{"id": "sha256:db6f467951f91bee0faeb1ab2f6d0e8907fa0df3d5f73b11e0568bedb23c02ac", "content": "The simplest way to get started is to use https://start.spring.io[start.spring.io] (or the wizards in Spring Tool Suits and Intellij IDEA) and create a project, selecting 'Spring for Apache Kafka' as a dependency.\nRefer to the {spring-boot-url}/reference/messaging/kafka.html[Spring Boot documentation] for more information about its opinionated auto configuration of the infrastructure beans.\n\nHere is a minimal consumer application.\n\n[[spring-boot-consumer-app]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/quick-tour.adoc", "title": "quick-tour", "heading": "Getting Started", "heading_level": 2, "file_order": 65, "section_index": 2, "content_hash": "db6f467951f91bee0faeb1ab2f6d0e8907fa0df3d5f73b11e0568bedb23c02ac", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/quick-tour.adoc"}}
{"id": "sha256:38bd77067f575269d663c0f97792cb7b5834bc18b5c6271552e8056e863e098b", "content": ".Application\n[tabs]\n======\nJava::\n+\n[source, java, role=\"primary\"]\n----\ninclude::{java-examples}/started/consumer/Application.java[tag=startedConsumer]\n----\n\nKotlin::\n+\n[source, kotlin, role=\"secondary\"]\n----\ninclude::{kotlin-examples}/started/consumer/Application.kt[tag=startedConsumer]\n----\n======\n\n.application.properties\n[source, properties]\n----\nspring.kafka.consumer.auto-offset-reset=earliest\n----\n\nThe `NewTopic` bean causes the topic to be created on the broker; it is not needed if the topic already exists.\n\n[[spring-boot-producer-app]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/quick-tour.adoc", "title": "quick-tour", "heading": "Spring Boot Consumer App", "heading_level": 3, "file_order": 65, "section_index": 3, "content_hash": "38bd77067f575269d663c0f97792cb7b5834bc18b5c6271552e8056e863e098b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/quick-tour.adoc"}}
{"id": "sha256:39291456596952ecdda21f17fd4e1965dcaf3023b2799863c2ce722fee705654", "content": ".Application\n[tabs]\n======\nJava::\n+\n[source,java,role=\"primary\"]\n----\ninclude::{java-examples}/started/producer/Application.java[tag=startedProducer]\n----\n\nKotlin::\n+\n[source,kotlin,role=\"secondary\"]\n----\ninclude::{kotlin-examples}/started/producer/Application.kt[tag=startedProducer]\n----\n======\n\n[[with-java-configuration-no-spring-boot]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/quick-tour.adoc", "title": "quick-tour", "heading": "Spring Boot Producer App", "heading_level": 3, "file_order": 65, "section_index": 4, "content_hash": "39291456596952ecdda21f17fd4e1965dcaf3023b2799863c2ce722fee705654", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/quick-tour.adoc"}}
{"id": "sha256:dce323df3ffc916d6d5d29756b3520810cace70235b86a571f217a9f310584f1", "content": "IMPORTANT: Spring for Apache Kafka is designed to be used in a Spring Application Context.\nFor example, if you create the listener container yourself outside of a Spring context, not all functions will work unless you satisfy all of the `+++...+++Aware` interfaces that the container implements.\n\nHere is an example of an application that does not use Spring Boot; it has both a `Consumer` and `Producer`.\n\n.Without Spring Boot\n[tabs]\n======\nJava::\n+\n[source,java,role=\"primary\"]\n----\ninclude::{java-examples}/started/noboot/Sender.java[tag=startedNoBootSender]\n\ninclude::{java-examples}/started/noboot/Listener.java[tag=startedNoBootListener]\n\ninclude::{java-examples}/started/noboot/Config.java[tag=startedNoBootConfig]\n----\n\nKotlin::\n+\n[source,kotlin,role=\"secondary\"]\n----\ninclude::{kotlin-examples}/started/noboot/Sender.kt[tag=startedNoBootSender]\n\ninclude::{kotlin-examples}/started/noboot/Listener.kt[tag=startedNoBootListener]\n\ninclude::{kotlin-examples}/started/noboot/Config.kt[tag=startedNoBootConfig]\n----\n======\n\nAs you can see, you have to define several infrastructure beans when not using Spring Boot.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/quick-tour.adoc", "title": "quick-tour", "heading": "With Java Configuration (No Spring Boot)", "heading_level": 3, "file_order": 65, "section_index": 5, "content_hash": "dce323df3ffc916d6d5d29756b3520810cace70235b86a571f217a9f310584f1", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/quick-tour.adoc"}}
{"id": "sha256:81459af2e3951d1e816fd45b2b874eb354d35270e2cbeaf2e31879003c45df04", "content": "[[reference]]\n\nThis part of the reference documentation details the various components that comprise Spring for Apache Kafka.\nThe xref:kafka.adoc[main chapter] covers the core classes to develop a Kafka application with Spring.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/reference.adoc", "title": "reference", "heading": "reference", "heading_level": 1, "file_order": 66, "section_index": 0, "content_hash": "81459af2e3951d1e816fd45b2b874eb354d35270e2cbeaf2e31879003c45df04", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/reference.adoc"}}
{"id": "sha256:2ed1fcbcd7226d109bfa9c670c422066ff657a6ae8ef2160f2b9c38e8fe52388", "content": "[[retry-topic]]\n\nVersion 2.9 changed the mechanism to bootstrap infrastructure beans; see xref:retrytopic/retry-config.adoc[Configuration] for the two mechanisms that are now required to bootstrap the feature.\n\nAchieving non-blocking retry / dlt functionality with Kafka usually requires setting up extra topics and creating and configuring the corresponding listeners.\nSince 2.7 Spring for Apache Kafka offers support for that via the `@RetryableTopic` annotation and `RetryTopicConfiguration` class to simplify that bootstrapping.\n\nSince 3.2, Spring for Apache Kafka supports non-blocking retries with xref:kafka/receiving-messages/class-level-kafkalistener.adoc[@KafkaListener on a Class].\n\nIMPORTANT: Non-blocking retries are not supported with xref:kafka/receiving-messages/listener-annotation.adoc#batch-listeners[Batch Listeners].\n\nIMPORTANT: Non-Blocking Retries cannot combine with xref:kafka/transactions.adoc#container-transaction-manager[Container Transactions].", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/retrytopic.adoc", "title": "retrytopic", "heading": "retrytopic", "heading_level": 1, "file_order": 67, "section_index": 0, "content_hash": "2ed1fcbcd7226d109bfa9c670c422066ff657a6ae8ef2160f2b9c38e8fe52388", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/retrytopic.adoc"}}
{"id": "sha256:3635ffed53eeedd7217c725863abad439f89154968265632af907b5b39bc7d47", "content": "[[streams-kafka-streams]]\n\nStarting with version 1.1.4, Spring for Apache Kafka provides first-class support for {kafka-url}/documentation/streams[Kafka Streams].\nTo use it from a Spring application, the `kafka-streams` jar must be present on classpath.\nIt is an optional dependency of the Spring for Apache Kafka project and is not downloaded transitively.\n\n[[basics]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "streams", "heading_level": 1, "file_order": 68, "section_index": 0, "content_hash": "3635ffed53eeedd7217c725863abad439f89154968265632af907b5b39bc7d47", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:20fa1dccb317938e1fccece26034062da5b9607dbed0b22031b8d32cb1811d3f", "content": "The reference Apache Kafka Streams documentation suggests the following way of using the API:\n\n[source, java]\n----\n\nStreamsBuilder builder = ...; // when using the Kafka Streams DSL\n\nStreamsConfig config = ...;\n\nKafkaStreams streams = new KafkaStreams(builder, config);\n\nstreams.start();\n\nstreams.close();\n----\n\nSo, we have two main components:\n\n* `StreamsBuilder`: With an API to build `KStream` (or `KTable`) instances.\n* `KafkaStreams`: To manage the lifecycle of those instances.\n\nNOTE: All `KStream` instances exposed to a `KafkaStreams` instance by a single `StreamsBuilder` are started and stopped at the same time, even if they have different logic.\nIn other words, all streams defined by a `StreamsBuilder` are tied with a single lifecycle control.\nOnce a `KafkaStreams` instance has been closed by `streams.close()`, it cannot be restarted.\nInstead, a new `KafkaStreams` instance to restart stream processing must be created.\n\n[[streams-spring]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Basics", "heading_level": 2, "file_order": 68, "section_index": 1, "content_hash": "20fa1dccb317938e1fccece26034062da5b9607dbed0b22031b8d32cb1811d3f", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:3da4a7ec04dc0608c3f0a7986c1050a3f04e9f3a6e8a04f9b73aca69af2345e0", "content": "To simplify using Kafka Streams from the Spring application context perspective and use the lifecycle management through a container, Spring for Apache Kafka introduces `StreamsBuilderFactoryBean`.\nThis is an `AbstractFactoryBean` implementation to expose a `StreamsBuilder` singleton instance as a bean.\nThe following example creates such a bean:\n\n[source, java]\n----\n@Bean\npublic FactoryBean<StreamsBuilder> myKStreamBuilder(KafkaStreamsConfiguration streamsConfig) {\n return new StreamsBuilderFactoryBean(streamsConfig);\n}\n----\n\nIMPORTANT: Starting with version 2.2, the stream configuration is now provided as a `KafkaStreamsConfiguration` object rather than a `StreamsConfig`.\n\nThe `StreamsBuilderFactoryBean` also implements `SmartLifecycle` to manage the lifecycle of an internal `KafkaStreams` instance.\nSimilar to the Kafka Streams API, you must define the `KStream` instances before you start the `KafkaStreams`.\nThat also applies for the Spring API for Kafka Streams.\nTherefore, when you use default `autoStartup = true` on the `StreamsBuilderFactoryBean`, you must declare `KStream` instances on the `StreamsBuilder` before the application context is refreshed.\nFor example, `KStream` can be a regular bean definition, while the Kafka Streams API is used without any impacts.\nThe following example shows how to do so:\n\n[source, java]\n----\n@Bean\npublic KStream<?, ?> kStream(StreamsBuilder kStreamBuilder) {\n KStream<Integer, String> stream = kStreamBuilder.stream(STREAMING_TOPIC1);\n // Fluent KStream API\n return stream;\n}\n----\n\nIf you would like to control the lifecycle manually (for example, stopping and starting by some condition), you can reference the `StreamsBuilderFactoryBean` bean directly by using the factory bean (`&`) {spring-framework-reference-url}/core/beans/factory-extension.html#beans-factory-extension-factorybean[prefix].\nSince `StreamsBuilderFactoryBean` uses its internal `KafkaStreams` instance, it is safe to stop and restart it again.\nA new `KafkaStreams` is created on each `start()`.\nYou might also consider using different `StreamsBuilderFactoryBean` instances, if you would like to control the lifecycles for `KStream` instances separately.\n\nYou also can specify `KafkaStreams.StateListener`, `Thread.UncaughtExceptionHandler`, and `StateRestoreListener` options on the `StreamsBuilderFactoryBean`, which are delegated to the internal `KafkaStreams` instance.\n\nAlso, apart from setting those options indirectly on `StreamsBuilderFactoryBean`, you can use a `KafkaStreamsCustomizer` callback interface to:\n\n1. (from _version 2.1.5_) configure an inner `KafkaStreams` instance using `customize(KafkaStreams)`\n2. (from _version 3.3.0_) instantiate a custom implementation of `KafkaStreams` using `initKafkaStreams(Topology, Properties, KafkaClientSupplier)`\n\nNote that `KafkaStreamsCustomizer` overrides the options provided by `StreamsBuilderFactoryBean`.\n\nIf you need to perform some `KafkaStreams` operations directly, you can access that internal `KafkaStreams` instance by using `StreamsBuilderFactoryBean.getKafkaStreams()`.\n\nYou can autowire `StreamsBuilderFactoryBean` bean by type, but you should be sure to use the full type in the bean definition, as the following example shows:\n\n[source,java]\n----\n@Bean\npublic StreamsBuilderFactoryBean myKStreamBuilder(KafkaStreamsConfiguration streamsConfig) {\n return new StreamsBuilderFactoryBean(streamsConfig);\n}\n...\n@Autowired\nprivate StreamsBuilderFactoryBean myKStreamBuilderFactoryBean;\n----\n\nAlternatively, you can add `@Qualifier` for injection by name if you use interface bean definition.\nThe following example shows how to do so:\n\n[source,java]\n----\n@Bean\npublic FactoryBean<StreamsBuilder> myKStreamBuilder(KafkaStreamsConfiguration streamsConfig) {\n return new StreamsBuilderFactoryBean(streamsConfig);\n}\n...\n@Autowired\n@Qualifier(\"&myKStreamBuilder\")\nprivate StreamsBuilderFactoryBean myKStreamBuilderFactoryBean;\n----\n\nStarting with version 2.4.1, the factory bean has a new property `infrastructureCustomizer` with type `KafkaStreamsInfrastructureCustomizer`; this allows customization of the `StreamsBuilder` (e.g. to add a state store) and/or the `Topology` before the stream is created.\n\n[source, java]\n----\npublic interface KafkaStreamsInfrastructureCustomizer {\n\n void configureBuilder(StreamsBuilder builder);\n\n void configureTopology(Topology topology);\n\n}\n----\n\nDefault no-op implementations are provided to avoid having to implement both methods if one is not required.\n\nA `CompositeKafkaStreamsInfrastructureCustomizer` is provided, for when you need to apply multiple customizers.\n\n[[streams-micrometer]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Spring Management", "heading_level": 2, "file_order": 68, "section_index": 2, "content_hash": "3da4a7ec04dc0608c3f0a7986c1050a3f04e9f3a6e8a04f9b73aca69af2345e0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:1069a5eb625b24475da7b518db8d32a9e1f71a26aa8fbf6aa1031cef81e01a00", "content": "Introduced in version 2.5.3, you can configure a `KafkaStreamsMicrometerListener` to automatically register micrometer meters for the `KafkaStreams` object managed by the factory bean:\n\n[source, java]\n----\nstreamsBuilderFactoryBean.addListener(new KafkaStreamsMicrometerListener(meterRegistry,\n Collections.singletonList(new ImmutableTag(\"customTag\", \"customTagValue\"))));\n----\n\n[[serde]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "KafkaStreams Micrometer Support", "heading_level": 2, "file_order": 68, "section_index": 3, "content_hash": "1069a5eb625b24475da7b518db8d32a9e1f71a26aa8fbf6aa1031cef81e01a00", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:8bf559845f24f78a703b2a9f509d50607a8e298899f78dd6e746ab92358db08e", "content": "For serializing and deserializing data when reading or writing to topics or state stores in JSON format, Spring for Apache Kafka provides a `JacksonJsonSerde` implementation that uses JSON, delegating to the `JacksonJsonSerializer` and `JacksonJsonDeserializer` described in xref:kafka/serdes.adoc[Serialization, Deserialization, and Message Conversion].\nThe `JacksonJsonSerde` implementation provides the same configuration options through its constructor (target type or `ObjectMapper`).\nIn the following example, we use the `JacksonJsonSerde` to serialize and deserialize the `Cat` payload of a Kafka stream (the `JacksonJsonSerde` can be used in a similar fashion wherever an instance is required):\n\n[source,java]\n----\nstream.through(Serdes.Integer(), new JacksonJsonSerde<>(Cat.class), \"cats\");\n----\n\nWhen constructing the serializer/deserializer programmatically for use in the producer/consumer factory, since version 2.3, you can use the fluent API, which simplifies configuration.\n\n[source, java]\n----\nstream.through(\n new JacksonJsonSerde<>(MyKeyType.class)\n .forKeys()\n .noTypeInfo(),\n new JacksonJsonSerde<>(MyValueType.class)\n .noTypeInfo(),\n \"myTypes\");\n----\n\n[[using-kafkastreambrancher]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Streams JSON Serialization and Deserialization", "heading_level": 2, "file_order": 68, "section_index": 4, "content_hash": "8bf559845f24f78a703b2a9f509d50607a8e298899f78dd6e746ab92358db08e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:19d933fb700479b628dd273f85171f585ffaac7b34c6d8e54e3388f1863eab83", "content": "The `KafkaStreamBrancher` class introduces a more convenient way to build conditional branches on top of `KStream`.\n\nConsider the following example that does not use `KafkaStreamBrancher`:\n\n[source,java]\n----\nKStream<String, String>[] branches = builder.stream(\"source\").branch(\n (key, value) -> value.contains(\"A\"),\n (key, value) -> value.contains(\"B\"),\n (key, value) -> true\n);\nbranches[0].to(\"A\");\nbranches[1].to(\"B\");\nbranches[2].to(\"C\");\n----\n\nThe following example uses `KafkaStreamBrancher`:\n\n[source,java]\n----\nnew KafkaStreamBrancher<String, String>()\n .branch((key, value) -> value.contains(\"A\"), ks -> ks.to(\"A\"))\n .branch((key, value) -> value.contains(\"B\"), ks -> ks.to(\"B\"))\n //default branch should not necessarily be defined in the end of the chain!\n .defaultBranch(ks -> ks.to(\"C\"))\n .onTopOf(builder.stream(\"source\"));\n //onTopOf method returns the provided stream so we can continue with method chaining\n----\n\n[[streams-config]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Using `KafkaStreamBrancher`", "heading_level": 2, "file_order": 68, "section_index": 5, "content_hash": "19d933fb700479b628dd273f85171f585ffaac7b34c6d8e54e3388f1863eab83", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:e6a5e36e1231444d86278b81368ab811aa627043c7a1b414edf46ba3fe01b40a", "content": "To configure the Kafka Streams environment, the `StreamsBuilderFactoryBean` requires a `KafkaStreamsConfiguration` instance.\nSee the Apache Kafka {kafka-url}/documentation/#streamsconfigs[documentation] for all possible options.\n\nIMPORTANT: Starting with version 2.2, the stream configuration is now provided as a `KafkaStreamsConfiguration` object, rather than as a `StreamsConfig`.\n\nTo avoid boilerplate code for most cases, especially when you develop microservices, Spring for Apache Kafka provides the `@EnableKafkaStreams` annotation, which you should place on a `@Configuration` class.\nAll you need is to declare a `KafkaStreamsConfiguration` bean named `defaultKafkaStreamsConfig`.\nA `StreamsBuilderFactoryBean` bean, named `defaultKafkaStreamsBuilder`, is automatically declared in the application context.\nYou can declare and use any additional `StreamsBuilderFactoryBean` beans as well.\nYou can perform additional customization of that bean, by providing a bean that implements `StreamsBuilderFactoryBeanConfigurer`.\nIf there are multiple such beans, they will be applied according to their `Ordered.order` property.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Configuration", "heading_level": 2, "file_order": 68, "section_index": 6, "content_hash": "e6a5e36e1231444d86278b81368ab811aa627043c7a1b414edf46ba3fe01b40a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:4ab5b907c9773884b23583253c1df5dfb304d30d092dca2b3246552abbc7c0cc", "content": "When the factory is stopped, the `KafkaStreams.close()` is called with 2 parameters :\n\n* closeTimeout : how long to wait for the threads to shutdown (defaults to `DEFAULT_CLOSE_TIMEOUT` set to 10 seconds). Can be configured using `StreamsBuilderFactoryBean.setCloseTimeout()`.\n* leaveGroupOnClose : to trigger consumer leave call from the group (defaults to `false`). Can be configured using `StreamsBuilderFactoryBean.setLeaveGroupOnClose()`.\n\nBy default, when the factory bean is stopped, the `KafkaStreams.cleanUp()` method is called.\nStarting with version 2.1.2, the factory bean has additional constructors, taking a `CleanupConfig` object that has properties to let you control whether the `cleanUp()` method is called during `start()` or `stop()` or neither.\nStarting with version 2.7, the default is to never clean up local state.\n\n[[streams-header-enricher]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Cleanup & Stop configuration", "heading_level": 3, "file_order": 68, "section_index": 7, "content_hash": "4ab5b907c9773884b23583253c1df5dfb304d30d092dca2b3246552abbc7c0cc", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:4a2e71bb30af18a8d2e789bd7314db6a2ce16cf7f1a2f82cf051a4d704a59751", "content": "Version 3.0 added the `HeaderEnricherProcessor` extension of `ContextualProcessor`; providing the same functionality as the deprecated `HeaderEnricher` which implemented the deprecated `Transformer` interface.\nThis can be used to add headers within the stream processing; the header values are SpEL expressions; the root object of the expression evaluation has 3 properties:\n\n* `record` - the `org.apache.kafka.streams.processor.api.Record` (`key`, `value`, `timestamp`, `headers`)\n* `key` - the key of the current record\n* `value` - the value of the current record\n* `context` - the `ProcessorContext`, allowing access to the current record metadata\n\nThe expressions must return a `byte[]` or a `String` (which will be converted to `byte[]` using `UTF-8`).\n\nTo use the enricher within a stream:\n\n[source, java]\n----\n.process(() -> new HeaderEnricherProcessor(expressions))\n----\n\nThe processor does not change the `key` or `value`; it simply adds headers.\n\nIMPORTANT: You need a new instance for each record.\n\n[source, java]\n----\n.process(() -> new HeaderEnricherProcessor<..., ...>(expressionMap))\n----\n\nHere is a simple example, adding one literal header and one variable:\n\n[source, java]\n----\nMap<String, Expression> headers = new HashMap<>();\nheaders.put(\"header1\", new LiteralExpression(\"value1\"));\nSpelExpressionParser parser = new SpelExpressionParser();\nheaders.put(\"header2\", parser.parseExpression(\"record.timestamp() + ' @' + record.offset()\"));\nProcessorSupplier supplier = () -> new HeaderEnricher<String, String>(headers);\nKStream<String, String> stream = builder.stream(INPUT);\nstream\n .process(() -> supplier)\n .to(OUTPUT);\n----\n\n[[streams-messaging]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Header Enricher", "heading_level": 2, "file_order": 68, "section_index": 8, "content_hash": "4a2e71bb30af18a8d2e789bd7314db6a2ce16cf7f1a2f82cf051a4d704a59751", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:a94e03be4e51e947169acc1cae5aeb1da4f67b6c3e859ae9dbb0e32bd2c4c3c7", "content": "Version 3.0 added the `MessagingProcessor` extension of `ContextualProcessor`, providing the same functionality as the deprecated `MessagingTransformer` which implemented the deprecated `Transformer` interface.\nThis allows a Kafka Streams topology to interact with a Spring Messaging component, such as a Spring Integration flow.\nThe transformer requires an implementation of `MessagingFunction`.\n\n[source, java]\n----\n@FunctionalInterface\npublic interface MessagingFunction {\n\n Message<?> exchange(Message<?> message);\n\n}\n----\n\nSpring Integration automatically provides an implementation using its `GatewayProxyFactoryBean`.\nIt also requires a `MessagingMessageConverter` to convert the key, value and metadata (including headers) to/from a Spring Messaging `Message<?>`.\nSee {spring-integration-url}/kafka.html#streams-integration[Calling a Spring Integration Flow from a `KStream`] for more information.\n\n[[streams-deser-recovery]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "`MessagingProcessor`", "heading_level": 2, "file_order": 68, "section_index": 9, "content_hash": "a94e03be4e51e947169acc1cae5aeb1da4f67b6c3e859ae9dbb0e32bd2c4c3c7", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:abd2248aba9252077840d2f9f18e1bfb1e48d6b0482d1d4373984be1e5656426", "content": "Version 2.3 introduced the `RecoveringDeserializationExceptionHandler` which can take some action when a deserialization exception occurs.\nRefer to the Kafka documentation about `DeserializationExceptionHandler`, of which the `RecoveringDeserializationExceptionHandler` is an implementation.\nThe `RecoveringDeserializationExceptionHandler` is configured with a `ConsumerRecordRecoverer` implementation.\nThe framework provides the `DeadLetterPublishingRecoverer` which sends the failed record to a dead-letter topic.\nSee xref:kafka/annotation-error-handling.adoc#dead-letters[Publishing Dead-letter Records] for more information about this recoverer.\n\nTo configure the recoverer, add the following properties to your streams configuration:\n\n[source, java]\n----\n@Bean(name = KafkaStreamsDefaultConfiguration.DEFAULT_STREAMS_CONFIG_BEAN_NAME)\npublic KafkaStreamsConfiguration kStreamsConfigs() {\n Map<String, Object> props = new HashMap<>();\n ...\n props.put(StreamsConfig.DEFAULT_DESERIALIZATION_EXCEPTION_HANDLER_CLASS_CONFIG,\n RecoveringDeserializationExceptionHandler.class);\n props.put(RecoveringDeserializationExceptionHandler.KSTREAM_DESERIALIZATION_RECOVERER, recoverer());\n ...\n return new KafkaStreamsConfiguration(props);\n}\n\n@Bean\npublic DeadLetterPublishingRecoverer recoverer() {\n return new DeadLetterPublishingRecoverer(kafkaTemplate(),\n (record, ex) -> new TopicPartition(\"recovererDLQ\", -1));\n}\n----\n\nOf course, the `recoverer()` bean can be your own implementation of `ConsumerRecordRecoverer`.\n\n[[kafka-streams-iq-support]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Recovery from Deserialization Exceptions", "heading_level": 2, "file_order": 68, "section_index": 10, "content_hash": "abd2248aba9252077840d2f9f18e1bfb1e48d6b0482d1d4373984be1e5656426", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:4b6114e59d68c6ee8f8153c46efa04e487c767d4217e3b1fcf7ac200cc651f0a", "content": "Starting with version 3.2, Spring for Apache Kafka provides basic facilities required for interactive queries in Kafka Streams.\nInteractive queries are useful in stateful Kafka Streams applications since they provide a way to constantly query the stateful stores in the application.\nThus, if an application wants to materialize the current view of the system under consideration, interactive queries provide a way to do that.\nTo learn more about interactive queries, see this {kafka-url}/documentation/streams/developer-guide/interactive-queries.html[article].\nThe support in Spring for Apache Kafka is centered around an API called `KafkaStreamsInteractiveQueryService` which is a facade around interactive queries APIs in Kafka Streams library.\nAn application can create an instance of this service as a bean and then later on use it to retrieve the state store by its name.\n\nThe following code snippet shows an example.\n\n[source, java]\n----\n@Bean\npublic KafkaStreamsInteractiveQueryService kafkaStreamsInteractiveQueryService(StreamsBuilderFactoryBean streamsBuilderFactoryBean) {\n final KafkaStreamsInteractiveQueryService kafkaStreamsInteractiveQueryService =\n new KafkaStreamsInteractiveQueryService(streamsBuilderFactoryBean);\n return kafkaStreamsInteractiveQueryService;\n}\n----\n\nAssuming that a Kafka Streams application has a state store called `app-store`, then that store can be retrieved via the `KafkaStreamsInteractiveQuery` API as shown below.\n\n[source, java]\n----\n@Autowired\nprivate KafkaStreamsInteractiveQueryService interactiveQueryService;\n\nReadOnlyKeyValueStore<Object, Object> appStore = interactiveQueryService.retrieveQueryableStore(\"app-store\", QueryableStoreTypes.keyValueStore());\n----\n\nOnce an application gains access to the state store, then it can query from it for key-value information.\n\nIn this case, the state store that the application uses is a read-only key value store.\nThere are other types of state stores that a Kafka Streams application can use.\nFor instance, if an application prefers to query a window based store, it can build that store in the Kafka Streams application business logic and then later on retrieve it.\nBecause of this reason, the API to retrieve the queryable store in `KafkaStreamsInteractiveQueryService` has a generic store type signature, so that the end-user can assign the proper type.\n\nHere is the type signature from the API.\n\n[source, java]\n----\npublic <T> T retrieveQueryableStore(String storeName, QueryableStoreType<T> storeType)\n----\n\nWhen calling this method, the user can specifically ask for the proper state store type, as we have done in the above example.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Interactive Query Support", "heading_level": 2, "file_order": 68, "section_index": 11, "content_hash": "4b6114e59d68c6ee8f8153c46efa04e487c767d4217e3b1fcf7ac200cc651f0a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:0f29cbc8b26574d3c967034e0740d3a440b282162ecf8b13d7e723ebb19e9d44", "content": "When trying to retrieve the state store using the `KafkaStreamsInteractiveQueryService`, there is a chance that the state store might not be found for various reasons.\nIf those reasons are transitory, `KafkaStreamsInteractiveQueryService` provides an option to retry the retrieval of the state store by allowing to inject a custom `RetryTemplate`.\nBy default, the `RetryTemplate` that is used in `KafkaStreamsInteractiveQueryService` uses a maximum attempts of three with a fixed backoff of one second.\n\nHere is how you can inject a custom `RetryTemplate` into `KafkaStreamsInteractiveQueryService` with the maximum attempts of ten.\n\n[source, java]\n----\n@Bean\npublic KafkaStreamsInteractiveQueryService kafkaStreamsInteractiveQueryService(StreamsBuilderFactoryBean streamsBuilderFactoryBean) {\n final KafkaStreamsInteractiveQueryService kafkaStreamsInteractiveQueryService =\n new KafkaStreamsInteractiveQueryService(streamsBuilderFactoryBean);\n RetryTemplate retryTemplate = new RetryTemplate();\n retryTemplate.setBackOffPolicy(new FixedBackOffPolicy());\n RetryPolicy retryPolicy = new SimpleRetryPolicy(10);\n retryTemplate.setRetryPolicy(retryPolicy);\n kafkaStreamsInteractiveQueryService.setRetryTemplate(retryTemplate);\n return kafkaStreamsInteractiveQueryService;\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Retrying State Store Retrieval", "heading_level": 3, "file_order": 68, "section_index": 12, "content_hash": "0f29cbc8b26574d3c967034e0740d3a440b282162ecf8b13d7e723ebb19e9d44", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:d4254aca231104efcd19a02002b2b2fa046ca0fd25cb221df9d5802401e3f89d", "content": "The API shown above for retrieving the state store - `retrieveQueryableStore` is intended for locally available key-value state stores.\nIn productions settings, Kafka Streams applications are most likely distributed based on the number of partitions.\nIf a topic has four partitions and there are four instances of the same Kafka Streams processor running, then each instance maybe responsible for processing a single partition from the topic.\nIn this scenario, calling `retrieveQueryableStore` may not give the correct result that an instance is looking for, although it might return a valid store.\nLet's assume that the topic with four partitions has data about various keys and a single partition is always responsible for a specific key.\nIf the instance that is calling `retrieveQueryableStore` is looking for information about a key that this instance does not host, then it will not receive any data.\nThis is because the current Kafka Streams instance does not know anything about this key.\nTo fix this, the calling instance first needs to make sure that they have the host information for the Kafka Streams processor instance where the particular key is hosted.\nThis can be retrieved from any Kafka Streams instance under the same `application.id` as below.\n\n[source, java]\n----\n@Autowired\nprivate KafkaStreamsInteractiveQueryService interactiveQueryService;\n\nHostInfo kafkaStreamsApplicationHostInfo = this.interactiveQueryService.getKafkaStreamsApplicationHostInfo(\"app-store\", 12345, new IntegerSerializer());\n----\n\nIn the example code above, the calling instance is querying for a particular key `12345` from the state-store named `app-store`.\nThe API also needs a corresponding key serializer, which in this case is the `IntegerSerializer`.\nKafka Streams looks through all it's instances under the same `application.id` and tries to find which instance hosts this particular key,\nOnce found, it returns that host information as a `HostInfo` object.\n\nThis is how the API looks like:\n\n[source, java]\n----\npublic <K> HostInfo getKafkaStreamsApplicationHostInfo(String store, K key, Serializer<K> serializer)\n----\n\nWhen using multiple instances of the Kafka Streams processors of the same `application.id` in a distributed way like this, the application is supposed to provide an RPC layer where the state stores can be queried over an RPC endpoint such as a REST one.\nSee this {kafka-url}/documentation/streams/developer-guide/interactive-queries.html#querying-remote-state-stores-for-the-entire-app[article] for more details on this.\nWhen using Spring for Apache Kafka, it is very easy to add a Spring based REST endpoint by using the spring-web technologies.\nOnce there is a REST endpoint, then that can be used to query the state stores from any Kafka Streams instance, given the `HostInfo` where the key is hosted is known to the instance.\n\nIf the key hosting the instance is the current instance, then the application does not need to call the RPC mechanism, but rather make an in-JVM call.\nHowever, the trouble is that an application may not know that the instance that is making the call is where the key is hosted because a particular server may lose a partition due to a consumer rebalance.\nTo fix this issue, `KafkaStreamsInteractiveQueryService` provides a convenient API for querying the current host information via an API method `getCurrentKafkaStreamsApplicationHostInfo()` that returns the current `HostInfo`.\nThe idea is that the application can first acquire information about where the key is held, and then compare the `HostInfo` with the one about the current instance.\nIf the `HostInfo` data matches, then it can proceed with a simple JVM call via the `retrieveQueryableStore`, otherwise go with the RPC option.\n\n[[kafka-streams-example]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Querying Remote State Stores", "heading_level": 3, "file_order": 68, "section_index": 13, "content_hash": "d4254aca231104efcd19a02002b2b2fa046ca0fd25cb221df9d5802401e3f89d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:8343c41e0749ba5757458111d3977e5c3d9f35983b1bfb90f201e1365877aa69", "content": "The following example combines the various topics we have covered in this chapter:\n\n[source, java]\n----\n@Configuration\n@EnableKafka\n@EnableKafkaStreams\npublic class KafkaStreamsConfig {\n\n @Bean(name = KafkaStreamsDefaultConfiguration.DEFAULT_STREAMS_CONFIG_BEAN_NAME)\n public KafkaStreamsConfiguration kStreamsConfigs() {\n Map<String, Object> props = new HashMap<>();\n props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"testStreams\");\n props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.Integer().getClass().getName());\n props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());\n props.put(StreamsConfig.DEFAULT_TIMESTAMP_EXTRACTOR_CLASS_CONFIG, WallclockTimestampExtractor.class.getName());\n return new KafkaStreamsConfiguration(props);\n }\n\n @Bean\n public StreamsBuilderFactoryBeanConfigurer configurer() {\n return fb -> fb.setStateListener((newState, oldState) -> {\n System.out.println(\"State transition from \" + oldState + \" to \" + newState);\n });\n }\n\n @Bean\n public KStream<Integer, String> kStream(StreamsBuilder kStreamBuilder) {\n KStream<Integer, String> stream = kStreamBuilder.stream(\"streamingTopic1\");\n stream\n .mapValues((ValueMapper<String, String>) String::toUpperCase)\n .groupByKey()\n .windowedBy(TimeWindows.ofSizeWithNoGrace(Duration.ofMillis(1_000)))\n .reduce((String value1, String value2) -> value1 + value2,\n Named.as(\"windowStore\"))\n .toStream()\n .map((windowedId, value) -> new KeyValue<>(windowedId.key(), value))\n .filter((i, s) -> s.length() > 40)\n .to(\"streamingTopic2\");\n\n stream.print(Printed.toSysOut());\n\n return stream;\n }\n\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/streams.adoc", "title": "streams", "heading": "Kafka Streams Example", "heading_level": 2, "file_order": 68, "section_index": 14, "content_hash": "8343c41e0749ba5757458111d3977e5c3d9f35983b1bfb90f201e1365877aa69", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/streams.adoc"}}
{"id": "sha256:f840c858736b92de71e3dedc29279f41e2d9718ec8231f9cccf45e66d09084af", "content": "[[testing]]\n\nThe `spring-kafka-test` jar contains some useful utilities to assist with testing your applications.\n\nIMPORTANT: When using Spring Boot, use the `spring-boot-starter-kafka-test` dependency instead:\n\n[tabs]\n======\nMaven::\n+\n[source,xml,subs=\"+attributes\",role=\"primary\"]\n----\n<dependency>\n <groupId>org.springframework.boot</groupId>\n <artifactId>spring-boot-starter-kafka-test</artifactId>\n <scope>test</scope>\n</dependency>\n----\n\nGradle::\n+\n[source,groovy,subs=\"+attributes\",role=\"secondary\"]\n----\ntestImplementation 'org.springframework.boot:spring-boot-starter-kafka-test'\n----\n======\n\n[[ekb]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "testing", "heading_level": 1, "file_order": 69, "section_index": 0, "content_hash": "f840c858736b92de71e3dedc29279f41e2d9718ec8231f9cccf45e66d09084af", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:fdaa6369f0edbbd7acb0f7de99db48a7bad7e132260ed172a0c093a9ad7da26b", "content": "Since Kafka 4.0 has fully transitioned to KRaft mode, only the `EmbeddedKafkaKraftBroker` implementation is now available:\n\n* `EmbeddedKafkaKraftBroker` - uses `Kraft` in combined controller and broker modes.\n\nThere are several techniques to configure the broker as discussed in the following sections.\n\n[[ktu]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Embedded Kafka Broker", "heading_level": 2, "file_order": 69, "section_index": 1, "content_hash": "fdaa6369f0edbbd7acb0f7de99db48a7bad7e132260ed172a0c093a9ad7da26b", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:f84710ec53d184503bff7b82db5a6b1acd02df99991b925dec9813ee67447e48", "content": "`org.springframework.kafka.test.utils.KafkaTestUtils` provides a number of static helper methods to consume records, retrieve various record offsets, and others.\nRefer to its javadoc:org.springframework.kafka.test.utils.KafkaTestUtils[Javadocs] for complete details.\n\n[[junit]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "KafkaTestUtils", "heading_level": 2, "file_order": 69, "section_index": 2, "content_hash": "f84710ec53d184503bff7b82db5a6b1acd02df99991b925dec9813ee67447e48", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:14df46acea2d2dbe4594d9d0330692cd78006355e2463bdeebb984f58eccdc32", "content": "`org.springframework.kafka.test.utils.KafkaTestUtils` provides some static methods to set up producer and consumer properties.\nThe following listing shows those method signatures:\n\n[source, java]\n----\n/**\n * Set up test properties for an {@code <Integer, String>} consumer.\n * @param group the group id.\n * @param autoCommit the auto commit.\n * @param embeddedKafka a {@link EmbeddedKafkaBroker} instance.\n * @return the properties.\n */\npublic static Map<String, Object> consumerProps(String group, String autoCommit,\n EmbeddedKafkaBroker embeddedKafka) { ... }\n\n/**\n * Set up test properties for an {@code <Integer, String>} producer.\n * @param embeddedKafka a {@link EmbeddedKafkaBroker} instance.\n * @return the properties.\n */\npublic static Map<String, Object> producerProps(EmbeddedKafkaBroker embeddedKafka) { ... }\n----\n\n[NOTE]\n====\nStarting with version 2.5, the `consumerProps` method sets the `ConsumerConfig.AUTO_OFFSET_RESET_CONFIG` to `earliest`.\nThis is because, in most cases, you want the consumer to consume any messages sent in a test case.\nThe `ConsumerConfig` default is `latest` which means that messages already sent by a test, before the consumer starts, will not receive those records.\nTo revert to the previous behavior, set the property to `latest` after calling the method.\n\nWhen using the embedded broker, it is generally best practice using a different topic for each test, to prevent cross-talk.\nIf this is not possible for some reason, note that the `consumeFromEmbeddedTopics` method's default behavior is to seek the assigned partitions to the beginning after assignment.\nSince it does not have access to the consumer properties, you must use the overloaded method that takes a `seekToEnd` boolean parameter to seek to the end instead of the beginning.\n====\n\n[NOTE]\n====\nSpring for Apache Kafka no longer supports JUnit 4.\nMigration to JUnit Jupiter is recommended.\n====\n\nThe `EmbeddedKafkaBroker` class has a utility method that lets you consume for all the topics it created.\nThe following example shows how to use it:\n\n[source, java]\n----\nMap<String, Object> consumerProps = KafkaTestUtils.consumerProps(\"testT\", \"false\", embeddedKafka);\nDefaultKafkaConsumerFactory<Integer, String> cf = new DefaultKafkaConsumerFactory<>(consumerProps);\nConsumer<Integer, String> consumer = cf.createConsumer();\nembeddedKafka.consumeFromAllEmbeddedTopics(consumer);\n----\n\nThe `KafkaTestUtils` has some utility methods to fetch results from the consumer.\nThe following listing shows those method signatures:\n\n[source, java]\n----\n/**\n * Poll the consumer, expecting a single record for the specified topic.\n * @param consumer the consumer.\n * @param topic the topic.\n * @return the record.\n * @throws org.junit.ComparisonFailure if exactly one record is not received.\n */\npublic static <K, V> ConsumerRecord<K, V> getSingleRecord(Consumer<K, V> consumer, String topic) { ... }\n\n/**\n * Poll the consumer for records.\n * @param consumer the consumer.\n * @return the records.\n */\npublic static <K, V> ConsumerRecords<K, V> getRecords(Consumer<K, V> consumer) { ... }\n----\n\nThe following example shows how to use `KafkaTestUtils`:\n\n[source, java]\n----\n...\ntemplate.sendDefault(0, 2, \"bar\");\nConsumerRecord<Integer, String> received = KafkaTestUtils.getSingleRecord(consumer, \"topic\");\n...\n----\n\nWhen the embedded Kafka broker is started by the `EmbeddedKafkaBroker`, a system property named `spring.embedded.kafka.brokers` is set to the address of the Kafka brokers.\nConvenient constants (`EmbeddedKafkaBroker.SPRING_EMBEDDED_KAFKA_BROKERS`) are provided for this property.\n\nInstead of default `spring.embedded.kafka.brokers` system property, the address of the Kafka brokers can be exposed to any arbitrary and convenient property.\nFor this purpose a `spring.embedded.kafka.brokers.property` (`EmbeddedKafkaBroker.BROKER_LIST_PROPERTY`) system property can be set before starting an embedded Kafka.\nFor example, with Spring Boot a `spring.kafka.bootstrap-servers` configuration property is expected to be set for auto-configuring Kafka client, respectively.\nSo, before running tests with an embedded Kafka on random ports, we can set `spring.embedded.kafka.brokers.property=spring.kafka.bootstrap-servers` as a system property - and the `EmbeddedKafkaBroker` will use it to expose its broker addresses.\nThis is now the default value for this property (starting with version 3.0.10).\n\nWith the `EmbeddedKafkaBroker.brokerProperties(Map<String, String>)`, you can provide additional properties for the Kafka servers.\nSee {kafka-url}/documentation/#brokerconfigs[Kafka Config] for more information about possible broker properties.\n\n[[configuring-topics]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "JUnit", "heading_level": 2, "file_order": 69, "section_index": 3, "content_hash": "14df46acea2d2dbe4594d9d0330692cd78006355e2463bdeebb984f58eccdc32", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:334643488ee390b0e6ec2bdc6b300e79d7c67de507cfd2175fa1b01569e769d4", "content": "The following example configuration creates topics called `cat` and `hat` with five partitions, a topic called `thing1` with 10 partitions, and a topic called `thing2` with 15 partitions:\n\n[source, java]\n----\n@SpringJUnitConfig\n@EmbeddedKafka(\n partitions = 5,\n topics = {\"cat\", \"hat\"}\n)\npublic class MyTests {\n\n @Autowired\n private EmbeddedKafkaBroker broker;\n\n @Test\n public void test() {\n broker.addTopics(new NewTopic(\"thing1\", 10, (short) 1), new NewTopic(\"thing2\", 15, (short) 1));\n ...\n }\n\n}\n----\n\nBy default, `addTopics` will throw an exception when problems arise (such as adding a topic that already exists).\nVersion 2.6 added a new version of that method that returns a `Map<String, Exception>`; the key is the topic name and the value is `null` for success, or an `Exception` for a failure.\n\n[[same-broker-multiple-tests]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Configuring Topics", "heading_level": 2, "file_order": 69, "section_index": 4, "content_hash": "334643488ee390b0e6ec2bdc6b300e79d7c67de507cfd2175fa1b01569e769d4", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:1c9c5ec591e7bf264fb98f61b2f53d0b8d1be42107046ef6390dfb4cfa231c21", "content": "You can use the same broker for multiple test classes with something similar to the following:\n\n[source, java]\n----\npublic final class EmbeddedKafkaHolder {\n\n private static EmbeddedKafkaBroker embeddedKafka = new EmbeddedKafkaZKBroker(1, false)\n .brokerListProperty(\"spring.kafka.bootstrap-servers\");\n\n private static volatile boolean started;\n\n public static EmbeddedKafkaBroker getEmbeddedKafka() {\n if (!started) {\n synchronized (EmbeddedKafkaBroker.class) {\n try {\n embeddedKafka.afterPropertiesSet();\n }\n catch (Exception e) {\n throw new KafkaException(\"Embedded broker failed to start\", e);\n }\n started = true;\n }\n }\n return embeddedKafka;\n }\n}\n----\n\nThis assumes a Spring Boot environment and the embedded broker replaces the bootstrap servers property.\n\nThen, in each test class, you can use something similar to the following:\n\n[source, java]\n----\nstatic {\n EmbeddedKafkaHolder.getEmbeddedKafka().addTopics(\"topic1\", \"topic2\");\n}\n\nprivate static final EmbeddedKafkaBroker broker = EmbeddedKafkaHolder.getEmbeddedKafka();\n----\n\nIf you are not using Spring Boot, you can obtain the bootstrap servers using `broker.getBrokersAsString()`.\n\nIMPORTANT: The preceding example provides no mechanism for shutting down the broker(s) when all tests are complete.\nThis could be a problem if, say, you run your tests in a Gradle daemon.\nYou should not use this technique in such a situation, or you should use something to call `destroy()` on the `EmbeddedKafkaBroker` when your tests are complete.\n\nStarting with version 3.0, the framework exposes a `GlobalEmbeddedKafkaTestExecutionListener` for the JUnit Platform; it is disabled by default.\nThis requires JUnit Platform 1.8 or greater.\nThe purpose of this listener is to start one global `EmbeddedKafkaBroker` for the whole test plan and stop it at the end of the plan.\nTo enable this listener, and therefore have a single global embedded Kafka cluster for all the tests in the project, the `spring.kafka.global.embedded.enabled` property must be set to `true` via system properties or JUnit Platform configuration.\nIn addition, these properties can be provided:\n\n- `spring.kafka.embedded.count` - the number of Kafka brokers to manage;\n- `spring.kafka.embedded.ports` - ports (comma-separated value) for every Kafka broker to start, `0` if random port is preferred; the number of values must be equal to the `count` mentioned above;\n- `spring.kafka.embedded.topics` - topics (comma-separated value) to create in the started Kafka cluster;\n- `spring.kafka.embedded.partitions` - number of partitions to provision for the created topics;\n- `spring.kafka.embedded.broker.properties.location` - the location of the file for additional Kafka broker configuration properties; the value of this property must follow the Spring resource abstraction pattern.\n\nEssentially these properties mimic some of the `@EmbeddedKafka` attributes.\n\nSee more information about configuration properties and how to provide them in the https://junit.org/junit5/docs/current/user-guide/#running-tests-config-params[JUnit Jupiter User Guide].\nFor example, a `spring.embedded.kafka.brokers.property=my.bootstrap-servers` entry can be added into a `junit-platform.properties` file in the testing classpath.\nStarting with version 3.0.10, the broker automatically sets this to `spring.kafka.bootstrap-servers`, by default, for testing with Spring Boot applications.\n\nNOTE: It is recommended to not combine a global embedded Kafka and per-test class in a single test suite.\nBoth of them share the same system properties, so it is very likely going to lead to unexpected behavior.\n\nNOTE: `spring-kafka-test` has transitive dependencies on `junit-jupiter-api` and `junit-platform-launcher` (the latter to support the global embedded broker).\nIf you wish to use the embedded broker and are NOT using JUnit, you may wish to exclude these dependencies.\n\n[[embedded-kafka-annotation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Using the Same Broker(s) for Multiple Test Classes", "heading_level": 2, "file_order": 69, "section_index": 5, "content_hash": "1c9c5ec591e7bf264fb98f61b2f53d0b8d1be42107046ef6390dfb4cfa231c21", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:29c7d057ada948a717b51efef42c60a0c6250dd38accd2c73c3602bd374fcce2", "content": "We generally recommend that you use a single broker instance to avoid starting and stopping the broker between tests (and use a different topic for each test).\nStarting with version 2.0, if you use Spring's test application context caching, you can also declare a `EmbeddedKafkaBroker` bean, so a single broker can be used across multiple test classes.\nFor convenience, we provide a test class-level annotation called `@EmbeddedKafka` to register the `EmbeddedKafkaBroker` bean.\nThe following example shows how to use it:\n\n[source, java]\n----\n@SpringJUnitConfig\n@DirtiesContext\n@EmbeddedKafka(partitions = 1,\n topics = {\n KafkaStreamsTests.STREAMING_TOPIC1,\n KafkaStreamsTests.STREAMING_TOPIC2 })\npublic class KafkaStreamsTests {\n\n @Autowired\n private EmbeddedKafkaBroker embeddedKafka;\n\n @Test\n void someTest() {\n Map<String, Object> consumerProps = KafkaTestUtils.consumerProps(\"testGroup\", \"true\", this.embeddedKafka);\n consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n ConsumerFactory<Integer, String> cf = new DefaultKafkaConsumerFactory<>(consumerProps);\n Consumer<Integer, String> consumer = cf.createConsumer();\n this.embeddedKafka.consumeFromAnEmbeddedTopic(consumer, KafkaStreamsTests.STREAMING_TOPIC2);\n ConsumerRecords<Integer, String> replies = KafkaTestUtils.getRecords(consumer);\n assertThat(replies.count()).isGreaterThanOrEqualTo(1);\n }\n\n @Configuration\n @EnableKafkaStreams\n public static class TestKafkaStreamsConfiguration {\n\n @Value(\"${\" + EmbeddedKafkaBroker.SPRING_EMBEDDED_KAFKA_BROKERS + \"}\")\n private String brokerAddresses;\n\n @Bean(name = KafkaStreamsDefaultConfiguration.DEFAULT_STREAMS_CONFIG_BEAN_NAME)\n public KafkaStreamsConfiguration kStreamsConfigs() {\n Map<String, Object> props = new HashMap<>();\n props.put(StreamsConfig.APPLICATION_ID_CONFIG, \"testStreams\");\n props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, this.brokerAddresses);\n return new KafkaStreamsConfiguration(props);\n }\n\n }\n\n}\n----\n\nStarting with version 2.2.4, you can also use the `@EmbeddedKafka` annotation to specify the Kafka ports property.\n\nNOTE: As of version 4.0, all ZooKeeper-related properties have been removed from the `@EmbeddedKafka` annotation since Kafka 4.0 uses KRaft exclusively.\n\nThe following example sets the `topics`, `brokerProperties`, and `brokerPropertiesLocation` attributes of `@EmbeddedKafka` support property placeholder resolutions:\n\n[source, java]\n----\n@TestPropertySource(locations = \"classpath:/test.properties\")\n@EmbeddedKafka(topics = { \"any-topic\", \"${kafka.topics.another-topic}\" },\n brokerProperties = { \"log.dir=${kafka.broker.logs-dir}\",\n \"listeners=PLAINTEXT://localhost:${kafka.broker.port}\",\n \"auto.create.topics.enable=${kafka.broker.topics-enable:true}\" },\n brokerPropertiesLocation = \"classpath:/broker.properties\")\n----\n\nIn the preceding example, the property placeholders `${kafka.topics.another-topic}`, `${kafka.broker.logs-dir}`, and `${kafka.broker.port}` are resolved from the Spring `Environment`.\nIn addition, the broker properties are loaded from the `broker.properties` classpath resource specified by the `brokerPropertiesLocation`.\nProperty placeholders are resolved for the `brokerPropertiesLocation` URL and for any property placeholders found in the resource.\nProperties defined by `brokerProperties` override properties found in `brokerPropertiesLocation`.\n\nYou can use the `@EmbeddedKafka` annotation with JUnit Jupiter.\n\n[[embedded-kafka-junit-jupiter]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "`@EmbeddedKafka` Annotation", "heading_level": 2, "file_order": 69, "section_index": 6, "content_hash": "29c7d057ada948a717b51efef42c60a0c6250dd38accd2c73c3602bd374fcce2", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:c53e4a1ccf6bbab75e207e1b7e998ad75e89039d4f03f5d6d45e2c0532c5cee8", "content": "Starting with version 2.3, there are two ways to use the `@EmbeddedKafka` annotation with JUnit Jupiter.\nWhen used with the `@SpringJunitConfig` annotation, the embedded broker is added to the test application context.\nYou can auto wire the broker into your test, at the class or method level, to get the broker address list.\n\nWhen *not* using the spring test context, the `EmbdeddedKafkaCondition` creates a broker; the condition includes a parameter resolver so you can access the broker in your test method.\n\n[source, java]\n----\n@EmbeddedKafka\npublic class EmbeddedKafkaConditionTests {\n\n @Test\n public void test(EmbeddedKafkaBroker broker) {\n String brokerList = broker.getBrokersAsString();\n ...\n }\n\n}\n----\n\nA standalone broker (outside the Spring's TestContext) will be created unless a class annotated `@EmbeddedKafka` is also annotated (or meta-annotated) with `ExtendWith(SpringExtension.class)`.\n`@SpringJunitConfig` and `@SpringBootTest` are so meta-annotated and the context-based broker will be used when either of those annotations are also present.\n\nIMPORTANT: When there is a Spring test application context available, the topics and broker properties can contain property placeholders, which will be resolved as long as the property is defined somewhere.\nIf there is no Spring context available, these placeholders won't be resolved.\n\n[[embedded-broker-in-springboottest-annotations]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "`@EmbeddedKafka` Annotation with JUnit Jupiter", "heading_level": 2, "file_order": 69, "section_index": 7, "content_hash": "c53e4a1ccf6bbab75e207e1b7e998ad75e89039d4f03f5d6d45e2c0532c5cee8", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:6f5ec30b13b89597a3233707a57dfa1132715ead3adea693c4bce7af299deb7e", "content": "https://start.spring.io/[Spring Initializr] now automatically adds the `spring-kafka-test` dependency in test scope to the project configuration.\n\n[IMPORTANT]\n====\nIf your application uses the Kafka binder in `spring-cloud-stream` and if you want to use an embedded broker for tests, you must remove the `spring-cloud-stream-test-support` dependency, because it replaces the real binder with a test binder for test cases.\nIf you wish some tests to use the test binder and some to use the embedded broker, tests that use the real binder need to disable the test binder by excluding the binder auto configuration in the test class.\nThe following example shows how to do so:\n\n=====\n[source, java]\n----\n@SpringJUnitConfig\n@SpringBootTest(properties = \"spring.autoconfigure.exclude=\"\n + \"org.springframework.cloud.stream.test.binder.TestSupportBinderAutoConfiguration\")\npublic class MyApplicationTests {\n ...\n}\n----\n=====\n====\n\nThere are several ways to use an embedded broker in a Spring Boot application test.\n\nThey include:\n\n* xref:testing.adoc#kafka-testing-embeddedkafka-annotation[`@EmbeddedKafka` Annotation or `EmbeddedKafkaBroker` Bean]\n\n[[embedded-broker-with-springjunitconfig-annotations]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Embedded Broker in `@SpringBootTest` Annotations", "heading_level": 2, "file_order": 69, "section_index": 8, "content_hash": "6f5ec30b13b89597a3233707a57dfa1132715ead3adea693c4bce7af299deb7e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:71870c634f915272ae316083f43e76f47966b5f0c1b7b7295808506c86e29187", "content": "When using `@EmbeddedKafka` with `@SpringJUnitConfig`, it is recommended to use `@DirtiesContext` on the test class.\nThis is to prevent potential race conditions occurring during the JVM shutdown after running multiple tests in a test suite.\nFor example, without using `@DirtiesContext`, the `EmbeddedKafkaBroker` may shutdown earlier while the application context still needs resources from it.\nSince every `EmbeddedKafka` test-runs create its own temporary directory, when this race condition occurs, it will produce error log messages indicating that the files that it is trying to delete or cleanup are not available anymore.\nAdding `@DirtiesContext` will ensure that the application context is cleaned up after each test and not cached, making it less vulnerable to potential resource race conditions like these.\n\n[[kafka-testing-embeddedkafka-annotation]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "`@EmbeddedKafka` with `@SpringJunitConfig`", "heading_level": 2, "file_order": 69, "section_index": 9, "content_hash": "71870c634f915272ae316083f43e76f47966b5f0c1b7b7295808506c86e29187", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:fe598f300596899dd5a01861a3fdd8e9ad93a3da62b10aa2bca23783a07e81b3", "content": "The following example shows how to use an `@EmbeddedKafka` Annotation to create an embedded broker:\n\n[source, java]\n----\n@SpringJUnitConfig\n@EmbeddedKafka(topics = \"someTopic\",\n bootstrapServersProperty = \"spring.kafka.bootstrap-servers\") // this is now the default\npublic class MyApplicationTests {\n\n @Autowired\n private KafkaTemplate<String, String> template;\n\n @Test\n void test() {\n ...\n }\n\n}\n----\n\nNOTE: The `bootstrapServersProperty` is automatically set to `spring.kafka.bootstrap-servers` by default, starting with version 3.0.10.\n\n[[hamcrest-matchers]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "`@EmbeddedKafka` Annotation or `EmbeddedKafkaBroker` Bean", "heading_level": 3, "file_order": 69, "section_index": 10, "content_hash": "fe598f300596899dd5a01861a3fdd8e9ad93a3da62b10aa2bca23783a07e81b3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:d565ed57fb68a194b6696811f2a0314596afa50a915797d31f2dfd6934095133", "content": "The `org.springframework.kafka.test.hamcrest.KafkaMatchers` provides the following matchers:\n\n[source, java]\n----\n/**\n * @param key the key\n * @param <K> the type.\n * @return a Matcher that matches the key in a consumer record.\n */\npublic static <K> Matcher<ConsumerRecord<K, ?>> hasKey(K key) { ... }\n\n/**\n * @param value the value.\n * @param <V> the type.\n * @return a Matcher that matches the value in a consumer record.\n */\npublic static <V> Matcher<ConsumerRecord<?, V>> hasValue(V value) { ... }\n\n/**\n * @param partition the partition.\n * @return a Matcher that matches the partition in a consumer record.\n */\npublic static Matcher<ConsumerRecord<?, ?>> hasPartition(int partition) { ... }\n\n/**\n * Matcher testing the timestamp of a {@link ConsumerRecord} assuming the topic has been set with\n * {@link org.apache.kafka.common.record.TimestampType#CREATE_TIME CreateTime}.\n *\n * @param ts timestamp of the consumer record.\n * @return a Matcher that matches the timestamp in a consumer record.\n */\npublic static Matcher<ConsumerRecord<?, ?>> hasTimestamp(long ts) {\n return hasTimestamp(TimestampType.CREATE_TIME, ts);\n}\n\n/**\n * Matcher testing the timestamp of a {@link ConsumerRecord}\n * @param type timestamp type of the record\n * @param ts timestamp of the consumer record.\n * @return a Matcher that matches the timestamp in a consumer record.\n */\npublic static Matcher<ConsumerRecord<?, ?>> hasTimestamp(TimestampType type, long ts) {\n return new ConsumerRecordTimestampMatcher(type, ts);\n}\n----\n\n[[assertj-conditions]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Hamcrest Matchers", "heading_level": 2, "file_order": 69, "section_index": 11, "content_hash": "d565ed57fb68a194b6696811f2a0314596afa50a915797d31f2dfd6934095133", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:d102de89bacc9d224dd2a3a6c9269c8c9521b09eb5c98f5323dedd1889082303", "content": "You can use the following AssertJ conditions:\n\n[source, java]\n----\n/**\n * @param key the key\n * @param <K> the type.\n * @return a Condition that matches the key in a consumer record.\n */\npublic static <K> Condition<ConsumerRecord<K, ?>> key(K key) { ... }\n\n/**\n * @param value the value.\n * @param <V> the type.\n * @return a Condition that matches the value in a consumer record.\n */\npublic static <V> Condition<ConsumerRecord<?, V>> value(V value) { ... }\n\n/**\n * @param key the key.\n * @param value the value.\n * @param <K> the key type.\n * @param <V> the value type.\n * @return a Condition that matches the key in a consumer record.\n * @since 2.2.12\n */\npublic static <K, V> Condition<ConsumerRecord<K, V>> keyValue(K key, V value) { ... }\n\n/**\n * @param partition the partition.\n * @return a Condition that matches the partition in a consumer record.\n */\npublic static Condition<ConsumerRecord<?, ?>> partition(int partition) { ... }\n\n/**\n * @param value the timestamp.\n * @return a Condition that matches the timestamp value in a consumer record.\n */\npublic static Condition<ConsumerRecord<?, ?>> timestamp(long value) {\n return new ConsumerRecordTimestampCondition(TimestampType.CREATE_TIME, value);\n}\n\n/**\n * @param type the type of timestamp\n * @param value the timestamp.\n * @return a Condition that matches the timestamp value in a consumer record.\n */\npublic static Condition<ConsumerRecord<?, ?>> timestamp(TimestampType type, long value) {\n return new ConsumerRecordTimestampCondition(type, value);\n}\n----\n\n[[example]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "AssertJ Conditions", "heading_level": 2, "file_order": 69, "section_index": 12, "content_hash": "d102de89bacc9d224dd2a3a6c9269c8c9521b09eb5c98f5323dedd1889082303", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:95ae41fcb6fa50c4626ccc2b14a45b7cf10012f0fe05f7c14a8dcf64ca6857c0", "content": "The following example brings together most of the topics covered in this chapter:\n\n[source, java]\n----\n@EmbeddedKafka(topics = KafkaTemplateTests.TEMPLATE_TOPIC)\npublic class KafkaTemplateTests {\n\n public static final String TEMPLATE_TOPIC = \"templateTopic\";\n public static EmbeddedKafkaBroker embeddedKafka;\n\n @BeforeAll\n\tpublic static void setUp() {\n embeddedKafka = EmbeddedKafkaCondition.getBroker();\n\t}\n\n @Test\n public void testTemplate() throws Exception {\n Map<String, Object> consumerProps = KafkaTestUtils.consumerProps(\"testT\", \"false\",\n embeddedKafka);\n DefaultKafkaConsumerFactory<Integer, String> cf =\n new DefaultKafkaConsumerFactory<>(consumerProps);\n ContainerProperties containerProperties = new ContainerProperties(TEMPLATE_TOPIC);\n KafkaMessageListenerContainer<Integer, String> container =\n new KafkaMessageListenerContainer<>(cf, containerProperties);\n final BlockingQueue<ConsumerRecord<Integer, String>> records = new LinkedBlockingQueue<>();\n container.setupMessageListener(new MessageListener<Integer, String>() {\n\n @Override\n public void onMessage(ConsumerRecord<Integer, String> record) {\n System.out.println(record);\n records.add(record);\n }\n\n });\n container.setBeanName(\"templateTests\");\n container.start();\n ContainerTestUtils.waitForAssignment(container,\n embeddedKafka.getPartitionsPerTopic());\n Map<String, Object> producerProps =\n KafkaTestUtils.producerProps(embeddedKafka);\n ProducerFactory<Integer, String> pf =\n new DefaultKafkaProducerFactory<>(producerProps);\n KafkaTemplate<Integer, String> template = new KafkaTemplate<>(pf);\n template.setDefaultTopic(TEMPLATE_TOPIC);\n template.sendDefault(\"foo\");\n assertThat(records.poll(10, TimeUnit.SECONDS), hasValue(\"foo\"));\n template.sendDefault(0, 2, \"bar\");\n ConsumerRecord<Integer, String> received = records.poll(10, TimeUnit.SECONDS);\n assertThat(received, hasKey(2));\n assertThat(received, hasPartition(0));\n assertThat(received, hasValue(\"bar\"));\n template.send(TEMPLATE_TOPIC, 0, 2, \"baz\");\n received = records.poll(10, TimeUnit.SECONDS);\n assertThat(received, hasKey(2));\n assertThat(received, hasPartition(0));\n assertThat(received, hasValue(\"baz\"));\n }\n}\n----\n\nThe preceding example uses the Hamcrest matchers.\nWith `AssertJ`, the final part looks like the following code:\n\n[source, java]\n----\nassertThat(records.poll(10, TimeUnit.SECONDS)).has(value(\"foo\"));\ntemplate.sendDefault(0, 2, \"bar\");\nConsumerRecord<Integer, String> received = records.poll(10, TimeUnit.SECONDS);\nassertThat(received).has(key(2));\nassertThat(received).has(value(\"bar\"));\nassertThat(received).has(partition(0));\ntemplate.send(TEMPLATE_TOPIC, 0, 2, \"baz\");\nreceived = records.poll(10, TimeUnit.SECONDS);\nassertThat(received).has(allOf(keyValue(2, \"baz\"), partition(0)));\n----\n\n[[mock-cons-prod]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Example", "heading_level": 2, "file_order": 69, "section_index": 13, "content_hash": "95ae41fcb6fa50c4626ccc2b14a45b7cf10012f0fe05f7c14a8dcf64ca6857c0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:fa337f95f76680e2ce5a86a9d1a4f8bba54540a734a69cca8bf1fb2c83be280e", "content": "The `kafka-clients` library provides `MockConsumer` and `MockProducer` classes for testing purposes.\n\nIf you wish to use these classes in some of your tests with listener containers or `KafkaTemplate` respectively, starting with version 3.0.7, the framework now provides `MockConsumerFactory` and `MockProducerFactory` implementations.\n\nThese factories can be used in the listener container and template instead of the default factories, which require a running (or embedded) broker.\n\nHere is an example of a simple implementation returning a single consumer:\n\n[source, java]\n----\n@Bean\nConsumerFactory<String, String> consumerFactory() {\n MockConsumer<String, String> consumer = new MockConsumer<>(OffsetResetStrategy.EARLIEST);\n TopicPartition topicPartition0 = new TopicPartition(\"topic\", 0);\n List<TopicPartition> topicPartitions = Collections.singletonList(topicPartition0);\n Map<TopicPartition, Long> beginningOffsets = topicPartitions.stream().collect(Collectors\n .toMap(Function.identity(), tp -> 0L));\n consumer.updateBeginningOffsets(beginningOffsets);\n consumer.schedulePollTask(() -> {\n consumer.addRecord(\n new ConsumerRecord<>(\"topic\", 0, 0L, 0L, TimestampType.NO_TIMESTAMP_TYPE, 0, 0, null, \"test1\",\n new RecordHeaders(), Optional.empty()));\n consumer.addRecord(\n new ConsumerRecord<>(\"topic\", 0, 1L, 0L, TimestampType.NO_TIMESTAMP_TYPE, 0, 0, null, \"test2\",\n new RecordHeaders(), Optional.empty()));\n });\n return new MockConsumerFactory(() -> consumer);\n}\n----\n\nIf you wish to test with concurrency, the `Supplier` lambda in the factory's constructor would need to create a new instance each time.\n\nWith the `MockProducerFactory`, there are two constructors; one to create a simple factory, and one to create factory that supports transactions.\n\nHere are examples:\n\n[source, java]\n----\n@Bean\nProducerFactory<String, String> nonTransFactory() {\n return new MockProducerFactory<>(() ->\n new MockProducer<>(true, new StringSerializer(), new StringSerializer()));\n}\n\n@Bean\nProducerFactory<String, String> transFactory() {\n MockProducer<String, String> mockProducer =\n new MockProducer<>(true, new StringSerializer(), new StringSerializer());\n mockProducer.initTransactions();\n return new MockProducerFactory<String, String>((tx, id) -> mockProducer, \"defaultTxId\");\n}\n----\n\nNotice in the second case, the lambda is a `BiFunction<Boolean, String>` where the first parameter is true if the caller wants a transactional producer; the optional second parameter contains the transactional id.\nThis can be the default (as provided in the constructor), or can be overridden by the `KafkaTransactionManager` (or `KafkaTemplate` for local transactions), if so configured.\nThe transactional id is provided in case you wish to use a different `MockProducer` based on this value.\n\nIf you are using producers in a multi-threaded environment, the `BiFunction` should return multiple producers (perhaps thread-bound using a `ThreadLocal`).\n\nIMPORTANT: Transactional ``MockProducer``s must be initialized for transactions by calling `initTransaction()`.\n\nWhen using the `MockProducer`, if you do not want to close the producer after each send, then you can provide a custom `MockProducer` implementation that overrides the `close` method that does not call the `close` method from the super class.\nThis is convenient for testing, when verifying multiple publishing on the same producer without closing it.\n\nHere is an example:\n\n[source,java]\n----\n@Bean\nMockProducer<String, String> mockProducer() {\n return new MockProducer<>(false, new StringSerializer(), new StringSerializer()) {\n @Override\n public void close() {\n\n }\n };\n}\n\n@Bean\nProducerFactory<String, String> mockProducerFactory(MockProducer<String, String> mockProducer) {\n return new MockProducerFactory<>(() -> mockProducer);\n}\n\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/testing.adoc", "title": "testing", "heading": "Mock Consumer and Producer", "heading_level": 2, "file_order": 69, "section_index": 14, "content_hash": "fa337f95f76680e2ce5a86a9d1a4f8bba54540a734a69cca8bf1fb2c83be280e", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/testing.adoc"}}
{"id": "sha256:fa4755be8591f1f2efb7ff549ed996c4421b0d152ac5132cf2ea3dd91154ed9a", "content": "[[tip-assign-all-parts]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/tips.adoc", "title": "Tips, Tricks and Examples", "heading": "Tips, Tricks and Examples", "heading_level": 1, "file_order": 70, "section_index": 0, "content_hash": "fa4755be8591f1f2efb7ff549ed996c4421b0d152ac5132cf2ea3dd91154ed9a", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/tips.adoc"}}
{"id": "sha256:9681ff5156bb630a4aa0aefac26f2b29fd3d6e9abd0fb47aff9a132e5af2a35d", "content": "Let's say you want to always read all records from all partitions (such as when using a compacted topic to load a distributed cache), it can be useful to manually assign the partitions and not use Kafka's group management.\nDoing so can be unwieldy when there are many partitions, because you have to list the partitions.\nIt's also an issue if the number of partitions changes over time, because you would have to recompile your application each time the partition count changes.\n\nThe following is an example of how to use the power of a SpEL expression to create the partition list dynamically when the application starts:\n\n[source, java]\n----\n@KafkaListener(topicPartitions = @TopicPartition(topic = \"compacted\",\n partitions = \"#{@finder.partitions('compacted')}\",\n partitionOffsets = @PartitionOffset(partition = \"*\", initialOffset = \"0\")))\npublic void listen(@Header(KafkaHeaders.RECEIVED_KEY) String key, String payload) {\n ...\n}\n\n@Bean\npublic PartitionFinder finder(ConsumerFactory<String, String> consumerFactory) {\n return new PartitionFinder(consumerFactory);\n}\n\npublic static class PartitionFinder {\n\n private final ConsumerFactory<String, String> consumerFactory;\n\n public PartitionFinder(ConsumerFactory<String, String> consumerFactory) {\n this.consumerFactory = consumerFactory;\n }\n\n public String[] partitions(String topic) {\n try (Consumer<String, String> consumer = consumerFactory.createConsumer()) {\n return consumer.partitionsFor(topic).stream()\n .map(pi -> \"\" + pi.partition())\n .toArray(String[]::new);\n }\n }\n\n}\n----\n\nUsing this in conjunction with `ConsumerConfig.AUTO_OFFSET_RESET_CONFIG=earliest` will load all records each time the application is started.\nYou should also set the container's `AckMode` to `MANUAL` to prevent the container from committing offsets for a `null` consumer group.\nStarting with version 3.1, the container will automatically coerce the `AckMode` to `MANUAL` when manual topic assignment is used with no consumer `group.id`.\nHowever, starting with version 2.5.5, as shown above, you can apply an initial offset to all partitions; see xref:kafka/receiving-messages/listener-annotation.adoc#manual-assignment[Explicit Partition Assignment] for more information.\n\n[[ex-jdbc-sync]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/tips.adoc", "title": "Tips, Tricks and Examples", "heading": "Manually Assigning All Partitions", "heading_level": 2, "file_order": 70, "section_index": 1, "content_hash": "9681ff5156bb630a4aa0aefac26f2b29fd3d6e9abd0fb47aff9a132e5af2a35d", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/tips.adoc"}}
{"id": "sha256:7ef5c2e02286fc809ff79afed650bf6ace7f20d2981ad5fab5ff92e7135c6dc3", "content": "The following Spring Boot application is an example of chaining database and Kafka transactions.\nThe listener container starts the Kafka transaction and the `@Transactional` annotation starts the DB transaction.\nThe DB transaction is committed first; if the Kafka transaction fails to commit, the record will be redelivered so the DB update should be idempotent.\n\n[source, java]\n----\n@SpringBootApplication\npublic class Application {\n\n public static void main(String[] args) {\n SpringApplication.run(Application.class, args);\n }\n\n @Bean\n public ApplicationRunner runner(KafkaTemplate<String, String> template) {\n return args -> template.executeInTransaction(t -> t.send(\"topic1\", \"test\"));\n }\n\n @Bean\n public DataSourceTransactionManager dstm(DataSource dataSource) {\n return new DataSourceTransactionManager(dataSource);\n }\n\n @Component\n public static class Listener {\n\n private final JdbcTemplate jdbcTemplate;\n\n private final KafkaTemplate<String, String> kafkaTemplate;\n\n public Listener(JdbcTemplate jdbcTemplate, KafkaTemplate<String, String> kafkaTemplate) {\n this.jdbcTemplate = jdbcTemplate;\n this.kafkaTemplate = kafkaTemplate;\n }\n\n @KafkaListener(id = \"group1\", topics = \"topic1\")\n @Transactional(\"dstm\")\n public void listen1(String in) {\n this.kafkaTemplate.send(\"topic2\", in.toUpperCase());\n this.jdbcTemplate.execute(\"insert into mytable (data) values ('\" + in + \"')\");\n }\n\n @KafkaListener(id = \"group2\", topics = \"topic2\")\n public void listen2(String in) {\n System.out.println(in);\n }\n\n }\n\n @Bean\n public NewTopic topic1() {\n return TopicBuilder.name(\"topic1\").build();\n }\n\n @Bean\n public NewTopic topic2() {\n return TopicBuilder.name(\"topic2\").build();\n }\n\n}\n----\n\n[source, properties]\n----\nspring.datasource.url=jdbc:mysql://localhost/integration?serverTimezone=UTC\nspring.datasource.username=root\nspring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver\n\nspring.kafka.consumer.auto-offset-reset=earliest\nspring.kafka.consumer.enable-auto-commit=false\nspring.kafka.consumer.properties.isolation.level=read_committed\n\nspring.kafka.producer.transaction-id-prefix=tx-\n\n#logging.level.org.springframework.transaction=trace\n#logging.level.org.springframework.kafka.transaction=debug\n#logging.level.org.springframework.jdbc=debug\n----\n\n[source, sql]\n----\ncreate table mytable (data varchar(20));\n----\n\nFor producer-only transactions, transaction synchronization works:\n\n[source, java]\n----\n@Transactional(\"dstm\")\npublic void someMethod(String in) {\n this.kafkaTemplate.send(\"topic2\", in.toUpperCase());\n this.jdbcTemplate.execute(\"insert into mytable (data) values ('\" + in + \"')\");\n}\n----\n\nThe `KafkaTemplate` will synchronize its transaction with the DB transaction and the commit/rollback occurs after the database.\n\nIf you wish to commit the Kafka transaction first, and only commit the DB transaction if the Kafka transaction is successful, use nested `@Transactional` methods:\n\n[source, java]\n----\n@Transactional(\"dstm\")\npublic void someMethod(String in) {\n this.jdbcTemplate.execute(\"insert into mytable (data) values ('\" + in + \"')\");\n sendToKafka(in);\n}\n\n@Transactional(\"kafkaTransactionManager\")\npublic void sendToKafka(String in) {\n this.kafkaTemplate.send(\"topic2\", in.toUpperCase());\n}\n----\n\n[[tip-json]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/tips.adoc", "title": "Tips, Tricks and Examples", "heading": "Examples of Kafka Transactions with Other Transaction Managers", "heading_level": 2, "file_order": 70, "section_index": 2, "content_hash": "7ef5c2e02286fc809ff79afed650bf6ace7f20d2981ad5fab5ff92e7135c6dc3", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/tips.adoc"}}
{"id": "sha256:7c52da6832cb60d89b83618ce238b1e2ce7391f98debf6f584631c103f5fb6b0", "content": "The serializer and deserializer support a number of customizations using properties, see xref:kafka/serdes.adoc#json-serde[JSON] for more information.\nThe `kafka-clients` code, not Spring, instantiates these objects, unless you inject them directly into the consumer and producer factories.\nIf you wish to configure the (de)serializer using properties, but wish to use, say, a custom `JsonMapper`, simply create a subclass and pass the custom mapper into the `super` constructor. For example:\n\n[source, java]\n----\npublic class CustomJsonSerializer extends JacksonJsonSerializer<Object> {\n\n public CustomJsonSerializer() {\n super(customizedJsonMapper());\n }\n\n private static JsonMapper customizedJsonMapper() {\n return JacksonMapperUtils.enhancedJsonMapper()\n .rebuild()\n // .enable() and/or .disable() features according to your needs\n .build();\n }\n}\n----", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/tips.adoc", "title": "Tips, Tricks and Examples", "heading": "Customizing the JsonSerializer and JsonDeserializer", "heading_level": 2, "file_order": 70, "section_index": 3, "content_hash": "7c52da6832cb60d89b83618ce238b1e2ce7391f98debf6f584631c103f5fb6b0", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/tips.adoc"}}
{"id": "sha256:8a6fe32661b2e505416a0010000dda0da0b780c18ae28a3496f5325a86e3aefe", "content": "[[whats-new-in-4-1-since-4.0]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/whats-new.adoc", "title": "What's new?", "heading": "What's new?", "heading_level": 1, "file_order": 71, "section_index": 0, "content_hash": "8a6fe32661b2e505416a0010000dda0da0b780c18ae28a3496f5325a86e3aefe", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/whats-new.adoc"}}
{"id": "sha256:e656869082499c386f17346afc3a065d6c10904f9d7cb6dc69e803d4cba8b6a6", "content": "This section covers the changes made from version 4.0 to version 4.1.\nFor changes in earlier versions, see xref:appendix/change-history.adoc[Change History].\n\n[[x41-kafka-listener-ack-mode]]", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/whats-new.adoc", "title": "What's new?", "heading": "What's New in 4.1 Since 4.0", "heading_level": 2, "file_order": 71, "section_index": 1, "content_hash": "e656869082499c386f17346afc3a065d6c10904f9d7cb6dc69e803d4cba8b6a6", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/whats-new.adoc"}}
{"id": "sha256:37851e18b32cba72f0fdb23a3ebee2d754664f9f27cb212ce21592d27338573c", "content": "The `@KafkaListener` annotation now supports an `ackMode` attribute, allowing individual listeners to override the container factory's default acknowledgment mode without creating separate container factory beans.\nThe attribute also supports SpEL expressions and property placeholders.\nSee xref:kafka/receiving-messages/listener-annotation.adoc[`@KafkaListener` Annotation] for more information.", "metadata": {"source_type": "repo_asciidoc", "project": "spring-kafka", "path": "antora/modules/ROOT/pages/whats-new.adoc", "title": "What's new?", "heading": "`@KafkaListener` Changes", "heading_level": 3, "file_order": 71, "section_index": 2, "content_hash": "37851e18b32cba72f0fdb23a3ebee2d754664f9f27cb212ce21592d27338573c", "source_url": "https://github.com/spring-projects/spring-kafka/blob/6a7caee679cc4f7b20032dbfef4a84f35196af93/spring-kafka-docs/src/main/antora/modules/ROOT/pages/whats-new.adoc"}}
